<p align="left"> <a href="READMECN.md">‰∏≠Êñá</a>&nbsp ÔΩú &nbspEnglish&nbsp ÔΩú &nbsp<a href="READMEJA.md">Êó•Êú¨Ë™û</a> ÔΩú &nbsp<a href="READMEFR.md">Fran√ßais</a> ÔΩú &nbsp<a href="READMEES.md">Espa√±ol</a> </p> <br><br> <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logoqwen.jpg" width="400"/> <p> <br> <p align="center"> ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ÔΩú &nbsp&nbspüñ•Ô∏è <a href="https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary">Demo</a> <br> <a href="assets/wechat.png">WeChat ÂæÆ‰ø°</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp ÔΩú &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a> </p> <br><br> > !Important > Qwen2 is here! You are welcome to follow QwenLM/Qwen2 and share your experience there. > > This repo QwenLM/Qwen is no longer actively maintained, due to substantial codebase differences. <br> | | Qwen-Chat | Qwen-Chat Int4 | Qwen-Chat Int8 | Qwen | |-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:| | 1.8B | <a href="https://modelscope.cn/models/qwen/Qwen-18B-Chat/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-18B-Chat">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-18B-Chat-Int4/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-18B-Chat-Int4">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-18B-Chat-Int8/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-18B-Chat-Int8">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-18B/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-18B">ü§ó</a> | | 7B | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-7B">ü§ó</a> | | 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-14B">ü§ó</a> | | 72B | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-72B-Chat">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int4">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int8">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B/summary">ü§ñ</a> <a href="https://huggingface.co/Qwen/Qwen-72B">ü§ó</a> | We opensource our Qwen series, now including Qwen, the base language models, namely Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B, as well as Qwen-Chat, the chat models, namely Qwen-1.8B-Chat, Qwen-7B-Chat, Qwen-14B-Chat, and Qwen-72B-Chat. Links are on the above table. Click them and check the model cards. Also, we release the technical report. Please click the paper link and check it out! In brief, we have strong base language models, which have been stably pretrained for up to 3 trillion tokens of multilingual data with a wide coverage of domains, languages with a focus on Chinese and English, etc. They are able to achieve competitive performance on benchmark datasets. Additionally, we have chat models that are aligned with human preference based on SFT and RLHF not released yet, which are able to chat, create content, extract information, summarize, translate, code, solve math problems, and so on, and are able to use tools, play as agents, or even play as code interpreters, etc. | Model | Release Date | Max Length | System Prompt Enhancement | of Pretrained Tokens | Minimum GPU Memory Usage of Finetuning Q-Lora | Minimum GPU Usage of Generating 2048 Tokens Int4 | Tool Usage | |:----------|:------------:|:----------:|:-------------------------:|:----------------------:|:-----------------------------------------------:|:--------------------------------------------------:|:----------:| | Qwen-1.8B | 23.11.30 | 32K | ‚úÖ | 2.2T | 5.8GB | 2.9GB | ‚úÖ | | Qwen-7B | 23.08.03 | 32K | ‚ùé | 2.4T | 11.5GB | 8.2GB | ‚úÖ | | Qwen-14B | 23.09.25 | 8K | ‚ùé | 3.0T | 18.7GB | 13.0GB | ‚úÖ | | Qwen-72B | 23.11.30 | 32K | ‚úÖ | 3.0T | 61.4GB | 48.9GB | ‚úÖ | In this repo, you can figure out: Quickstart with Qwen, and enjoy the simple inference. Details about the quantization models, including GPTQ and KV cache quantization. Statistics of inference performance, including speed and memory. Tutorials on finetuning, including full-parameter tuning, LoRA, and Q-LoRA. Instructions on deployment, with the example of vLLM and FastChat. Instructions on building demos, including WebUI, CLI demo, etc. Introduction to DashScope API service, as well as the instructions on building an OpenAI-style API for your model. Information about Qwen for tool use, agent, and code interpreter Statistics of long-context understanding evaluation License agreement ... Also, if you meet problems, turn to FAQ for help first. Still feeling struggled? Feel free to shoot us issues better in English so that more people can understand you! If you would like to help us, send us pull requests with no hesitation! We are always excited about PR! Would like to chat with us or date us coffee time? Welcome to our Discord or WeChat! <br><br> News and Updates 2023.11.30 üî• We release Qwen-72B and Qwen-72B-Chat, which are trained on 3T tokens and support 32k context, along with Qwen-1.8B, and Qwen-1.8B-Chat, on ModelScope and Hugging Face. We have also strengthened the System Prompt capabilities of the Qwen-72B-Chat and Qwen-1.8B-Chat, see example documentation. Additionally, support the inference on Ascend 910 and Hygon DCU. Check and for more details. 2023.10.17 We release the Int8 quantized model Qwen-7B-Chat-Int8 and Qwen-14B-Chat-Int8. 2023.9.25 üî• We release Qwen-14B and Qwen-14B-Chat on ModelScope and Hugging Face, along with qwen.cpp and Qwen-Agent. Codes and checkpoints of Qwen-7B and Qwen-7B-Chat are also updated. PLEASE PULL THE LATEST VERSION! - Compared to Qwen-7B original, Qwen-7B uses more training tokens, increasing from 2.2T tokens to 2.4T tokens, while the context length extends from 2048 to 8192. The Chinese knowledge and coding ability of Qwen-7B have been further improved. 2023.9.12 We now support finetuning on the Qwen-7B models, including full-parameter finetuning, LoRA and Q-LoRA. 2023.8.21 We release the Int4 quantized model for Qwen-7B-Chat, Qwen-7B-Chat-Int4, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation. 2023.8.3 We release both Qwen-7B and Qwen-7B-Chat on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance. <br> Performance Qwen models outperform the baseline models of similar model sizes on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., which evaluate the models‚Äô capabilities on natural language understanding, mathematic problem solving, coding, etc. Qwen-72B achieves better performance than LLaMA2-70B on all tasks and outperforms GPT-3.5 on 7 out of 10 tasks. <p align="left"> <img src="assets/radar72b.jpg" width=600px/> <p> <br> | Model | MMLU | C-Eval | GSM8K | MATH | HumanEval | MBPP | BBH | CMMLU | |:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:| | | 5-shot | 5-shot | 8-shot | 4-shot | 0-shot | 3-shot | 3-shot | 5-shot | | LLaMA2-7B | 46.8 | 32.5 | 16.7 | 3.3 | 12.8 | 20.8 | 38.2 | 31.8 | | LLaMA2-13B | 55.0 | 41.4 | 29.6 | 5.0 | 18.9 | 30.3 | 45.6 | 38.4 | | LLaMA2-34B | 62.6 | - | 42.2 | 6.2 | 22.6 | 33.0 | 44.1 | - | | ChatGLM2-6B | 47.9 | 51.7 | 32.4 | 6.5 | - | - | 33.7 | - | | InternLM-7B | 51.0 | 53.4 | 31.2 | 6.3 | 10.4 | 14.0 | 37.0 | 51.8 | | InternLM-20B | 62.1 | 58.8 | 52.6 | 7.9 | 25.6 | 35.6 | 52.5 | 59.0 | | Baichuan2-7B | 54.7 | 56.3 | 24.6 | 5.6 | 18.3 | 24.2 | 41.6 | 57.1 | | Baichuan2-13B | 59.5 | 59.0 | 52.8 | 10.1 | 17.1 | 30.2 | 49.0 | 62.0 | | Yi-34B | 76.3 | 81.8 | 67.9 | 15.9 | 26.2 | 38.2 | 66.4 | 82.6 | | XVERSE-65B | 70.8 | 68.6 | 60.3 | - | 26.3 | - | - | - | | Qwen-1.8B | 45.3 | 56.1 | 32.3 | 2.3 | 15.2 | 14.2 | 22.3 | 52.1 | | Qwen-7B | 58.2 | 63.5 | 51.7 | 11.6 | 29.9 | 31.6 | 45.0 | 62.2 | | Qwen-14B | 66.3 | 72.1 | 61.3 | 24.8 | 32.3 | 40.8 | 53.4 | 71.0 | | Qwen-72B | 77.4 | 83.3 | 78.9 | 35.2 | 35.4 | 52.2 | 67.7 | 83.6 | For all compared models, we report the best scores between their official reported results and OpenCompass. For more experimental results detailed model performance on more benchmark datasets and details, please refer to our technical report by clicking here. <br><br> Requirements python 3.8 and above pytorch 1.12 and above, 2.0 and above are recommended transformers 4.32 and above CUDA 11.4 and above are recommended this is for GPU users, flash-attention users, etc. <br> Quickstart Below, we provide simple examples to show how to use Qwen-Chat with ü§ñ ModelScope and ü§ó Transformers. You can use our pre-built docker images to skip most of the environment setup steps, see Section "Using Pre-built Docker Images" for more details. If not using docker, please make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries. If your device supports fp16 or bf16, we recommend installing flash-attention we support flash attention 2 now. for higher efficiency and lower memory usage. flash-attention is optional and the project can run normally without installing it Now you can start with ModelScope or Transformers. ü§ó Transformers To use Qwen-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. Remember to pass in the correct model names or paths, such as "Qwen/Qwen-7B-Chat" and "Qwen/Qwen-14B-Chat". However, please make sure that you are using the latest code. Running Qwen, the base language model, is also simple. <details> <summary>Running Qwen</summary> </details> <p id="DownloadModel"> In the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below: </p> ü§ñ ModelScope ModelScope is an open-source platform for Model-as-a-Service MaaS, which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below: Batch Inference Qwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below: CPU To deploy our models on CPU, we strongly advise you to use qwen.cpp, which is a pure C++ implementation of Qwen and tiktoken. Check the repo for more details! Also, it is also simple to directly run the model on CPU, which requires your specification of device: However, it is likely that you suffer from extremely low inference efficiency. Multiple GPUs If you suffer from lack of GPU memory and you would like to run the model on more than 1 GPU, you can directly use the default loading method, which is now supported by Transformers. The previous method based on is deprecated. However, though this method is simple, the efficiency of the native pipeline parallelism is low. We advise you to use vLLM with FastChat and please read the section for deployment. x86 Platforms When deploy on Core‚Ñ¢/Xeon¬Æ Scalable Processors or with Arc‚Ñ¢ GPU, OpenVINO‚Ñ¢ Toolkit is recommended. You can install and run this example notebook. For related issues, you are welcome to file an issue at OpenVINO repo. DashScope The most simple way to use Qwen through APIs is DashScope API service through Alibaba Cloud. We give an introduction to the usage. Additionally, we provide a script for you to deploy an OpenAI-style API on your own servers. DashScope is the large language model API service provided by Alibaba Cloud, which now supports Qwen. Note that the models behind DashScope are in-house versions temporarily without details provided. The services include and , where the former one runs faster and the latter achieves better performance. For more information, visit the documentation here. Please head to the official website link to create a DashScope account and obtain the API key AK. We recommend setting the AK with an environment variable: Then please install the packages and click here for the documentation. If you use Python, you can install DashScope with pip: If you use JAVA SDK, you can install it in this way: The simplest way to use DashScope is the usage with messages, which is similar to OpenAI API. The example is demonstrated below: For more usages, please visit the official website for more details. <br><br> Quantization GPTQ We provide a solution based on AutoGPTQ, and release the Int4 and Int8 quantized models, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed. Here we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq e.g., torch 2.0 and above, transformers 4.32.0 and above, etc. and install the required packages: If you meet problems installing , we advise you to check out the official repo to find a wheel. > Note: The pre-compiled packages strongly depend on the version of and its CUDA version. Moreover, due to recent update, > you may also encounter unsupported version errors from , , or . > We recommend using the latest versions meeting the following requirements: > - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1 > - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0 Then you can load the quantized model easily and run inference as same as usual: We illustrate the model performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below: | Quantization | MMLU | CEval val | GSM8K | Humaneval | |----------------------|:----:|:-----------:|:-----:|:---------:| | Qwen-1.8B-Chat BF16| 43.3 | 55.6 | 33.7 | 26.2 | | Qwen-1.8B-Chat Int8| 43.1 | 55.8 | 33.0 | 27.4 | | Qwen-1.8B-Chat Int4| 42.9 | 52.8 | 31.2 | 25.0 | | Qwen-7B-Chat BF16 | 55.8 | 59.7 | 50.3 | 37.2 | | Qwen-7B-Chat Int8 | 55.4 | 59.4 | 48.3 | 34.8 | | Qwen-7B-Chat Int4 | 55.1 | 59.2 | 49.7 | 29.9 | | Qwen-14B-Chat BF16 | 64.6 | 69.8 | 60.1 | 43.9 | | Qwen-14B-Chat Int8 | 63.6 | 68.6 | 60.0 | 48.2 | | Qwen-14B-Chat Int4 | 63.3 | 69.0 | 59.8 | 45.7 | | Qwen-72B-Chat BF16 | 74.4 | 80.1 | 76.4 | 64.6 | | Qwen-72B-Chat Int8 | 73.5 | 80.1 | 73.5 | 62.2 | | Qwen-72B-Chat Int4 | 73.4 | 80.1 | 75.3 | 61.6 | Quantization of KV cache > NOTE: Please be aware that due to the internal mechanism of Hugging Face, the support files for this functionality > i.e., and may be missing. Please manually download > them from the Hugging Face Hub and place them into the same folder as the other module files. The attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments and in are provided to enable KV cache quantization. The specific use method is as follows: Attention: Currently, KV cache quantization and flash attention cannot be used at the same time. If you enable KV cache quantization and flash attention at the same time , , , is disabled by default . We have verified that the use of the quantized Int8-KV-Cache model does not suffer from significant performance degradation in downstream evaluation. In the following, we focus on profiling its memory footprint in different conditions. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. We use BF16 models to generate 1024 tokens by default, and "OOM" indicates out-of-memory error. With KV cache quantization, the model can infer with a larger batch size bs. | USE KV Cache | bs=1 | bs=4 | bs=16 | bs=32 | bs=64 | bs=100 | |--------------|:------:|:------:|:------:|:------:|:------:|:------:| | No | 16.3GB | 24.1GB | 31.7GB | 48.7GB | OOM | OOM | | Yes | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB | With KV cache quantization the model can save more memory when generating longer sequence , sequence length, referring to the number of tokens generated at the stage of inference. | USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 | |--------------|:------:|:-------:|:-------:|:-------:|:-------:| | No | 15.2GB | 16.3GB | 17.6GB | 19.5GB | 23.2GB | | Yes | 15GB | 15.5GB | 15.8GB | 16.6GB | 17.6GB | The model with KV cache quantization will convert the format of from float to int8, and meanwhile the quantized will also store the quantization parameters. Specific steps are as follows: 1. Quantize key/value 2. Store into layerpast The following is the format of quantized : The original format of is shown below: If you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows: <br> Inference Performance This section provides the statistics of speed and memory of models in different precisions. The speed and memory profiling are conducted using this script. We measured the average inference speed tokens/s and GPU memory usage of generating 2048 with the models in BF16, Int8, and Int4. <table> <tr> <td>Model Size</td> <td>Quantization</td> <td>Speed Tokens/s</td> <td>GPU Memory Usage</td> </tr> <tr> <td rowspan="3">1.8B</td> <td>BF16</td> <td>54.09</td> <td>4.23GB</td> </tr> <tr> <td>Int8</td> <td>55.56</td> <td>3.48GB</td> </tr> <tr> <td>Int4</td> <td>71.07</td> <td>2.91GB</td> </tr> <tr> <td rowspan="3">7B</td> <td>BF16</td> <td>40.93</td> <td>16.99GB</td> </tr> <tr> <td>Int8</td> <td>37.47</td> <td>11.20GB</td> </tr> <tr> <td>Int4</td> <td>50.09</td> <td>8.21GB</td> </tr> <tr> <td rowspan="3">14B</td> <td>BF16</td> <td>32.22</td> <td>30.15GB</td> </tr> <tr> <td>Int8</td> <td>29.28</td> <td>18.81GB</td> </tr> <tr> <td>Int4</td> <td>38.72</td> <td>13.01GB</td> </tr> <tr> <td rowspan="3">72B</td> <td>BF16</td> <td>8.48</td> <td>144.69GB 2xA100</td> </tr> <tr> <td>Int8</td> <td>9.05</td> <td>81.27GB 2xA100</td> </tr> <tr> <td>Int4</td> <td>11.32</td> <td>48.86GB</td> </tr> <tr> <td>72B + vLLM</td> <td>BF16</td> <td>17.60</td> <td>2xA100</td> </tr> </table> The profiling runs on a single A100-SXM4-80G GPU except 2xA100 is mentioned with PyTorch 2.0.1, CUDA 11.8, and Flash-Attention 2. 72B + vLLM uses PyTorch 2.1.0 and Cuda 11.8. The inference speed is averaged over the encoded and generated tokens. Note: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using finetune.pypydantic<2.0$DATA--deepspeed--bf16 True--fp16 Truepeftmodulestosavempi4pypipconda.cpp.cupeft>=0.8.0trustremotecode=TrueValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.peft<0.8.0newmodeldirectory.cu.cpp--modelmaxlength.py.cu.cppgenerationconfig.jsonconfig.jsonQwen-7B-Chat--bits 4config.json--deepspeed finetune/dsconfigzero3.jsonfinetune/finetunelorads.shclidemo.py-c--cpu-onlystream=Falseqwenllm/qwen:cu117>= 515.48.07qwenllm/qwen:cu114>= 470.82.01qwenllm/qwen:cu121>= 530.30.02qwenllm/qwen:latestqwenllm/qwen:cu117http://localhost:$PORTdocker logs qwendocker rm -f qwendocker run: <br> üî• System Prompt Qwen-1.8-Chat and Qwen-72B-Chat have been fully trained on diverse system prompts with multiple rounds of complex interactions, so that they can follow a variety of system prompts and realize model customization in context, further improving the scalability of Qwen-chat. With System Prompt, Qwen-Chat can realize roly playing, language style transfer, task setting, and behavior setting. !assets/systempromptlanguagestyle.png !assets/systempromptroleplayen.png For more information, please refer to the example documentation. Tool Usage Qwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even augment Qwen with a Python Code Interpreter. We provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to the ReAct example. Based on this principle, we provide support for function calling in openaiapi.py. We have tested the model's tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well: <table> <tr> <th colspan="4" align="center">Chinese Tool-Use Benchmark Version 20231206</th> </tr> <tr> <th align="center">Model</th><th align="center">Tool Selection Acc.‚Üë</th><th align="center">Tool Input Rouge-L‚Üë</th><th align="center">False Positive Error‚Üì</th> </tr> <tr> <td>GPT-4</td><td align="center">98.0%</td><td align="center">0.953</td><td align="center">23.9%</td> </tr> <tr> <td>GPT-3.5</td><td align="center">74.5%</td><td align="center">0.807</td><td align="center">80.6%</td> </tr> <tr> <td>Qwen-18B-Chat</td><td align="center">85.0%</td><td align="center">0.839</td><td align="center">27.6%</td> </tr> <tr> <td>Qwen-7B-Chat</td><td align="center">95.5%</td><td align="center">0.900</td><td align="center">11.6%</td> </tr> <tr> <td>Qwen-14B-Chat</td><td align="center">96.9%</td><td align="center">0.917</td><td align="center">5.6%</td> </tr> <tr> <td>Qwen-72B-Chat</td><td align="center">98.2%</td><td align="center">0.927</td><td align="center">1.1%</td> </tr> </table> To assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this link. We have observed that Qwen performs well in terms of code executability and result accuracy when generating code: <table> <tr> <th colspan="5" align="center">Code Interpreter Benchmark Version 20231206</th> </tr> <tr> <th rowspan="2" align="center">Model</th> <th colspan="3" align="center">Accuracy of Code Execution Results %</th> <th colspan="1" align="center">Executable Rate of Code %</th> </tr> <tr> <th align="center">Math‚Üë</th><th align="center">Visualization-Hard‚Üë</th><th align="center">Visualization-Easy‚Üë</th><th align="center">General‚Üë</th> </tr> <tr> <td>GPT-4</td> <td align="center">82.8</td> <td align="center">66.7</td> <td align="center">60.8</td> <td align="center">82.8</td> </tr> <tr> <td>GPT-3.5</td> <td align="center">47.3</td> <td align="center">33.3</td> <td align="center">55.7</td> <td align="center">74.1</td> </tr> <tr> <td>LLaMA2-13B-Chat</td> <td align="center">8.3</td> <td align="center">1.2</td> <td align="center">15.2</td> <td align="center">48.3</td> </tr> <tr> <td>CodeLLaMA-13B-Instruct</td> <td align="center">28.2</td> <td align="center">15.5</td> <td align="center">21.5</td> <td align="center">74.1</td> </tr> <tr> <td>InternLM-20B-Chat</td> <td align="center">34.6</td> <td align="center">10.7</td> <td align="center">25.1</td> <td align="center">65.5</td> </tr> <tr> <td>ChatGLM3-6B</td> <td align="center">54.2</td> <td align="center">4.8</td> <td align="center">15.2</td> <td align="center">67.1</td> </tr> <tr> <td>Qwen-1.8B-Chat</td> <td align="center">25.6</td> <td align="center">21.4</td> <td align="center">22.8</td> <td align="center">65.5</td> </tr> <tr> <td>Qwen-7B-Chat</td> <td align="center">41.9</td> <td align="center">23.8</td> <td align="center">38.0</td> <td align="center">67.2</td> </tr> <tr> <td>Qwen-14B-Chat</td> <td align="center">58.4</td> <td align="center">31.0</td> <td align="center">45.6</td> <td align="center">65.5</td> </tr> <tr> <td>Qwen-72B-Chat</td> <td align="center">72.7</td> <td align="center">41.7</td> <td align="center">43.0</td> <td align="center">82.8</td> </tr> </table> <p align="center"> <br> <img src="assets/codeinterpretershowcase001.jpg" /> <br> <p> <br> Long-Context Understanding To extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-14B from 2K to over 8K tokens, and Qwen-1.8B/7B from 8K to 32K tokens. For Qwen-72B, we adapt RoPE to longer contexts with a larger rotary base. Qwen-72B supports the max context length of 32K tokens. We conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen can reach outstanding performance in the scenario of long context. Results are demonstrated below: <table> <tr> <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th> </tr> <tr> <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th> </tr> <tr> <td>Qwen-7B original</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td> </tr> <tr> <td>+ dynamicntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td> </tr> <tr> <td>+ dynamicntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td> </tr> <tr> <td>+ dynamicntk + logn + windowattn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td> </tr> <tr> <tr> <td>Qwen-1.8B</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.13</b></td><td align="center"><b>3.89</b></td><td align="center">17.42</td><td align="center">433.85</td> </tr> <tr> <td>+ dynamicntk + logn + windowattn</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.14</b></td><td align="center"><b>3.93</b></td><td align="center"><b>3.82</b></td><td align="center"><b>3.83</b></td> </tr> <tr> <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td> </tr> <tr> <td>+ dynamicntk + logn + windowattn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td> </tr> <tr> <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td> </tr> <tr> <td>+ dynamicntk + logn + windowattn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td> </tr> <tr> <td>Qwen-72B</td><td align="center"><b>-</b></td><td align="center"><b>-</b></td><td align="center">-</td><td align="center"><b>2.83</b></td><td align="center"><b>2.73</b></td><td align="center"><b>2.72</b></td> </tr> </tr> </table> Furthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on L-Eval closed-ended tasks. The results are as follows: | Model | Input Length | Average | Coursera | GSM | QuALITY | TOEFL | CodeU | SFcition | |:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:| | ChatGPT-3.5-16k | 16K | 60.73 | 63.51 | 84.00 | 61.38 | 78.43 | 12.22 | 64.84 | | Qwen-72B-Chat | 32K | 62.30 | 58.13 | 76.00 | 77.22 | 86.24 | 6.66 | 69.53 | We conducted the "needle in a haystack" experiment the idea came from @Greg Kamradt to test whether the model can retrieve information at different positions in the inputs of different lengths, the result is as follows: !assets/qwen72bneedleinahaystack.png The above results show that Qwen-72B-Chat can accurately retrieve information placed in various positions within an input length of 32k, proving its excellent long text understanding capabilities. Tokenizer Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the documentation. <br><br> Reproduction For your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check eval/EVALUATION.md for more information. Note that the reproduction may lead to slight differences from our reported results. <br><br> FAQ If you meet problems, please refer to FAQ and the issues first to search a solution before you launch a new issue. <br><br> Citation If you find our work helpful, feel free to give us a cite. <br> License Agreement The source code provided at <https://github.com/QwenLM/Qwen> is licensed under the Apache 2.0 License that can be found at the root directory. Researchers and developers are free to use the codes and model weights of both Qwen and Qwen-Chat. For their commercial use, please check the License Agreement accompanying each model. - Qwen-72B, Qwen-14B, and Qwen-7B are licensed under the Tongyi Qianwen LICENSE AGREEMENT that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please fill out the form 72B, 14B, and 7B to apply. - Qwen-1.8B is licensed under the Tongyi Qianwen RESEARCH LICENSE AGREEMENT that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please contact us. <br><br> Contact Us If you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwenopensource@alibabacloud.com.