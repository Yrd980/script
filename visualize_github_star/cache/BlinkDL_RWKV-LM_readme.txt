RWKV: Parallelizable RNN with Transformer-level LLM Performance pronounced as "RwaKuv" rʌkuv in IPA, from 4 major params: R W K V RWKV website: https://rwkv.com with 90+ RWKV-related papers RWKV twitter: https://twitter.com/BlinkDLAI lastest news RWKV discord: https://discord.gg/bDSBUMeFpc 9k+ members RWKV-7 "Goose" is the strongest linear-time & constant-space no kv-cache & attention-free & 100% RNN architecture on this planet at this moment, suitable for LLM and multimodal applications and more see rwkv.com. IMPORTANT: Use PreLN LayerNorm instead of RMSNorm for RWKV. I think it's related to better initial state, because I am not using trainable initial state found it useless when using LayerNorm. RWKV-7 is a meta-in-context learner, test-time-training its state on the context via in-context gradient descent at every token. RWKV is a Linux Foundation AI project, so totally free. RWKV runtime is already in Windows & Office. You are welcome to ask the RWKV community such as RWKV discord for advice on upgrading your attention/ssm models to rwkv7 models : === Please use https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7/traintemp as RWKV-7 reference implementation. The default config only requires 1 GPU with 10G VRAM you can reduce bsz if you have less VRAM, so it's easy to test. Note FLA RWKV-7 is NOT aligned with reference implementation yet, and you will get less performance. This is because RWKV-7 is the whole model with carefully set stuffs, including different init / wd / lr for each parameter, so it's readily scalable and very stable spike-free. But the price to pay is there is no good simple "RWKV-7 layer" because a pytorch layer can't make sure itself is using correct init and hyperparameters. So if you need to use RWKV-7 for another task, please study traintemp code only several hundred lines and change it to suit you. === RWKV-7 can do math. See https://github.com/BlinkDL/RWKV-LM/blob/main/Research/rwkv7-g0-7.2b.md for details. <img width="555" height="784" alt="image" src="https://github.com/user-attachments/assets/095b4576-962f-4274-ae1a-855406ec76c1" /> History of RWKV from v1 to v7: https://wiki.rwkv.com/advance/architecture.html note: AI-written. might contain errors <img src="RWKV-v7-niah.png"> Gradio Demo 1: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1 Gradio Demo 2: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2 WebGPU Demo: https://cryscan.github.io/web-rwkv-puzzles//chat Latest RWKV weights: https://huggingface.co/BlinkDL === RWKV-Runner GUI: https://github.com/josStorer/RWKV-Runner/releases Ai00 Server: https://github.com/Ai00-X/ai00server RWKV pip pkg: https://pypi.org/project/rwkv/ PEFT Lora etc.: https://github.com/JL-er/RWKV-PEFT RLHF: https://github.com/OpenMOSE/RWKV-LM-RLHF 400+ RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories Faster RWKV-7 kernels: https://github.com/johanwind/windrwkv === RWKV-5/6 Eagle/Finch paper: https://arxiv.org/abs/2404.05892 Chat demo code: https://github.com/BlinkDL/ChatRWKV/blob/main/APIDEMOCHAT.py RWKV-7 demo code: https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7 https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkvv7demo.py GPT-like mode https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkvv7demornn.py RNN mode https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkvv7demofast.py Both mode, fastest RWKV-6 demo code: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/rwkvv6demo.py RWKV-6 demo code: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVv6demo.py HOW TO TRAIN RWKV-7/6/5 on MiniPile 1.5G tokens For reference, use python 3.10+, torch 2.5+, cuda 12.5+, latest deepspeed, but keep pytorch-lightning==1.9.5 Train RWKV-7: RWKV-7 weight example for 1.5B L24-D2048, vocab 65536: | name | shape | comment | initialization | |---------------------|---------------|--------------|-----------------| | emb.weight | 65536, 2048 | wdecay | see code | | blocks.0.ln0.weight | 2048 | for layer 0 | 1 | | blocks.0.ln0.bias | 2048 | for layer 0 | 0 | | | | | | | blocks..ln1.weight | 2048 | | 1 | | blocks..ln1.bias | 2048 | | 0 | | blocks..att.xr | 1, 1, 2048 | | see code | | blocks..att.xw | 1, 1, 2048 | | see code | | blocks..att.xk | 1, 1, 2048 | | see code | | blocks..att.xv | 1, 1, 2048 | | see code | | blocks..att.xa | 1, 1, 2048 | | see code | | blocks..att.xg | 1, 1, 2048 | | see code | | blocks..att.w0 | 1, 1, 2048 | lr 2x | see code | | blocks..att.w1 | 2048, 96 | | 0 | | blocks..att.w2 | 96, 2048 | | see code | | blocks..att.a0 | 1, 1, 2048 | | 0 | | blocks..att.a1 | 2048, 96 | | 0 | | blocks..att.a2 | 96, 2048 | | see code | | blocks..att.v0 | 1, 1, 2048 | for layer 1+ | 1 | | blocks..att.v1 | 2048, 64 | for layer 1+ | 0 | | blocks..att.v2 | 64, 2048 | for layer 1+ | see code | | blocks..att.g1 | 2048, 256 | | 0 | | blocks..att.g2 | 256, 2048 | | see code | | blocks..att.kk | 1, 1, 2048 | | 1 | | blocks..att.ka | 1, 1, 2048 | | 1 | | blocks..att.rk | 32, 64 | | 0 | | blocks..att.receptance.weight | 2048, 2048 | wdecay | see code | | blocks..att.key.weight | 2048, 2048 | wdecay | see code | | blocks..att.value.weight | 2048, 2048 | wdecay | see code | | blocks..att.output.weight | 2048, 2048 | wdecay | 0 | | blocks..att.lnx.weight | 2048 | | see code | | blocks..att.lnx.bias | 2048 | | 0 | | | | | | | blocks..ln2.weight | 2048 | | 1 | | blocks..ln2.bias | 2048 | | 0 | | blocks..ffn.xk | 1, 1, 2048 | | see code | | blocks..ffn.key.weight | 8192, 2048 | wdecay | see code | | blocks..ffn.value.weight | 2048, 8192 | wdecay | 0 | | | | | | | lnout.weight | 2048 | | 1 | | lnout.bias | 2048 | | 0 | | head.weight | 65536, 2048 | wdecay | see code | Train RWKV-6: use /RWKV-v5/ and use --mytesting "x060" in demo-training-prepare.sh and demo-training-run.sh Your loss curve should look almost exactly the same as this, with the same ups and downs if you use the same bsz & config: <img src="RWKV-v5-minipile.png" width="500"> You can run your model using https://pypi.org/project/rwkv/ use "rwkvvocabv20230424" instead of "20Btokenizer.json" Use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/makedata.py to prepare binidx data from jsonl, and compute "--myexittokens" and "--magicprime". Use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/computemagicprime.py to compute "--myexittokens" and "--magicprime" for existing binidx. Much faster tokenizer of large data: https://github.com/cahya-wirawan/json2bin https://github.com/cahya-wirawan/rwkv-tokenizer https://github.com/m8than/RWKV-World-Tokenizer-CPP The "epoch" in train.py is "mini-epoch" not real epoch. only for convenience, and 1 mini-epoch = 40320 ctxlen tokens. For example, if your binidx has 1498226207 tokens and ctxlen=4096, set "--myexittokens 1498226207" this will override epochcount, and it will be 1498226207/40320 4096 = 9.07 miniepochs. The trainer will auto-exit after "--myexittokens" tokens. Set "--magicprime" to the largest 3n+2 prime smaller than datalen/ctxlen-1 = 1498226207/4096-1 = 365776, which is "--magicprime 365759" in this case. simple: prepare SFT jsonl => repeat your SFT data 3 or 4 times in makedata.py. more repetition leads to overfitting. advanced: repeat your SFT data 3 or 4 times in your jsonl note makedata.py will shuffle all jsonl items => add some base data such as slimpajama to your jsonl => and only repeat 1 times in makedata.py. Fix training spikes: see the "Fixing RWKV-6 Spikes" part on this page. Or use RWKV-7 much better. RWKV-7 is very stable and spike-free verified for 0.1/0.4/1.5/2.9b: <img src="RWKV-v7-loss.png" width="500"> Simple inference for RWKV-6: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVv6demo.py Simple inference for RWKV-5: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVv5demo.py Note: In state = kv + w state everything must be in fp32 because w can be very close to 1. So we can keep state and w in fp32, and convert kv to fp32. lmeval: https://github.com/BlinkDL/ChatRWKV/blob/main/runlmeval.py Tips for small model / small data: When I train RWKV music models, I use deep & narrow such as L29-D512 dimensions, and apply wd and dropout such as wd=2 dropout=0.02. Note RWKV-LM dropout is very effective - use 1/4 of your usual value. HOW TO TRAIN RWKV-7 on Pile 332G tokens See https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/demo-training-prepare-v7-pile.sh and https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/demo-training-run-v7-pile.sh Get these files first: pile20Btokenizertextdocument.bin 664230651068 bytes pile20Btokenizertextdocument.idx 4212099722 bytes HOW TO FINETUNE RWKV-5 MODELS Use .jsonl format for your data see https://huggingface.co/BlinkDL/rwkv-5-world for formats. Use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/makedata.py to tokenizer it using World tokenizer into binidx, suitable for finetuning World models. Rename the base checkpoint in your model folder to rwkv-init.pth, and change the training commands to use --nlayer 32 --nembd 4096 --vocabsize 65536 --lrinit 1e-5 --lrfinal 1e-5 for 7B. 0.1B = --nlayer 12 --nembd 768 // 0.4B = --nlayer 24 --nembd 1024 // 1.5B = --nlayer 24 --nembd 2048 // 3B = --nlayer 32 --nembd 2560 // 7B = --nlayer 32 --nembd 4096 State-tuning tuning the initial state. zero inference overhead Currently unoptimized implementation, takes same vram as full SFT use rwkv 0.8.26+ to auto-load the trained "timestate" Initializing RWKV 5/6 Models When you train RWKV from scratch, try my initialization for best performance. Check generateinitweight of src/model.py: !!! If you are using positional embedding, maybe it's better to remove block.0.ln0 and use default initialization for emb.weight instead of my uniforma=-1e-4, b=1e-4 !!! Fixing RWKV-6 Spikes 0. upgrade to RWKV-7. It's very stable. 1. when training from scratch, add "k = k torch.clampw, max=0.exp" before "RUNCUDARWKV6r, k, v, w, u", and remember to change your inference code too. you will see faster convergence. 2. use "--adameps 1e-18" 3. "--beta2 0.95" if you see spikes 4. in trainer.py do "lr = lr 0.01 + 0.99 trainer.globalstep / wstep" originally 0.2 + 0.8, and "--warmupsteps 20" 5. "--weightdecay 0.1" leads to better final loss if you are training lots of data. set lrfinal to 1/100 of lrinit when doing this. Introducing RWKV RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer parallelizable. And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the "GPT" mode to quickly compute the hidden state for the "RNN" mode. So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctxlen, and free sentence embedding using the final hidden state. All latest RWKV weights: https://huggingface.co/BlinkDL HF-compatible RWKV weights: https://huggingface.co/RWKV nanoRWKV: https://github.com/BlinkDL/nanoRWKV does not require custom CUDA kernel to train, works for any GPU/CPU Cool Community RWKV Projects: All 400+ RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories https://github.com/OpenGVLab/Vision-RWKV Vision RWKV https://github.com/feizc/Diffusion-RWKV Diffusion RWKV https://github.com/cgisky1980/ai00rwkvserver Fastest WebGPU inference nVidia/AMD/Intel https://github.com/cryscan/web-rwkv backend for ai00rwkvserver https://github.com/saharNooby/rwkv.cpp Fast CPU/cuBLAS/CLBlast inference: int4/int8/fp16/fp32 https://github.com/JL-er/RWKV-PEFT lora/pissa/Qlora/Qpissa/state tuning https://github.com/RWKV/RWKV-infctx-trainer Infctx trainer https://github.com/daquexian/faster-rwkv https://github.com/mlc-ai/mlc-llm/pull/1275 https://github.com/TheRamU/Fay/blob/main/READMEEN.md Digital Assistant with RWKV https://github.com/harrisonvanderbyl/rwkv-cpp-cuda Fast GPU inference with cuda/amd/vulkan RWKV v6 in 250 lines with tokenizer too: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVv6demo.py RWKV v5 in 250 lines with tokenizer too: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVv5demo.py RWKV v4 in 150 lines model, inference, text generation: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKVin150lines.py RWKV v4 preprint https://arxiv.org/abs/2305.13048 RWKV v4 introduction, and in 100 lines of numpy: https://johanwind.github.io/2023/03/23/rwkvoverview.html https://johanwind.github.io/2023/03/23/rwkvdetails.html !RWKV-7 !MQAR !RWKV-paper RWKV v6 illustrated: !RWKV-v6 !RWKV-v5-benchmark-1 A cool paper Spiking Neural Network using RWKV: https://github.com/ridgerchu/SpikeGPT You are welcome to join the RWKV discord https://discord.gg/bDSBUMeFpc to build upon it. We have plenty of potential compute A100 40Gs now thanks to Stability and EleutherAI, so if you have interesting ideas I can run them. !RWKV-eval2 RWKV loss vs token position for 10000 ctx4k+ documents in Pile. RWKV 1B5-4k is mostly flat after ctx1500, but 3B-4k and 7B-4k and 14B-4k have some slopes, and they are getting better. This debunks the old view that RNNs cannot model long ctxlens. We can predict that RWKV 100B will be great, and RWKV 1T is probably all you need : !RWKV-ctxlen ChatRWKV with RWKV 14B ctx8192: !RWKV-chat I believe RNN is a better candidate for fundamental models, because: 1 It's more friendly for ASICs no kv cache. 2 It's more friendly for RL. 3 When we write, our brain is more similar to RNN. 4 The universe is like an RNN too because of locality. Transformers are non-local models. RWKV-3 1.5B on A40 tf32 = always 0.015 sec/token, tested using simple pytorch code no CUDA, GPU utilization 45%, VRAM 7823M GPT2-XL 1.3B on A40 tf32 = 0.032 sec/token for ctxlen 1000, tested using HF, GPU utilization 45% too interesting, VRAM 9655M Training speed: new training code RWKV-4 14B BF16 ctxlen4096 = 114K tokens/s on 8x8 A100 80G ZERO2+CP. old training code RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G. I am doing image experiments too For example: https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder and RWKV will be able to do txt2img diffusion : My idea: 256x256 rgb image -> 32x32x13bit latents -> apply RWKV to compute transition probability for each of the 32x32 grid -> pretend the grids are independent and "diffuse" using these probabilities. Smooth training - no loss spikes! lr & bsz change around 15G tokens !RWKV-loss !RWKV-eval All of the trained models will be open-source. Inference is very fast only matrix-vector multiplications, no matrix-matrix multiplications even on CPUs, so you can even run a LLM on your phone. How it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It's very simple once you understand it. RWKV is parallelizable because the time-decay of each channel is data-independent and trainable. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 these are called "gates", while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN then you can use outputs of later layers of the previous token if you want extra performance. !RWKV-formula Here are some of my TODOs. Let's work together : HuggingFace integration check https://github.com/huggingface/transformers/issues/17230 , and optimized CPU & iOS & Android & WASM & WebGL inference. RWKV is a RNN and very friendly for edge devices. Let's make it possible to run a LLM on your phone. Test it on bidirectional & MLM tasks, and image & audio & video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of decoder previous hidden state & encoder final hidden state. Hence all decoder tokens will have access to the encoder output. Now training RWKV-4a with one single tiny extra attention just a few extra lines comparing with RWKV-4 to further improve some difficult zeroshot tasks such as LAMBADA for smaller models. See https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829 User feedback: > I've so far toyed around the character-based model on our relatively small pre-training dataset around 10GB of text, and the results are extremely good - similar ppl to models taking much, much longer to train. > dear god rwkv is fast. i switched to another tab after starting training it from scratch & when i returned it was emitting plausible english & maori words, i left to go microwave some coffee & when i came back it was producing fully grammatically correct sentences. Tweet from Sepp Hochreiter thank you!: https://twitter.com/HochreiterSepp/status/1524270961314484227 You can find me BlinkDL in the EleutherAI Discord too: https://www.eleuther.ai/get-involved/ !RWKV-demo Quick start IMPORTANT: Use deepspeed==0.7.0 pytorch-lightning==1.9.5 torch==1.13.1+cu117 and cuda 11.7.1 or 11.7 note torch2 + deepspeed has weird bugs and hurts model performance Use https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo latest code, compatible with v4. Here is a great prompt for testing Q&A of LLMs. Works for any model: found by minimizing ChatGPT ppls for RWKV 1.5B Inference Run RWKV-4 Pile models: Download models from https://huggingface.co/BlinkDL. Set TOKENMODE = 'pile' in run.py and run it. It's fast even on CPU the default mode. Colab for RWKV-4 Pile 1.5B: https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM Run RWKV-4 Pile models in your browser and onnx version: see this issue https://github.com/BlinkDL/RWKV-LM/issues/7 RWKV-4 Web Demo: https://josephrocca.github.io/rwkv-v4-web/demo/ note: only greedy sampling for now For the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPCdev. Run run.py in https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN. You can even run it in your browser: https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng https://blinkdl.github.io/AI-Writer/eng/ this is using tf.js WASM single-thread mode. Training / Fine-tuning pip install deepspeed==0.7.0 // pip install pytorch-lightning==1.9.5 // torch 1.13.1+cu117 NOTE: add weight decay 0.1 or 0.01 and dropout 0.1 or 0.01 when training on small amt of data. try x=x+dropoutattx x=x+dropoutffnx x=dropoutx+attx x=dropoutx+ffnx etc. Training RWKV-4 from scratch: run train.py, which by default is using the enwik8 dataset unzip https://data.deepai.org/enwik8.zip. You will be training the "GPT" version because it's paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens. Fine-tuning RWKV-4 Pile models: use 'prepare-data.py' in https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3 to tokenize .txt into train.npy data. Then use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/train.py to train it. Read the inference code in src/model.py and try using the final hidden state（.xx .aa .bb as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb .aa divided by .bb. Colab for fine-tuning RWKV-4 Pile models: https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKVv4RNNPileFineTuning.ipynb Large corpus: Use https://github.com/Abel2076/json2binidxtool to convert .jsonl into .bin and .idx The jsonl format sample one line for each document: generated by code like this: Infinite ctxlen training WIP: https://github.com/Blealtan/RWKV-LM-LoRA/tree/dev-infctx How to use RWKV hidden state as text embedding Consider RWKV 14B. The state has 200 vectors, that is, 5 vectors for each block: fp16 xx, fp32 aa, fp32 bb, fp32 pp, fp16 xx. Do not avg pool because different vectors xx aa bb pp xx in the state have very different meanings and ranges. You can probably remove pp. I suggest firstly collect the mean+stdev statistics of each channel of each vector, and normalize all of them note: the normalization should be data-indepedent and collected from various texts. Then train a linear classifer. Towards RWKV-5 just to record some new ideas Lastest Design RWKV-5 is multi-head and here shows one head. There is also a LayerNorm for each head hence actually GroupNorm. $$ $$ $$ RWKV-6 Dynamic Mix & Dynamic Decay. Example do this for both TimeMix & ChannelMix: !RWKV-v6 RWKV-7 Use parallelized mode to quickly generate the state, then use a finetuned full RNN the layers of token n can use outputs of all layer of token n-1 for sequential generation. Some old ideas 1. Now time decay is like 0.999^T 0.999 is learnable. Change it to something like 0.999^T + 0.1 where 0.1 is learnable too. The 0.1 part will be kept forever. Or, A^T + B^T + C = fast-decay + slow-decay + constant. Can even use different formulas for example, K^2 instead of e^K for a decay component, or, without normalization. 2. Use complex-valued decay so, rotation instead of decay in some channels. 3. Inject some trainable and extrapolatable positional encoding? 4. Aside from 2d rotation, we can try other Lie groups such as 3d rotation SO3 . Non-abelian RWKV lol. 5. RWKV might be great on analog devices search for Analog Matrix-vector multiplication & Photonic Matrix-vector multiplication. The RNN mode is very hardware-friendly processing-in-memory. Can be a SNN too https://github.com/ridgerchu/SpikeGPT. I wonder if it can be optimized for quantum computation. 6. Trainable initial hidden state xx aa bb pp xx. 7. Layerwise or even row/column-wise, elementwise LR, and test Lion optimizer. Vision Tasks 1. I find it's good to add a 2d pos encoding: 2. In a BPE langauge model, it's the best to use tokenShift of 1 token you can mix more tokens in a char-level English model. However you can try tokenShift of N or N-1 or N+1 tokens if the image size is N x N, because that will be like mixing the token above the current positon or the token above the to-be-predicted positon with current token. You can use try different tokenShift styles for "ATT" & "FFN", or mixing different tokenShift styles - such as mixing token A with token A-1 and token A-N-1 etc. Misc Maybe we can improve memorization by simply repeating the context I guess 2 times is enough. Example: Reference -> Referenceagain -> Question -> Answer Idea: Bytes-aware Embedding The idea is to make sure each token in vocab understand its length and raw UTF-8 bytes. Let a = maxlentoken for all token in vocab. Define AA : floatademb Let b = maxleninutf8bytestoken for all token in vocab. Define BB : floatb256demb For each token X in vocab, let x0, x1, ..., xn be its raw UTF-8 bytes. We will add some extra values to its embedding EMBX: EMBX += AAlenX + BB0x0 + BB1x1 + ... + BBnxn note: AA BB are learnable weights We can do this for the final Lineardemb, nvocab projection too. We can use some small networks to generate AA and BB, for some extra regularization for example, BBmxi and BBnxi should be related. Old Idea I have an idea to improve tokenization. We can hardcode some channels to have meanings. Example: Channel 0 = "space" Channel 1 = "capitalize first letter" Channel 2 = "capitalize all letters" Therefore: Embedding of "abc": 0, 0, 0, x0, x1, x2 , .. Embedding of " abc": 1, 0, 0, x0, x1, x2, .. Embedding of " Abc": 1, 1, 0, x0, x1, x2, .. Embedding of "ABC": 0, 0, 1, x0, x1, x2, ... ...... so they will share most of the embedding. And we can rapidly compute the output probability of all variations of "abc". Note: the above method is assuming that p" xyz" / p"xyz" is the same for any "xyz", which can be wrong. Better: define embspace embcapitalizefirst embcapitalizeall to be a function of emb. Maybe the Best: let 'abc' ' abc' etc. to share the last 90% of their embeddings. At this moment, all our tokenizers spend too many items to represent all variations of 'abc' ' abc' ' Abc' etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. The method here can improve this. I plan to test this in a new version of RWKV. Idea: Better Initial States Example single-round Q & A: 1. Generate the final state of all wiki documents. 2. For any user Q, find the best wiki document, and use its final state as the initial state. 3. Train a model to directly generate the optimal initial state for any user Q. However this can be a bit more tricky for multi-round Q & A : How it works RWKV is inspired by Apple's AFT https://arxiv.org/abs/2105.14103. Moreover it's using a number of my tricks, such as: SmallInitEmb: https://github.com/BlinkDL/SmallInitEmb applicable to all transformers which helps the embedding quality, and stabilizes Post-LN which is what I am using. Token-shift: https://github.com/BlinkDL/RWKV-LMtoken-shift-time-shift-mixing applicable to all transformers, especially helpful for char-level models. Head-QK: https://github.com/BlinkDL/RWKV-LMthe-head-qk-trick-learning-to-copy-and-avoid-tokens applicable to all transformers. Note: it's helpful, but I disabled it in the Pile model to keep it 100% RNN. Extra R-gate in the FFN applicable to all transformers. I am also using reluSquared from Primer. Better initilization: I init most of the matrices to ZERO see RWKVInit in https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py. You can transfer some parameters from a small model to a large model note: I sort & smooth them too, for faster and better convergence see https://www.reddit.com/r/MachineLearning/comments/umq908/rrwkvv2rnnaparallelizablernnwith/. My CUDA kernel: https://github.com/BlinkDL/RWKV-CUDA to speedup training. The pseudocode execution from top to bottom: !RWKV-v2-RNN The a b c d factors work together to build a time-decay curve: X, 1, W, W^2, W^3, .... Write out the formulas for "token at pos 2" and "token at pos 3" and you will get the idea: a and b: EMAs of kv and k. c and d: these are a and b combined with "self-attention". kv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel. The R-gate is important for performance. k = info strength of this token to be passed to future tokens. r = whether to apply the info to this token. RWKV-3 improvements Use different trainable TimeMix factors for R / K / V in SA and FF layers. Example: Use preLN instead of postLN more stable & faster convergence: Explaining the code for RWKV-3 GPT mode The GPT mode - overview The building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT. The only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training. It is important to initialize emb to tiny values, such as nn.init.uniforma=-1e-4, b=1e-4, to utilize my trick https://github.com/BlinkDL/SmallInitEmb. For the 1.5B RWKV-3, I use Adam no wd, no dropout optimizer on 8 A100 40G. batchSz = 32 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small. For the first 15B tokens, LR is fixed at 3e-4, and beta=0.9, 0.99. Then I set beta=0.9, 0.999, and do an exponential decay of LR, reaching 1e-5 at 332B tokens. The GPT mode - ATT block The RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway. The self.key, self.receptance, self.output matrices are all initialized to zero. The timemix, timedecay, timefirst vectors are transferred from a smaller trained model note: I sort & smooth them too. The GPT mode - FFN block The FFN block has three tricks comparing with the usual GPT: 1. My timemix trick. 2. The sqReLU from the Primer paper. 3. An extra receptance-gate similar to the receptance-gate in ATT block. The self.value, self.receptance matrices are all initialized to zero. RWKV-4 improvements !RWKV-v3-plan From GPT to RWKV the formulas Let Ft be the system state at t. Let xt be the new external input at t. In GPT, predicting Ft+1 requires considering F0, F1, .. Ft. So it takes OT^2 to generate a length T sequence. The simplified formula for GPT: !F\mathrmt+1=\frac\sum\mathrmi=0^\mathrmt \exp \mathbfQx\mathrmt \mathbfKF\mathrmi \cdot\mathbfVF\mathrmi\sum\mathrmi=0^\mathrmt \exp \mathbfQx\mathrmt \mathbfKF\mathrmihttps://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D It's very capable in theory, however that does not mean we can fully utilize its capability with usual optimizers. I suspect the loss landscape is too difficult for our current methods. Compare with the simplified formula for RWKV the parallel mode, looks similar to Apple's AFT: !F\mathrmt+1=\sigma\mathbfRx\mathrmt \cdot \frac\sum\mathrmi=0^\mathrmt \exp \mathbfW \cdot\mathrmt-\mathrmi \cdot \exp \mathbfKF\mathrmi \cdot\mathbfVF\mathrmi\sum\mathrmi=0^\mathrmt \exp \mathbfW \cdot\mathrmt-\mathrmi \cdot \exp \mathbfK F\mathrmihttps://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D The R, K, V are trainable matrices, and W is a trainable vector time-decay factor for each channel. In GPT, the contribution of Fi to Ft+1 is weighted by ! \exp \mathbfQx\mathrmt \mathbfKF\mathrmi https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+. In RWKV-2, the contribution of Fi to Ft+1 is weighted by !\sigma\mathbfRx\mathrmt \cdot \exp \mathbfW \cdot\mathrmt-\mathrmi \cdot \exp \mathbfKF\mathrmi https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+. The !\sigma is a non-linearity and we can use sigmoid. Note !\sigma\mathbfRx\mathrmthttps://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29 is not in the denominator, and I call R the "receptance". The !\exp \mathbfW \cdot\mathrmt-\mathrmi is the time-decay factor. I proposed the same idea scaling the attention by distance in Aug 2020 and called it the "time-weighting" check the commit history of https://github.com/BlinkDL/minGPT-tuned. Here comes the punchline: we can rewrite it into a RNN recursive formula. Note: !F1=\sigma\mathbfR x0 \cdot \frac \exp \mathbfK F0 \cdot\mathbfV F0\exp \mathbfK F0https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D !F2=\sigma\mathbfR x1 \cdot \frac \exp \mathbfK F1 \cdot\mathbfV F1+\exp \mathbfW \cdot \exp \mathbfK F0 \cdot\mathbfV F0 \exp \mathbfK F1+\exp \mathbfW \cdot \exp \mathbfK F0https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D Therefore it's straightforward to verify: !Ft+1=\sigma\mathbfR xt \cdot \frac\exp \mathbfKF\mathrmt \cdot\mathbfVF\mathrmt+\exp \mathbfW \cdot A\mathrmt \exp \mathbfKF\mathrmt+\exp \mathbfW \cdot B\mathrmthttps://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D where At and Bt are the numerator and denominator of the previous step, respectively. I believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note P^-1 D P^n = P^-1 D^n P, so it is similar to repeatedly applying a general diagonalizable matrix. Moreover it's possible to turn it into a continuous ODE a bit similar to State Space Models. I will write about it later. Star History !Star History Charthttps://star-history.com/BlinkDL/RWKV-LM&Date Multimodal ideas I have an idea for text --> 32x32 RGB image using a LM transformer, RWKV, etc.. Will test it soon. Firstly, LM loss instead of L2 loss, so the image will not be blurry. Secondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 for each pixel, instead of 2^24. Therefore, a 32x32 RGB image = a len1024 sequence of vocab512 image tokens, which is a typical input for usual LMs. Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too. Thirdly, 2D positional embeddings that are easy for the model to understand. For example, add one-hot X & Y coords to the first 64=32+32 channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 =32+20. Moreover probably we can add the float X & Y coords normalized to 0~1 range to another 2 channels. And other periodic pos. encoding might help too will test. Finally, RandRound when doing the color quantization in the DataLoader. For example, if the float level is 4.578, then there is a 57.8% chance to use 5, and 1-57.8% chance to use 4. And we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4. Multi-task training might help too. I will try this dataset format: TxtFirst Desc of Img txt tokens Img img tokens and sometimes ImgFirst img tokens Txt Desc of Img txt tokens ... the order of the imgs should be randomized in the DataLoader, and TxtFirst ImgFirst Img Txt are special tokens and do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a img -> txt task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images. How to sample a large dataset for training I am using a trick to sample the Pile deterministically yet randomly enough. Let's say the pile has x chunks a chunk = ctxlen tokens. pick a prime number p just less than x, and make sure p = 2 mod 3. Use step step step mod p to sample it. Add some bias to step for extra randomness. The top-p-x sampling method for inference We propose a new sampling method called top-p-x: it's like top-p, and the only difference is you also keep all tokens whose prob > x. Try x = 0.01 first. Better Learning Rate Schedule via Variantional Method of Loss Curve I propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics phenomenology w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy. UPDATE: In "Conclusion 1.", use the best-fitting regime ignore the initial steps where our approximations break down to fit the parameters. Try this: fixed lr for 1 hr, then exponential decay to 0.2 lr in 12 hrs, and choose the t=1hr, 13hr segment. In the last three plots, black = predicted loss curve of the new LR schedule, blue = original unoptimized real loss curve, orange = new LR schedule. !betterlrschedule RWKV v1 We propose the RWKV language model, with alternating time-mix and channel-mix layers: <img src= "https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum%7Bu%7D+%26%26%5Ctextbf%7BW%7D%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7Dt%28%5Ctext%7BK%7D%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csumd+%26%26%5Ctextbf%7BW%7D%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A" alt="\beginalign \textTime-mix : && \textTMt,c &&=&&\textsigmoid\textRt,c &&\cdot&& &&\textstyle\sumu &&\textbfWt,u,c &&\cdot&& \textsoftmaxt\textKu,c &&\cdot&& \textVu,c\\ \textChannel-mix : && \textCMt,c &&=&&\textsigmoid\textRt,c &&\cdot&& &&\textstyle\sumd &&\textbfWc,d &&\cdot&& \textgelu\textKt,d &&\cdot&& \textVt,d \endalign "> The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into Rtarget Wsrc, target Ksrc. So we can call R "receptance", and sigmoid means it's in 0~1 range. The Time-mix is similar to AFT https://arxiv.org/abs/2105.14103. There are two differences. 1 We changed the normalization denominator. For masked language models, we define: <img src= "https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7Dt%28%5Ctext%7BK%7D%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D%7Bu%2Cc%7D%29%7D%7B%5Csum%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D%7Bv%2Cc%7D%29%7D" alt="\textsoftmaxt\textKu,c = \frac\exp\textKu,c\sumv \leq t\exp\textKv,c"> UPDATE: We are using the original AFT normalization in v2 Initialize K and R matrices and the output projection matrix to ZERO for fast & stable convergence. 2 We decompose Wt,u,c and introduce multi-head W here h is the corresponding head of c: <img src= "https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W%7Bt%2Cu%2Cc%7D%3Dfh%28t-u%29%5Ccdot+%5Calphah%28u%29+%5Ccdot+%5Cbetah%28t%29" alt="Wt,u,c=fht-u\cdot \alphahu \cdot \betaht"> Moreover we multiply the final output of Time-mix layer by γt. The reason for the α β γ factors, is because the context size is smaller when t is small, and this can be compensated using the α β γ factors. UPDATE: We remove α β γ factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN The Channel-mix is similar to GeGLU https://arxiv.org/abs/2002.05202 with an extra R factor. Initialize R and W matrices to ZERO for fast & stable convergence. Finally, we add extra token-shift time-shift mixing as in https://github.com/BlinkDL/minGPT-tuned. Token-shift time-shift mixing The token-shift explicitly uses half the channels of this token & half the channels of prev token to generate all vectors QKV, RWKV, .... Dividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM. However for BPE-level English LM, it's only effective if your embedding is large enough at least 1024 - so the usual small L12-D768 model is not enough. My theory on the effectiveness of token-shift: When we train a GPT, the hidden representation of a token has to accomplish two different objects: 1. Predict the next token. Sometimes this is easy obvious next token. 2. Collect all previous context info, so later tokens can use it. This is always hard. The shifted channels can focus on 2, so we have good propagation of info. It's like some kind of residual connection, or a small RNN inside the transformer. You can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers. p.s. There is a MHApro model in this repo with strong performance. Give it a try : The Head-QK Trick: learning to copy and avoid tokens In usual transformer, a small model has difficulty copying tokens such as person names in the context. We add extra Q & K to the final output such that the model can directly copy or avoid tokens in the context. Afterwards the model will teach itself NER named entity recognition if you look at the learned weights. Note: when a token occurs multiple times in the context, it might be better to use maxprob instead of sumprob. The top-a sampling method We also propose a new sampling method called top-a as in src/utils.py: 1 Find the max probability pmax after softmax. 2 Remove all entries whose probability is lower than 0.2 powpmax, 2. So it's adaptive, hence "top-a". 3 Feel free to tune the 0.2 and 2 factor. Tune 0.2 first. The idea of top-a: 1. If maxprob=0.9, then remove all tokens with prob < 0.162 so, removing all alternatives 2. If maxprob=0.5, then remove all tokens with prob < 0.05 so, allowing more choices 3. If maxprob=0.1, then remove all tokens with prob < 0.002 so, allowing lots of possibilities Performance Character-level loss on simplebooks-92 dataset https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip !RWKV-vs-MHA Gray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params. Red: RWKV "linear" attention - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params. Green: MHA+Rotary+GeGLU+Tokenshift. 17.2M params. Blue: MHApro MHA with various tweaks & RWKV-type-FFN - slow - needs more VRAM - good performance. 16.6M params. Initialization We use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special timew curves. Check model.py for details. Some learned timew examples: !RWKV-time-w