rust-bert !Build Statushttps://github.com/guillaume-be/rust-bert/actions !Latest versionhttps://crates.io/crates/rustbert !Documentationhttps://docs.rs/rust-bert !License Rust-native state-of-the-art Natural Language Processing models and pipelines. Port of Hugging Face's Transformers library, using tch-rs or onnxruntime bindings and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads see below and ready-to-use pipelines. Benchmarks are available at the end of this document. Get started with tasks including question answering, named entity recognition, translation, summarization, text generation, conversational agents and more in just a few lines of code: Output: The tasks currently supported include: - Translation - Summarization - Multi-turn dialogue - Zero-shot classification - Sentiment Analysis - Named Entity Recognition - Part of Speech tagging - Question-Answering - Language Generation - Masked Language Model - Sentence Embeddings - Keywords extraction <details> <summary> <b>Expand to display the supported models/tasks matrix </b> </summary> | | Sequence classification | Token classification | Question answering | Text Generation | Summarization | Translation | Masked LM | Sentence Embeddings | |:------------:|:---------------------------:|:------------------------:|:----------------------:|:-------------------:|:-----------------:|:---------------:|:-------------:|:-----------------------:| | DistilBERT | ✅ | ✅ | ✅ | | | | ✅ | ✅ | | MobileBERT | ✅ | ✅ | ✅ | | | | ✅ | | | DeBERTa | ✅ | ✅ | ✅ | | | | ✅ | | | DeBERTa v2 | ✅ | ✅ | ✅ | | | | ✅ | | | FNet | ✅ | ✅ | ✅ | | | | ✅ | | | BERT | ✅ | ✅ | ✅ | | | | ✅ | ✅ | | RoBERTa | ✅ | ✅ | ✅ | | | | ✅ | ✅ | | GPT | | | | ✅ | | | | | | GPT2 | | | | ✅ | | | | | | GPT-Neo | | | | ✅ | | | | | | GPT-J | | | | ✅ | | | | | | BART | ✅ | | | ✅ | ✅ | | | | | Marian | | | | | | ✅ | | | | MBart | ✅ | | | ✅ | | | | | | M2M100 | | | | ✅ | | | | | | NLLB | | | | ✅ | | | | | | Electra | | ✅ | | | | | ✅ | | | ALBERT | ✅ | ✅ | ✅ | | | | ✅ | ✅ | | T5 | | | | ✅ | ✅ | ✅ | | ✅ | | LongT5 | | | | ✅ | ✅ | | | | | XLNet | ✅ | ✅ | ✅ | ✅ | | | ✅ | | | Reformer | ✅ | | ✅ | ✅ | | | ✅ | | | ProphetNet | | | | ✅ | ✅ | | | | | Longformer | ✅ | ✅ | ✅ | | | | ✅ | | | Pegasus | | | | | ✅ | | | | </details> Getting started This library relies on the tch crate for bindings to the C++ Libtorch API. The libtorch library is required can be downloaded either automatically or manually. The following provides a reference on how to set-up your environment to use these bindings, please refer to the tch for detailed information or support. Furthermore, this library relies on a cache folder for downloading pre-trained models. This cache location defaults to , but can be changed by setting the environment variable. Note that the language models used by this library are in the order of the 100s of MBs to GBs. Manual installation recommended 1. Download from https://pytorch.org/get-started/locally/. This package requires : if this version is no longer available on the "get started" page, the file should be accessible by modifying the target link, for example for a Linux version with CUDA12. NOTE: When using as dependency from crates.io, please check the required on the published package readme as it may differ from the version documented here applying to the current repository version. 2. Extract the library to a location of your choice 3. Set the following environment variables Linux: Windows macOS + Homebrew Automatic installation Alternatively, you can let the script automatically download the library for you. The feature flag needs to be enabled. The CPU version of libtorch will be downloaded by default. To download a CUDA version, please set the environment variable to . Note that the libtorch library is large order of several GBs for the CUDA-enabled version and the first build may therefore take several minutes to complete. Verifying installation Verify your installation and linking with libtorch by adding the dependency to your or by cloning the rust-bert source and running an example: ONNX Support Optional ONNX support can be enabled via the optional feature. This crate then leverages the ort crate with bindings to the onnxruntime C++ library. We refer the user to this page project for further installation instructions/support. 1. Enable the optional feature. The crate does not include any optional dependencies for , the end user should select the set of features that would be adequate for pulling the required C++ library. 2. The current recommended installation is to use dynamic linking by pointing to an existing library location. Use the cargo feature for . 3. set the to point to the location of downloaded onnxruntime library // depending on the operating system. These can be downloaded from the release page of the onnxruntime project Most architectures including encoders, decoders and encoder-decoders are supported. the library aims at keeping compatibility with models exported using the Optimum library. A detailed guide on how to export a Transformer model to ONNX using Optimum is available at https://huggingface.co/docs/optimum/main/en/exporters/onnx/usageguides/exportamodel The resources used to create ONNX models are similar to those based on Pytorch, replacing the pytorch by the ONNX model. Since ONNX models are less flexible than their Pytorch counterparts in the handling of optional arguments, exporting a decoder or encoder-decoder model to ONNX will usually result in multiple files. These files are expected but not all are necessary for use in this library as per the table below: | Architecture | Encoder file | Decoder without past file | Decoder with past file | |-----------------------------|--------------|---------------------------|------------------------| | Encoder e.g. BERT | required | not used | not used | | Decoder e.g. GPT2 | not used | required | optional | | Encoder-decoder e.g. BART | required | required | optional | Note that the computational efficiency will drop when the file is optional but not provided since the model will not used cached past keys and values for the attention mechanism, leading to a high number of redundant computations. The Optimum library offers export options to ensure such a model file is created. The base encoder and decoder model architecture are available and exposed for convenience in the and modules, respectively. Generation models pure decoder or encoder/decoder architectures are available in the module. ost pipelines are available for ONNX model checkpoints, including sequence classification, zero-shot classification, token classification including named entity recognition and part-of-speech tagging, question answering, text generation, summarization and translation. These models use the same configuration and tokenizer files as their Pytorch counterparts when used in a pipeline. Examples leveraging ONNX models are given in the directory Ready-to-use pipelines Based on Hugging Face's pipelines, ready to use end-to-end NLP pipelines are available as part of this crate. The following capabilities are currently available: Disclaimer The contributors of this repository are not responsible for any generation from the 3rd party utilization of the pretrained systems proposed herein. <details> <summary> <b>1. Question Answering</b> </summary> Extractive question answering from a given question and context. DistilBERT model fine-tuned on SQuAD Stanford Question Answering Dataset Output: </details> &nbsp; <details> <summary> <b>2. Translation </b> </summary> Translation pipeline supporting a broad range of source and target languages. Leverages two main architectures for translation tasks: - Marian-based models, for specific source/target combinations - M2M100 models allowing for direct translation between 100 languages at a higher computational cost and lower performance for some selected languages Marian-based pretrained models for the following language pairs are readily available in the library - but the user can import any Pytorch-based model for predictions - English <-> French - English <-> Spanish - English <-> Portuguese - English <-> Italian - English <-> Catalan - English <-> German - English <-> Russian - English <-> Chinese - English <-> Dutch - English <-> Swedish - English <-> Arabic - English <-> Hebrew - English <-> Hindi - French <-> German For languages not supported by the proposed pretrained Marian models, the user can leverage a M2M100 model supporting direct translation between 100 languages without intermediate English translation The full list of supported languages is available in the crate documentation Output: </details> &nbsp; <details> <summary> <b>3. Summarization </b> </summary> Abstractive summarization using a pretrained BART model. example from: WikiNews Output: </details> &nbsp; <details> <summary> <b>4. Dialogue Model </b> </summary> Conversation model based on Microsoft's DialoGPT. This pipeline allows the generation of single or multi-turn conversations between a human and a model. The DialoGPT's page states that > The human evaluation results indicate that the response generated from > DialoGPT is comparable to human response quality under a single-turn > conversation Turing test. > DialoGPT repository The model uses a to keep track of active conversations and generate responses to them. Example output: </details> &nbsp; <details> <summary> <b>5. Natural Language Generation </b> </summary> Generate language based on a prompt. GPT2 and GPT available as base models. Include techniques such as beam search, top-k and nucleus sampling, temperature setting and repetition penalty. Supports batch generation of sentences from several prompts. Sequences will be left-padded with the model's padding token if present, the unknown token otherwise. This may impact the results, it is recommended to submit prompts of similar length for best results Example output: </details> &nbsp; <details> <summary> <b>6. Zero-shot classification </b> </summary> Performs zero-shot classification on input sentences with provided labels using a model fine-tuned for Natural Language Inference. Output: </details> &nbsp; <details> <summary> <b>7. Sentiment analysis </b> </summary> Predicts the binary sentiment for a sentence. DistilBERT model fine-tuned on SST-2. Example courtesy of IMDb Output: </details> &nbsp; <details> <summary> <b>8. Named Entity Recognition </b> </summary> Extracts entities Person, Location, Organization, Miscellaneous from text. BERT cased large model fine-tuned on CoNNL03, contributed by the MDZ Digital Library team at the Bavarian State Library. Models are currently available for English, German, Spanish and Dutch. Output: </details> &nbsp; <details> <summary> <b>9. Keywords/keyphrases extraction</b> </summary> Extract keywords and keyphrases extractions from input documents Output: </details> &nbsp; <details> <summary> <b>10. Part of Speech tagging </b> </summary> Extracts Part of Speech tags Noun, Verb, Adjective... from text. Output: </details> &nbsp; <details> <summary> <b>11. Sentence embeddings </b> </summary> Generate sentence embeddings vector representation. These can be used for applications including dense information retrieval. Output: </details> &nbsp; <details> <summary> <b>12. Masked Language Model </b> </summary> Predict masked words in input sentences. Output: </details> Benchmarks For simple pipelines sequence classification, tokens classification, question answering the performance between Python and Rust is expected to be comparable. This is because the most expensive part of these pipeline is the language model itself, sharing a common implementation in the Torch backend. The End-to-end NLP Pipelines in Rust provides a benchmarks section covering all pipelines. For text generation tasks summarization, translation, conversation, free text generation, significant benefits can be expected up to 2 to 4 times faster processing depending on the input and application. The article Accelerating text generation with Rust focuses on these text generation applications and provides more details on the performance comparison to Python. Loading pretrained and custom model weights The base model and task-specific heads are also available for users looking to expose their own transformer based models. Examples on how to prepare the date using a native tokenizers Rust library are available in for BERT, DistilBERT, RoBERTa, GPT, GPT2 and BART. Note that when importing models from Pytorch, the convention for parameters naming needs to be aligned with the Rust schema. Loading of the pre-trained weights will fail if any of the model parameters weights cannot be found in the weight files. If this quality check is to be skipped, an alternative method can be invoked from the variables store. Pretrained models are available on Hugging face's model hub and can be loaded using defined in this library. A conversion utility script is included in to convert Pytorch weights to a set of weights compatible with this library. This script requires Python and to be set-up, and can be used as follows: where is the location of the original Pytorch weights. Citation If you use for your work, please cite End-to-end NLP Pipelines in Rust: Acknowledgements Thank you to Hugging Face for hosting a set of weights compatible with this Rust library. The list of ready-to-use pretrained models is listed at https://huggingface.co/models?filter=rust.