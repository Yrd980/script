<p align="center"> </p> <h1 align="center">Hercules</h1> <p align="center"> Fast, insightful and highly customizable Git history analysis.<br><br> <a href="http://godoc.org/gopkg.in/src-d/hercules.v10"><img src="https://godoc.org/gopkg.in/src-d/hercules.v10?status.svg" alt="GoDoc"></a> <a href="https://travis-ci.com/src-d/hercules"><img src="https://travis-ci.com/src-d/hercules.svg?branch=master" alt="Travis build Status"></a> <a href="https://ci.appveyor.com/project/vmarkovtsev/hercules"><img src="https://ci.appveyor.com/api/projects/status/49f0lm3v2y6xyph3?svg=true" alt="AppVeyor build status"></a> <a href="https://pypi.python.org/pypi/labours"><img src="https://img.shields.io/pypi/v/labours.svg" alt="PyPi package status"></a> <a href="https://hub.docker.com/r/srcd/hercules"><img src="https://img.shields.io/docker/build/srcd/hercules.svg" alt="Docker build status"></a> <a href="https://codecov.io/gh/src-d/hercules"><img src="https://codecov.io/github/src-d/hercules/coverage.svg" alt="Code coverage"></a> <a href="https://goreportcard.com/report/github.com/src-d/hercules"><img src="https://goreportcard.com/badge/github.com/src-d/hercules" alt="Go Report Card"></a> <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="Apache 2.0 license"></a> </p> <p align="center"> <a href="overview">Overview</a> • <a href="usage">How To Use</a> • <a href="installation">Installation</a> • <a href="contributions">Contributions</a> • <a href="license">License</a> </p> -------- Table of Contents ================= Overview Installation Build from source GitHub Action Contributions License Usage Caching GitHub Action Docker image Built-in analyses Project burndown Files People Churn matrix Code ownership Couples Structural hotness Aligned commit series Added vs changed lines through time Efforts through time Sentiment positive and negative comments Everything in a single pass Plugins Merging Bad unicode errors Plotting Custom plotting backend Caveats Burndown Out-Of-Memory Roadmap Overview Hercules is an amazingly fast and highly customizable Git repository analysis engine written in Go. Batteries are included. Powered by go-git. Notice November 2020: the main author is back from the limbo and is gradually resuming the development. See the roadmap. There are two command-line tools: and . The first is a program written in Go which takes a Git repository and executes a Directed Acyclic Graph DAG of analysis tasks over the full commit history. The second is a Python script which shows some predefined plots over the collected data. These two tools are normally used together through a pipe. It is possible to write custom analyses using the plugin system. It is also possible to merge several analysis results together - relevant for organizations. The analyzed commit history includes branches, merges, etc. Hercules has been successfully used for several internal projects at sourced. There are blog posts: 1, 2 and a presentation. Please contribute by testing, fixing bugs, adding new analyses, or coding swagger! !Hercules DAG of Burndown analysis <p align="center">The DAG of burndown and couples analyses with UAST diff refining. Generated with <code>hercules --burndown --burndown-people --couples --feature=uast --dry-run --dump-dag doc/dag.dot https://github.com/src-d/hercules</code></p> !git/git image <p align="center">torvalds/linux line burndown granularity 30, sampling 30, resampled by year. Generated with <code>hercules --burndown --first-parent --pb https://github.com/torvalds/linux | labours -f pb -m burndown-project</code> in 1h 40min.</p> Installation Grab binary from the Releases page. is installable from PyPi: https://pip.pypa.io/en/stable/installing/ is the Python package manager. Numpy and Scipy can be installed on Windows using http://www.lfd.uci.edu/~gohlke/pythonlibs/ Build from source You are going to need Go >= v1.11 and https://github.com/google/protobuf/releases. GitHub Action It is possible to run Hercules as a GitHub Action: Hercules on GitHub Marketplace. Please refer to the sample workflow which demonstrates how to setup. Contributions ...are welcome! See CONTRIBUTING and code of conduct. License Apache 2.0 Usage The most useful and reliably up-to-date command line reference: Some examples: allows to read the output from which was saved on disk. Caching It is possible to store the cloned repository on disk. The subsequent analysis can run on the corresponding directory instead of cloning from scratch: GitHub Action The action produces the artifact named . Since it is currently impossible to pack several files in one artifact, all the charts and Tensorflow Projector files are packed in the inner tar archive. In order to view the embeddings, go to projector.tensorflow.org, click "Load" and choose the two TSVs. Then use UMAP or T-SNE. Docker image Built-in analyses Project burndown Line burndown statistics for the whole repository. Exactly the same what git-of-theseus does but much faster. Blaming is performed efficiently and incrementally using a custom RB tree tracking algorithm, and only the last modification date is recorded while running the analysis. All burndown analyses depend on the values of granularity and sampling. Granularity is the number of days each band in the stack consists of. Sampling is the frequency with which the burnout state is snapshotted. The smaller the value, the more smooth is the plot but the more work is done. There is an option to resample the bands inside , so that you can define a very precise distribution and visualize it different ways. Besides, resampling aligns the bands across periodic boundaries, e.g. months or years. Unresampled bands are apparently not aligned and start from the project's birth date. Files Burndown statistics for every file in the repository which is alive in the latest revision. Note: it will generate separate graph for every file. You don't want to run it on repository with many files. People Burndown statistics for the repository's contributors. If is not specified, the identities are discovered by the following algorithm: 0. We start from the root commit towards the HEAD. Emails and names are converted to lower case. 1. If we process an unknown email and name, record them as a new developer. 2. If we process a known email but unknown name, match to the developer with the matching email, and add the unknown name to the list of that developer's names. 3. If we process an unknown email but known name, match to the developer with the matching name, and add the unknown email to the list of that developer's emails. If is specified, it should point to a text file with the custom identities. The format is: every line is a single developer, it contains all the matching emails and names separated by . The case is ignored. Overwrites matrix !Wireshark top 20 overwrites matrix <p align="center">Wireshark top 20 devs - overwrites matrix</p> Beside the burndown information, collects the added and deleted line statistics per developer. Thus it can be visualized how many lines written by developer A are removed by developer B. This indicates collaboration between people and defines expertise teams. The format is the matrix with N rows and N+2 columns, where N is the number of developers. 1. First column is the number of lines the developer wrote. 2. Second column is how many lines were written by the developer and deleted by unidentified developers if is not specified, it is always 0. 3. The rest of the columns show how many lines were written by the developer and deleted by identified developers. The sequence of developers is stored in YAML node. Code ownership !Ember.js top 20 code ownership <p align="center">Ember.js top 20 devs - code ownership</p> also allows to draw the code share through time stacked area plot. That is, how many lines are alive at the sampled moments in time for each identified developer. Couples !Linux kernel file couples <p align="center">torvalds/linux files' coupling in Tensorflow Projector</p> Important: it requires Tensorflow to be installed, please follow official instructions. The files are coupled if they are changed in the same commit. The developers are coupled if they change the same file. records the number of couples throughout the whole commit history and outputs the two corresponding co-occurrence matrices. then trains Swivel embeddings - dense vectors which reflect the co-occurrence probability through the Euclidean distance. The training requires a working Tensorflow installation. The intermediate files are stored in the system temporary directory or if it is specified. The trained embeddings are written to the current working directory with the name depending on . The output format is TSV and matches Tensorflow Projector so that the files and people can be visualized with t-SNE implemented in TF Projector. Structural hotness Thanks to Babelfish, hercules is able to measure how many times each structural unit has been modified. By default, it looks at functions; refer to Semantic UAST XPath manual to switch to something else. Couples analysis automatically loads "shotness" data if available. !Jinja2 functions grouped by structural hotness <p align="center"><code>hercules --shotness --pb https://github.com/pallets/jinja | labours -m couples -f pb</code></p> Aligned commit series !tensorflow/tensorflow <p align="center">tensorflow/tensorflow aligned commit series of top 50 developers by commit number.</p> We record how many commits made, as well as lines added, removed and changed per day for each developer. We plot the resulting commit time series using a few tricks to show the temporal grouping. In other words, two adjacent commit series should look similar after normalization. 1. We compute the distance matrix of the commit series. Our distance metric is Dynamic Time Warping. We use FastDTW algorithm which has linear complexity proportional to the length of time series. Thus the overall complexity of computing the matrix is quadratic. 2. We compile the linear list of commit series with Seriation technique. Particularly, we solve the Travelling Salesman Problem which is NP-complete. However, given the typical number of developers which is less than 1,000, there is a good chance that the solution does not take much time. We use Google or-tools solver. 3. We find 1-dimensional clusters in the resulting path with HDBSCAN algorithm and assign colors accordingly. 4. Time series are smoothed by convolving with the Slepian window. This plot allows to discover how the development team evolved through time. It also shows "commit flashmobs" such as Hacktoberfest. For example, here are the revealed insights from the plot above: 1. "Tensorflow Gardener" is classified as the only outlier. 2. The "blue" group of developers covers the global maintainers and a few people who left at the top. 3. The "red" group shows how core developers join the project or become less active. Added vs changed lines through time !tensorflow/tensorflow <p align="center">tensorflow/tensorflow added and changed lines through time.</p> from the previous section allows to plot how many lines were added and how many existing changed deleted or replaced through time. This plot is smoothed. Efforts through time !kubernetes/kubernetes <p align="center">kubernetes/kubernetes efforts through time.</p> Besides, allows to plot how many lines have been changed added or removed by each developer. The upper part of the plot is an accumulated integrated lower part. It is impossible to have the same scale for both parts, so the lower values are scaled, and hence there are no lower Y axis ticks. There is a difference between the efforts plot and the ownership plot, although changing lines correlate with owning lines. Sentiment positive and negative comments !Django sentiment <p align="center">It can be clearly seen that Django comments were positive/optimistic in the beginning, but later became negative/pessimistic.<br><code>hercules --sentiment --pb https://github.com/django/django | labours -m sentiment -f pb</code></p> We extract new and changed comments from source code on every commit, apply BiDiSentiment general purpose sentiment recurrent neural network and plot the results. Requires libtensorflow. E.g. https://github.com/pygame/pygame/commit/b6091d38c8a5639d311858660b38841d96598509diff-eae59f175858fcef57cb17e733981c73R27 is negative and https://github.com/keras-team/keras/commit/7d52af64c03e71bcd23112a7086dc8aab1b37ed2diff-ff634bb5c5441d7052449f20018872b8R549 is positive. Don't expect too much though - as was written, the sentiment model is general purpose and the code comments have different nature, so there is no magic for now. Hercules must be built with "tensorflow" tag - it is not by default: Such a build requires https://www.tensorflow.org/install/installgo. Everything in a single pass Plugins Hercules has a plugin system and allows to run custom analyses. See PLUGINS.md. Merging is the command which joins several analysis results in Protocol Buffers format together. Bad unicode errors YAML does not support the whole range of Unicode characters and the parser on side may raise exceptions. Filter the output from through to discard such offending characters. Plotting These options affects all plots: sets the general style of the plot see . changes the plot background to be either white or black. chooses the Matplotlib backend. sets the size of the figure in inches. The default is . required in macOS you can pin the default Matplotlib backend with These options are effective in burndown charts only: changes the font size, activate the stretched burndown layout. Custom plotting backend It is possible to output all the information needed to draw the plots in JSON format. Simply append to the output and you are done. The data format is not fully specified and depends on the Python code which generates it. Each JSON file should contain which reflects the plot kind. Caveats 1. Processing all the commits may fail in some rare cases. If you get an error similar to https://github.com/src-d/hercules/issues/106 please report there and specify as a workaround. 1. Burndown collection may fail with an Out-Of-Memory error. See the next session for the workarounds. 1. Parsing YAML in Python is slow when the number of internal objects is big. ' output for the Linux kernel in "couples" mode is 1.5 GB and takes more than an hour / 180GB RAM to be parsed. However, most of the repositories are parsed within a minute. Try using Protocol Buffers instead and . 1. To speed up yaml parsing Burndown Out-Of-Memory If the analyzed repository is big and extensively uses branching, the burndown stats collection may fail with an OOM. You should try the following: 1. Read the repo from disk instead of cloning into memory. 2. Use to avoid analyzing the unwanted files. It is also possible to constrain the . 3. Use the hibernation feature: . Play with those two numbers to start hibernating right before the OOM. 4. Hibernate on disk: . 5. , you win. Roadmap Switch from to . Upgrade the codebase to be compatible with the latest Go version. Update the docs regarding the copyrights and such. Fix the reported bugs. Remove the dependency on Babelfish for parsing the code. It is abandoned and a better alternative should be found. Remove the ad-hoc analyses added while sourced was agonizing.