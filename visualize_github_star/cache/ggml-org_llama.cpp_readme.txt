llama.cpp !llama !License: MIThttps://opensource.org/licenses/MIT !Releasehttps://github.com/ggml-org/llama.cpp/releases !Serverhttps://github.com/ggml-org/llama.cpp/actions/workflows/server.yml Manifesto / ggml / ops LLM inference in C/C++ Recent API changes - Changelog for API - Changelog for REST API Hot topics - Support for the model with native MXFP4 format has been added | PR | Collaboration with NVIDIA | Comment - Hot PRs: All | Open - Multimodal support arrived in : 12898 | documentation - VS Code extension for FIM completions: https://github.com/ggml-org/llama.vscode - Vim/Neovim plugin for FIM completions: https://github.com/ggml-org/llama.vim - Introducing GGUF-my-LoRA https://github.com/ggml-org/llama.cpp/discussions/10123 - Hugging Face Inference Endpoints now support GGUF out of the box! https://github.com/ggml-org/llama.cpp/discussions/9669 - Hugging Face GGUF editor: discussion | tool ---- Quick start Getting started with llama.cpp is straightforward. Here are several ways to install it on your machine: - Install using brew, nix or winget - Run with Docker - see our Docker documentation - Download pre-built binaries from the releases page - Build from source by cloning this repository - check out our build guide Once installed, you'll need a model to work with. Head to the Obtaining and quantizing models section to learn more. Example command: Description The main goal of is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud. - Plain C/C++ implementation without any dependencies - Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks - AVX, AVX2, AVX512 and AMX support for x86 architectures - 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use - Custom CUDA kernels for running LLMs on NVIDIA GPUs support for AMD GPUs via HIP and Moore Threads GPUs via MUSA - Vulkan and SYCL backend support - CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity The project is the main playground for developing new features for the ggml library. <details> <summary>Models</summary> Typically finetunes of the base models below are supported as well. Instructions for adding support for new models: HOWTO-add-model.md Text-only - X LLaMA ü¶ô - x LLaMA 2 ü¶ôü¶ô - x LLaMA 3 ü¶ôü¶ôü¶ô - X Mistral 7B - x Mixtral MoE - x DBRX - X Falcon - X Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2 - X Vigogne French - X BERT - X Koala - X Baichuan 1 & 2 + derivations - X Aquila 1 & 2 - X Starcoder models - X Refact - X MPT - X Bloom - x Yi models - X StableLM models - x Deepseek models - x Qwen models - x PLaMo-13B - x Phi models - x PhiMoE - x GPT-2 - x Orion 14B - x InternLM2 - x CodeShell - x Gemma - x Mamba - x Grok-1 - x Xverse - x Command-R models - x SEA-LION - x GritLM-7B + GritLM-8x7B - x OLMo - x OLMo 2 - x OLMoE - x Granite models - x GPT-NeoX + Pythia - x Snowflake-Arctic MoE - x Smaug - x Poro 34B - x Bitnet b1.58 models - x Flan T5 - x Open Elm models - x ChatGLM3-6b + ChatGLM4-9b + GLMEdge-1.5b + GLMEdge-4b - x GLM-4-0414 - x SmolLM - x EXAONE-3.0-7.8B-Instruct - x FalconMamba Models - x Jais - x Bielik-11B-v2.3 - x RWKV-6 - x QRWKV-6 - x GigaChat-20B-A3B - X Trillion-7B-preview - x Ling models - x LFM2 models Multimodal - x LLaVA 1.5 models, LLaVA 1.6 models - x BakLLaVA - x Obsidian - x ShareGPT4V - x MobileVLM 1.7B/3B models - x Yi-VL - x Mini CPM - x Moondream - x Bunny - x GLM-EDGE - x Qwen2-VL </details> <details> <summary>Bindings</summary> - Python: ddh0/easy-llama - Python: abetlen/llama-cpp-python - Go: go-skynet/go-llama.cpp - Node.js: withcatai/node-llama-cpp - JS/TS llama.cpp server client: lgrammel/modelfusion - JS/TS Programmable Prompt Engine CLI: offline-ai/cli - JavaScript/Wasm works in browser: tangledgroup/llama-cpp-wasm - Typescript/Wasm nicer API, available on npm: ngxson/wllama - Ruby: yoshoku/llamacpp.rb - Rust more features: edgenai/llamacpp-rs - Rust nicer API: mdrokz/rust-llama.cpp - Rust more direct bindings: utilityai/llama-cpp-rs - Rust automated build from crates.io: ShelbyJenkins/llmclient - C/.NET: SciSharp/LLamaSharp - C/VB.NET more features - community license: LM-Kit.NET - Scala 3: donderom/llm4s - Clojure: phronmophobic/llama.clj - React Native: mybigday/llama.rn - Java: kherud/java-llama.cpp - Zig: deins/llama.cpp.zig - Flutter/Dart: netdur/llamacppdart - Flutter: xuegao-tzx/Fllama - PHP API bindings and features built on top of llama.cpp: distantmagic/resonance more info - Guile Scheme: guilellamacpp - Swift srgtuszy/llama-cpp-swift - Swift ShenghaiWang/SwiftLlama - Delphi Embarcadero/llama-cpp-delphi </details> <details> <summary>UIs</summary> to have a project listed here, it should clearly state that it depends on - AI Sublime Text plugin MIT - cztomsik/ava MIT - Dot GPL - eva MIT - iohub/collama Apache-2.0 - janhq/jan AGPL - johnbean393/Sidekick MIT - KanTV Apache-2.0 - KodiBot GPL - llama.vim MIT - LARS AGPL - Llama Assistant GPL - LLMFarm MIT - LLMUnity MIT - LMStudio proprietary - LocalAI MIT - LostRuins/koboldcpp AGPL - MindMac proprietary - MindWorkAI/AI-Studio FSL-1.1-MIT - Mobile-Artificial-Intelligence/maid MIT - Mozilla-Ocho/llamafile Apache-2.0 - nat/openplayground MIT - nomic-ai/gpt4all MIT - ollama/ollama MIT - oobabooga/text-generation-webui AGPL - PocketPal AI MIT - psugihara/FreeChat MIT - ptsochantaris/emeltal MIT - pythops/tenere AGPL - ramalama MIT - semperai/amica MIT - withcatai/catai MIT - Autopen GPL </details> <details> <summary>Tools</summary> - akx/ggify ‚Äì download PyTorch models from HuggingFace Hub and convert them to GGML - akx/ollama-dl ‚Äì download models from the Ollama library to be used directly with llama.cpp - crashr/gppm ‚Äì launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption - gpustack/gguf-parser - review/check the GGUF file and estimate the memory usage - Styled Lines proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example </details> <details> <summary>Infrastructure</summary> - Paddler - Open-source LLMOps platform for hosting and scaling AI in your own infrastructure - GPUStack - Manage GPU clusters for running LLMs - llamacppcanister - llama.cpp as a smart contract on the Internet Computer, using WebAssembly - llama-swap - transparent proxy that adds automatic model switching with llama-server - Kalavai - Crowdsource end to end LLM deployment at any scale - llmaz - ‚ò∏Ô∏è Easy, advanced inference platform for large language models on Kubernetes. </details> <details> <summary>Games</summary> - Lucy's Labyrinth - A simple maze game where agents controlled by an AI model will try to trick you. </details> Supported backends | Backend | Target devices | | --- | --- | | Metal | Apple Silicon | | BLAS | All | | BLIS | All | | SYCL | Intel and Nvidia GPU | | MUSA | Moore Threads GPU | | CUDA | Nvidia GPU | | HIP | AMD GPU | | Vulkan | GPU | | CANN | Ascend NPU | | OpenCL | Adreno GPU | | WebGPU In Progressdocs/build.mdwebgpu | All | | RPC | All | Obtaining and quantizing models The Hugging Face platform hosts a number of LLMs compatible with : - Trending - LLaMA You can either manually download the GGUF file or directly use any -compatible models from Hugging Face or other model hosting sites, such as ModelScope, by using this CLI argument: . For example: By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable . For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. . After downloading a model, use the CLI tools to run it locally - see below. requires the model to be stored in the GGUF file format. Models in other data formats can be converted to GGUF using the Python scripts in this repo. The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with : - Use the GGUF-my-repo space to convert to GGUF format and quantize model weights to smaller sizes - Use the GGUF-my-LoRA space to convert LoRA adapters to GGUF format more info: https://github.com/ggml-org/llama.cpp/discussions/10123 - Use the GGUF-editor space to edit GGUF meta data in the browser more info: https://github.com/ggml-org/llama.cpp/discussions/9268 - Use the Inference Endpoints to directly host in the cloud more info: https://github.com/ggml-org/llama.cpp/discussions/9669 To learn more about model quantization, read this documentation tools/main A CLI tool for accessing and experimenting with most of 's functionality. - <details open> <summary>Run in conversation mode</summary> Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding and specifying a suitable chat template with </details> - <details> <summary>Run in conversation mode with custom chat template</summary> </details> - <details> <summary>Run simple text completion</summary> To disable conversation mode explicitly, use </details> - <details> <summary>Constrain the output with a custom grammar</summary> The grammars/ folder contains a handful of sample grammars. To write your own, check out the GBNF Guide. For authoring more complex JSON grammars, check out https://grammar.intrinsiclabs.ai/ </details> tools/server A lightweight, OpenAI API compatible, HTTP server for serving LLMs. - <details open> <summary>Start a local HTTP server with default configuration on port 8080</summary> </details> - <details> <summary>Support multiple-users and parallel decoding</summary> </details> - <details> <summary>Enable speculative decoding</summary> </details> - <details> <summary>Serve an embedding model</summary> </details> - <details> <summary>Serve a reranking model</summary> </details> - <details> <summary>Constrain all outputs with a grammar</summary> </details> tools/perplexity A tool for measuring the perplexity ^1 and other quality metrics of a model over a given text. - <details open> <summary>Measure the perplexity over a text file</summary> </details> - <details> <summary>Measure KL divergence</summary> </details> ^1: https://huggingface.co/docs/transformers/perplexity tools/llama-bench Benchmark the performance of the inference for various parameters. - <details open> <summary>Run default benchmark</summary> </details> tools/run A comprehensive example for running models. Useful for inferencing. Used with RamaLama ^3. - <details> <summary>Run a model with a specific prompt by default it's pulled from Ollama registry</summary> </details> ^3: RamaLama examples/simple A minimal example for implementing apps with . Useful for developers. - <details> <summary>Basic text completion</summary> </details> Contributing - Contributors can open PRs - Collaborators can push to branches in the repo and merge PRs into the branch - Collaborators will be invited based on contributions - Any help with managing issues, PRs and projects is very appreciated! - See good first issues for tasks suitable for first contributions - Read the CONTRIBUTING.md for more information - Make sure to read this: Inference at the edge - A bit of backstory for those who are interested: Changelog podcast Other documentation - main cli - server - GBNF grammars Development documentation - How to build - Running on Docker - Build on Android - Performance troubleshooting - GGML tips & tricks Seminal papers and background on the models If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT: - LLaMA: - Introducing LLaMA: A foundational, 65-billion-parameter large language model - LLaMA: Open and Efficient Foundation Language Models - GPT-3 - Language Models are Few-Shot Learners - GPT-3.5 / InstructGPT / ChatGPT: - Aligning language models to follow instructions - Training language models to follow instructions with human feedback XCFramework The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example: The above example is using an intermediate build of the library. This can be modified to use a different version by changing the URL and checksum. Completions Command-line completion is available for some environments. Bash Completion Optionally this can be added to your or to load it automatically. For example: Dependencies - yhirose/cpp-httplib - Single-header HTTP server, used by - MIT license - stb-image - Single-header image format decoder, used by multimodal subsystem - Public domain - nlohmann/json - Single-header JSON library, used by various tools/examples - MIT License - minja - Minimal Jinja parser in C++, used by various tools/examples - MIT License - linenoise.cpp - C++ library that provides readline-like line editing capabilities, used by - BSD 2-Clause License - curl - Client-side URL transfer library, used by various tools/examples - CURL License - miniaudio.h - Single-header audio format decoder, used by multimodal subsystem - Public domain