<div align="center"> ðŸ’¥ Flash Linear Attention !hfmodelhttps://huggingface.co/fla-hub !Discordhttps://discord.gg/vDaJTmKNcS </div> This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. All implementations are written purely in PyTorch and Triton, making them platform-agnostic. Currently verified platforms include NVIDIA, AMD, and Intel. Any pull requests are welcome! <div align="center"> <img width="400" alt="image" src="https://github.com/fla-org/flash-linear-attention/assets/18402347/02ff2e26-1495-4088-b701-e72cd65ac6cf"> </div> News Models Installation Usage Token Mixing Fused Modules Generation Hybrid Models Training Evaluation Benchmarks Citation Star History Acknowledgments News - $\texttt2025-07$: ðŸ³ Add MLA implementation to paper. - $\texttt2025-07$: ðŸ›£ï¸ Added PaTH Attention to fla paper. - $\texttt2025-06$: ðŸŽ‰ Added MesaNet to fla paper. - $\texttt2025-06$: ðŸ Add Comba implementation to paper. - $\texttt2025-05$: ðŸŽ‰ Add Rodimus&ast; implementation to paper. - $\texttt2025-04$: ðŸŽ‰ Add DeltaProduct implementation to paper. - $\texttt2025-04$: ðŸŽ‰ Add FoX implementation to paper. - $\texttt2025-03$: ~~We have changed the default to the magic ðŸ³ 0.006~~ The was rolled back to the default value of 0.02. For actual training, we recommend trying both. - $\texttt2025-02$: ðŸ³ Add NSA implementations to . See kernels here. - $\texttt2025-01$: ðŸ”¥ We are migrating to -based training framework. Check out the flame repo for more details. - $\texttt2025-01$: ðŸŽ‰ Add RWKV7 implementations both kernels and models to . - $\texttt2024-12$: Integrated to repo - $\texttt2024-12$: ðŸŽ‰ Add Gated DeltaNet implementation to paper. - $\texttt2024-12$: ðŸš€ now officially supports kernels with variable-length inputs. - $\texttt2024-11$: The inputs are now switched from head-first to seq-first format. - $\texttt2024-11$: ðŸ’¥ now provides a flexible way for training hybrid models. - $\texttt2024-10$: ðŸ”¥ Announcing , a minimal and scalable framework for training models. Check out the details here. - $\texttt2024-09$: now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training. - $\texttt2024-09$: ðŸŽ‰ Add GSA implementation to paper. - $\texttt2024-05$: ðŸŽ‰ Add DeltaNet implementation to paper. - $\texttt2024-05$: ðŸ’¥ v0.1: a variety of subquadratic kernels/layers/models integrated RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see Models. - $\texttt2023-12$: ðŸ’¥ Launched , offering a collection of implementations for state-of-the-art linear attention models. Models Roughly sorted according to the timeline supported in . The recommended training mode is when available. | Year | Venue | Model | Paper | Code | | | :--- | :------ | :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------: | | 2023 | | RetNet | Retentive network: a successor to transformer for large language models | official | fla | | 2024 | ICML | GLA | Gated Linear Attention Transformers with Hardware-Efficient Training | official | fla | | 2024 | ICML | Based | Simple linear attention language models balance the recall-throughput tradeoff | official | fla | | 2024 | ACL | Rebased | Linear Transformers with Learnable Kernel Functions are Better In-Context Models | official | fla | | 2024 | NeurIPS | DeltaNet | Parallelizing Linear Transformers with Delta Rule over Sequence Length | official | fla | | 2022 | ACL | ABC | ABC: Attention with Bounded-memory Control | | fla | | 2023 | NeurIPS | HGRN | Hierarchically Gated Recurrent Neural Network for Sequence Modeling | official | fla | | 2024 | COLM | HGRN2 | HGRN2: Gated Linear RNNs with State Expansion | official | fla | | 2024 | COLM | RWKV6 | Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence | official | fla | | 2024 | | LightNet | You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet | official | fla | | 2025 | ICLR | Samba | Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling | official | fla | | 2024 | ICML | Mamba2 | Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality | official | fla | | 2024 | NeurIPS | GSA | Gated Slot Attention for Efficient Linear-Time Sequence Modeling | official | fla | | 2025 | ICLR | Gated DeltaNet | Gated Delta Networks: Improving Mamba2 with Delta Rule | official | fla | | 2025 | | RWKV7 | RWKV-7 "Goose" with Expressive Dynamic State Evolution | official | fla | | 2025 | | NSA | Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention | | fla | | 2025 | ICLR | FoX | Forgetting Transformer: Softmax Attention with a Forget Gate | official | fla | | 2025 | | DeltaProduct | DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products | | fla | | 2025 | ICLR | Rodimus&ast; | Rodimus: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions | official | fla | | 2025 | | MesaNet | MesaNet: Sequence Modeling by Locally Optimal Test-Time Training | | fla | | 2025 | | Comba | Comba: Improving Bilinear RNNs with Closed-loop Control | official | fla | | 2025 | | PaTH | PaTH Attention: Position Encoding via Accumulating Householder Transformations | | fla | Installation !nvidia-4090-cihttps://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml !nvidia-a100-cihttps://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml !nvidia-h100-cihttps://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml !intel-b580-cihttps://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml The following requirements should be satisfied - PyTorch >= 2.5 - Triton >=3.0 or nightly version, see FAQs - einops - transformers >=4.45.0 - datasets >=3.3.0 - causal-conv1d >=1.4.0 You can install with pip: As is actively developed now, for the latest features and updates, an alternative way is to install the package from source or manage with submodules If you have installed and pre version, please use the following command: Usage Token Mixing We provide fla.layersflafla.modulesRotary EmbeddingNorm LayersRMSNormLayerNormGroupNormRMSNormLinearLayerNormLinearGroupNormLinearNorm Layers with GatingCross EntropyLinear Cross EntropyLinear KL Divergencefuselinearcrossentropyfla-hubflaattnflametorchtitanflalmevalflaflalm-evaluation-harnesslm-evaluation-harnesslm-evaluation-harnesslmeval.evaluatelmeval.simpleevaluate, simply run the following to load the library's stock tasks! Benchmarks We compared our Triton-based RetNet implementation with CUDA-based FlashAttention2, using a batch size of 8, 32 heads, and a head dimension of 128, across different sequence lengths. These tests were conducted on a single H100 80GB GPU, as illustrated in the following graph <div align="center"> <img width="500" alt="image" src="https://github.com/user-attachments/assets/c2607015-63af-43d1-90d1-ad5fe1670a03"> </div> Citation If you find this repository helpful, please cite our work: Star History !Stargazers repo roster for @fla-org/flash-linear-attentionhttps://github.com/fla-org/flash-linear-attention/stargazers !Star History Charthttps://star-history.com/fla-org/flash-linear-attention&Date Acknowledgments We extend our gratitude to Bitdeer for providing CI server resources that power our infrastructure.