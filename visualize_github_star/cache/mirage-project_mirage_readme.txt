<div align="center"> Mirage Persistent Kernel: Compiling LLMs into a MegaKernel | Join Slack | Roadmap | Blog Post | </div> Latest News ðŸ”¥ 2025/06 We released Mirage Persistent Kernel MPK, a compiler and runtime that automatically transforms multi-GPU LLM inference into a high-performance megakernel. About Mirage Persistent Kernel MPK is a compiler and runtime system that automatically transforms LLM inference into a single megakernelâ€”a fused GPU kernel that performs all necessary computation and communication within a single kernel launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2Ã— to 6.7Ã—, all while requiring minimal developer effort. Quick Installation The fastest way to try MPK is to install it directly from source: > ðŸ”§2025/06/19 We are working on pre-built binary wheels for MPK and will update the installation instructions once they are available. Quickstart Mirage allows you to compile LLMs from the Hugging Face model zoo into a megakernel using just a few dozen lines of Pythonâ€”mainly to define the kernelâ€™s inputs and outputs. See this demo script that compiles the Qwen3-8B model into a megakernel. We start by running the demo with native Triton and FlashInfer kernels: To compile and execute the megakernel using MPK: To enable profiling which visualizes the execution timeline of each task: How MPK Works Once you've imported the Mirage package, you can instantiate a persistent kernel using the following API: and : number of GPUs and current GPU rank. , , : the number of workers, local schedulers, and remote schedulers. They must match the number of physical SMs + + / 4. The megakernel currently requires two meta tensors: is an array of integer tracking the current decoding step, and is incremented by MPK after each decoding iteration; is a tensor of shape , storing prompts and MPK generated tokens. To attach an existing : is used by MPK to refer to the tensor in the generated megakernel in CUDA. To allocate a new tensor: and specify the dimensions and data type of the tensor. is used by MPK to refer to this new tensor in the megakernel. indicates how the tensor is allocated and must be or the latter is required for remote GPU access, e.g., during all-reduce. Defining the Computation Graph You can compose the LLMâ€™s computation graph by chaining fused operations. For example: fuses an RMSNorm layer and a Linear layer in the megakernel. and are the weight tensors for RMSNorm and Linear. and are the input and output tensors of this fused layer. and specifies the number of thread blocks i.e., number of tasks in the task graph and number of thread within each thread block. To minimize latency, it is suggested that the total number of thread blocks is a multiplier of the number of workers to avoid outliers. Compilation & Execution Once the computation graph is defined, compile it with: Then, run the optimized megakernel as: Contribution We welcome feedback, contributions, and collaborations from the community! Please join our Slack channel. Please let us know if you encounter any bugs or have any suggestions by submitting an issue. Citation A paper describing Mirage's techniques is available on arxiv. Please cite Mirage as: License Mirage uses Apache License 2.0.