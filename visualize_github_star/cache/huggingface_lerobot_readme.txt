<p align="center"> <img alt="LeRobot, Hugging Face Robotics Library" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png" width="100%"> <br/> <br/> </p> <div align="center"> !Testshttps://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain !Python versionshttps://www.python.org/downloads/ !Licensehttps://github.com/huggingface/lerobot/blob/main/LICENSE !Statushttps://pypi.org/project/lerobot/ !Versionhttps://pypi.org/project/lerobot/ !Contributor Covenanthttps://github.com/huggingface/lerobot/blob/main/CODEOFCONDUCT.md !Discordhttps://discord.gg/s3KuuzsPFb <!-- !Coveragehttps://codecov.io/gh/huggingface/lerobot --> </div> <h2 align="center"> <p><a href="https://huggingface.co/docs/lerobot/hopejr"> Build Your Own HopeJR Robot!</a></p> </h2> <div align="center"> <img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/hopejr/hopejr.png" alt="HopeJR robot" title="HopeJR robot" width="60%" /> <p><strong>Meet HopeJR â€“ A humanoid robot arm and hand for dexterous manipulation!</strong></p> <p>Control it with exoskeletons and gloves for precise hand movements.</p> <p>Perfect for advanced manipulation tasks! ðŸ¤–</p> <p><a href="https://huggingface.co/docs/lerobot/hopejr"> See the full HopeJR tutorial here.</a></p> </div> <br/> <h2 align="center"> <p><a href="https://huggingface.co/docs/lerobot/so101"> Build Your Own SO-101 Robot!</a></p> </h2> <div align="center"> <table> <tr> <td align="center"><img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101.webp" alt="SO-101 follower arm" title="SO-101 follower arm" width="90%"/></td> <td align="center"><img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101-leader.webp" alt="SO-101 leader arm" title="SO-101 leader arm" width="90%"/></td> </tr> </table> <p><strong>Meet the updated SO100, the SO-101 â€“ Just â‚¬114 per arm!</strong></p> <p>Train it in minutes with a few simple moves on your laptop.</p> <p>Then sit back and watch your creation act autonomously! ðŸ¤¯</p> <p><a href="https://huggingface.co/docs/lerobot/so101"> See the full SO-101 tutorial here.</a></p> <p>Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!</p> <p>Check out the <a href="https://huggingface.co/docs/lerobot/lekiwi">LeKiwi tutorial</a> and bring your robot to life on wheels.</p> <img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp" alt="LeKiwi mobile robot" title="LeKiwi mobile robot" width="50%"> </div> <br/> <h3 align="center"> <p>LeRobot: State-of-the-art AI for real-world robotics</p> </h3> --- ðŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models. ðŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning. ðŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there. ðŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: huggingface.co/lerobot Examples of pretrained models on simulation environments <table> <tr> <td><img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/alohaact.gif" width="100%" alt="ACT policy on ALOHA env"/></td> <td><img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarmtdmpc.gif" width="100%" alt="TDMPC policy on SimXArm env"/></td> <td><img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pushtdiffusion.gif" width="100%" alt="Diffusion policy on PushT env"/></td> </tr> <tr> <td align="center">ACT policy on ALOHA env</td> <td align="center">TDMPC policy on SimXArm env</td> <td align="center">Diffusion policy on PushT env</td> </tr> </table> Installation LeRobot works with Python 3.10+ and PyTorch 2.2+. Environment Setup Create a virtual environment with Python 3.10 and activate it, e.g. with https://docs.anaconda.com/free/miniconda/index.html: When using , install in your environment: > NOTE: This usually installs for your platform compiled with the encoder. If is not supported check supported encoders with , you can: > > - On any platform Explicitly install using: > > > > - On Linux only Install ffmpeg build dependencies and compile ffmpeg from source with libsvtav1, and make sure you use the corresponding ffmpeg binary to your install with . Install LeRobot ðŸ¤— From Source First, clone the repository and navigate into the directory: Then, install the library in editable mode. This is useful if you plan to contribute to the code. > NOTE: If you encounter build errors, you may need to install additional dependencies , , and . On Linux, run: > . For other systems, see: Compiling PyAV For simulations, ðŸ¤— LeRobot comes with gymnasium environments that can be installed as extras: - aloha - xarm - pusht For instance, to install ðŸ¤— LeRobot with aloha and pusht, use: Installation from PyPI Core Library: Install the base package with: This installs only the default dependencies. Extra Features: To install additional functionality, use one of the following: Replace with your desired features. Available Tags: For a full list of optional dependencies, see: https://pypi.org/project/lerobot/ Weights & Biases To use Weights and Biases for experiment tracking, log in with note: you will also need to enable WandB in the configuration. See below. Visualize datasets Check out example 1 that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub. You can also locally visualize episodes from a dataset on the hub by executing our script from the command line: or from a dataset in a local folder with the option and the in the following case the dataset will be searched for in It will open and display the camera streams, robot states and actions, like this: https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4request&X-Amz-Date=20240505T172924Z&X-Amz-Expires=300&X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&X-Amz-SignedHeaders=host&actorid=24889239&keyid=0&repoid=748713144 Our script can also visualize datasets stored on a distant server. See for more instructions. The format A dataset in format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. and can be indexed into like any Hugging Face and PyTorch dataset. For instance will retrieve a single temporal frame from the dataset containing observations and an action as PyTorch tensors ready to be fed to a model. A specificity of is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting to a list of relative times with respect to the indexed frame. For example, with one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself corresponding to the 0 entry. See example 1loadlerobotdataset.py for more details on . Under the hood, the format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor. Here are the important details and internal structure organization of a typical instantiated with . The exact features will change from dataset to dataset but not the main aspects: A is serialised using several widespread file formats for each of its parts, namely: - hfdataset stored using Hugging Face datasets library serialization to parquet - videos are stored in mp4 format to save space - metadata are stored in plain json/jsonl files Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the argument if it's not in the default location. Evaluate a pretrained policy Check out example 2 that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment. We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on lerobot/diffusionpusht: Note: After training your own policy, you can re-evaluate the checkpoints with: See for more instructions. Train your own policy Check out example 3 that illustrates how to train a model using our core library in python, and example 4 that shows how to use our training script from command line. To use wandb for logging training and evaluation curves, make sure you've run as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding . A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check here for the explanation of some commonly used metrics in logs. \<img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png" alt="WandB logs example"\> Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See for more instructions. Reproduce state-of-the-art SOTA We provide some pretrained policies on our hub page that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running: reproduces SOTA results for Diffusion Policy on the PushT task. Contribute If you would like to contribute to ðŸ¤— LeRobot, please check out our contribution guide. Add a pretrained policy Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like e.g. lerobot/diffusionpusht. You first need to find the checkpoint folder located inside your experiment directory e.g. . Within that there is a directory which should contain: - : A serialized version of the policy configuration following the policy's dataclass config. - : A set of parameters, saved in Hugging Face Safetensors format. - : A consolidated configuration containing all parameters used for training. The policy configuration should match exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility. To upload these to the hub, run the following: See eval.py for an example of how other people may use your policy. Acknowledgment - The LeRobot team ðŸ¤— for building SmolVLA Paper, Blog. - Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from ALOHA and Mobile ALOHA. - Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from Diffusion Policy and UMI Gripper. - Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from TDMPC and FOWM. - Thanks to Antonio Loquercio and Ashish Kumar for their early support. - Thanks to Seungjae Jay Lee, Mahi Shafiullah and colleagues for open sourcing VQ-BeT policy and helping us adapt the codebase to our repository. The policy is adapted from VQ-BeT repo. Citation If you want, you can cite this work with: Star History !Star History Charthttps://star-history.com/huggingface/lerobot&Timeline