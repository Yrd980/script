<div align="center"> <h1>GPT-SoVITS-WebUI</h1> A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.<br><br> !madewithlovehttps://github.com/RVC-Boss/GPT-SoVITS <a href="https://trendshift.io/repositories/7033" target="blank"><img src="https://trendshift.io/api/badge/repositories/7033" alt="RVC-Boss%2FGPT-SoVITS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> <!-- img src="https://counter.seku.su/cmoe?name=gptsovits&theme=r34" /><br> --> !Pythonhttps://www.python.org !GitHub releasehttps://github.com/RVC-Boss/gpt-sovits/releases !Train In Colabhttps://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/Colab-WebUI.ipynb !Huggingfacehttps://lj1995-gpt-sovits-proplus.hf.space/ !Image Sizehttps://hub.docker.com/r/xxxxrt666/gpt-sovits !简体中文https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e !Englishhttps://rentry.co/GPT-SoVITS-guide/ !Change Loghttps://github.com/RVC-Boss/GPT-SoVITS/blob/main/docs/en/ChangelogEN.md !Licensehttps://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE English | 中文简体 | 日本語 | 한국어 | Türkçe </div> --- Features: 1. Zero-shot TTS: Input a 5-second vocal sample and experience instant text-to-speech conversion. 2. Few-shot TTS: Fine-tune the model with just 1 minute of training data for improved voice similarity and realism. 3. Cross-lingual Support: Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese. 4. WebUI Tools: Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models. Check out our demo video here! Unseen speakers few-shot fine-tuning demo: https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb RTFinference speed of GPT-SoVITS v2 ProPlus: 0.028 tested in 4060Ti, 0.014 tested in 4090 1400words~=4min, inference time is 3.36s, 0.526 in M4 CPU. You can test our huggingface demo half H200 to experience high-speed inference . 请不要尬黑GPT-SoVITS推理速度慢，谢谢！ User guide: 简体中文 | English Installation For users in China, you can click here to use AutoDL Cloud Docker to experience the full functionality online. Tested Environments | Python Version | PyTorch Version | Device | | -------------- | ---------------- | ------------- | | Python 3.10 | PyTorch 2.5.1 | CUDA 12.4 | | Python 3.11 | PyTorch 2.5.1 | CUDA 12.4 | | Python 3.11 | PyTorch 2.7.0 | CUDA 12.8 | | Python 3.9 | PyTorch 2.8.0dev | CUDA 12.8 | | Python 3.9 | PyTorch 2.5.1 | Apple silicon | | Python 3.11 | PyTorch 2.7.0 | Apple silicon | | Python 3.9 | PyTorch 2.2.2 | CPU | Windows If you are a Windows user tested with win>=10, you can download the integrated package and double-click on go-webui.bat to start GPT-SoVITS-WebUI. Users in China can download the package here. Install the program by running the following commands: Linux macOS Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead. Install the program by running the following commands: Install Manually Install Dependences Install FFmpeg Conda Users Ubuntu/Debian Users Windows Users Download and place ffmpeg.exe and ffprobe.exe in the GPT-SoVITS root Install Visual Studio 2017 MacOS Users Running GPT-SoVITS with Docker Docker Image Selection Due to rapid development in the codebase and a slower Docker image release cycle, please: - Check Docker Hub for the latest available image tags - Choose an appropriate image tag for your environment - means the Docker image does not include ASR models and UVR5 models. You can manually download the UVR5 models, while the program will automatically download the ASR models as needed - The appropriate architecture image amd64/arm64 will be automatically pulled during Docker Compose - Docker Compose will mount all files in the current directory. Please switch to the project root directory and pull the latest code before using the Docker image - Optionally, build the image locally using the provided Dockerfile for the most up-to-date changes Environment Variables - : Controls whether half-precision fp16 is enabled. Set to if your GPU supports it to reduce memory usage. Shared Memory Configuration On Windows Docker Desktop, the default shared memory size is small and may cause unexpected behavior. Increase e.g., to in your Docker Compose file based on your available system memory. Choosing a Service The defines two services: - & : Full version with all features. - & : Lightweight version with reduced dependencies and functionality. To run a specific service with Docker Compose, use: Building the Docker Image Locally If you want to build the image yourself, use: Accessing the Running Container Bash Shell Once the container is running in the background, you can access it using: Pretrained Models If runs successfully, you may skip No.1,2,3 Users in China can download all these models here. 1. Download pretrained models from GPT-SoVITS Models and place them in . 2. Download G2PW models from G2PWModel.zipHF| G2PWModel.zipModelScope, unzip and rename to , and then place them in .Chinese TTS Only 3. For UVR5 Vocals/Accompaniment Separation & Reverberation Removal, additionally, download models from UVR5 Weights and place them in . - If you want to use or models for UVR5, you can manually download the model and corresponding configuration file, and put them in . Rename the model file and configuration file, ensure that the model and configuration files have the same and corresponding names except for the suffix. In addition, the model and configuration file names must include in order to be recognized as models of the roformer class. - The suggestion is to directly specify the model type in the model name and configuration file name, such as , . If not specified, the features will be compared from the configuration file to determine which type of model it is. For example, the model and its corresponding configuration file are a pair, and are also a pair. 4. For Chinese ASR additionally, download models from Damo ASR Model, Damo VAD Model, and Damo Punc Model and place them in . 5. For English or Japanese ASR additionally, download models from Faster Whisper Large V3 and place them in . Also, other models may have the similar effect with smaller disk footprint. Dataset Format The TTS annotation .list file format: Language dictionary: - 'zh': Chinese - 'ja': Japanese - 'en': English - 'ko': Korean - 'yue': Cantonese Example: Finetune and inference Open WebUI Integrated Package Users Double-click or use if you want to switch to V1,then double-click or use Others if you want to switch to V1,then Or maunally switch version in WebUI Finetune Path Auto-filling is now supported 1. Fill in the audio path 2. Slice the audio into small chunks 3. Denoiseoptinal 4. ASR 5. Proofreading ASR transcriptions 6. Go to the next Tab, then finetune the model Open Inference WebUI Integrated Package Users Double-click or use ,then open the inference webui at Others OR then open the inference webui at V2 Release Notes New Features: 1. Support Korean and Cantonese 2. An optimized text frontend 3. Pre-trained model extended from 2k hours to 5k hours 4. Improved synthesis quality for low-quality reference audio more details> Use v2 from v1 environment: 1. to update some packages 2. Clone the latest codes from github. 3. Download v2 pretrained models from huggingface and put them into . Chinese v2 additional: G2PWModel.zipHF| G2PWModel.zipModelScopeDownload G2PW models, unzip and rename to , and then place them in . V3 Release Notes New Features: 1. The timbre similarity is higher, requiring less training data to approximate the target speaker the timbre similarity is significantly improved using the base model directly without fine-tuning. 2. GPT model is more stable, with fewer repetitions and omissions, and it is easier to generate speech with richer emotional expression. more details> Use v3 from v2 environment: 1. to update some packages 2. Clone the latest codes from github. 3. Download v3 pretrained models s1v3.ckpt, s2Gv3.pth and models--nvidia--bigvganv224khz100band256x folder from huggingface and put them into . additional: for Audio Super Resolution model, you can read how to download V4 Release Notes New Features: 1. Version 4 fixes the issue of metallic artifacts in Version 3 caused by non-integer multiple upsampling, and natively outputs 48k audio to prevent muffled sound whereas Version 3 only natively outputs 24k audio. The author considers Version 4 a direct replacement for Version 3, though further testing is still needed. more details> Use v4 from v1/v2/v3 environment: 1. to update some packages 2. Clone the latest codes from github. 3. Download v4 pretrained models gsv-v4-pretrained/s2v4.ckpt, and gsv-v4-pretrained/vocoder.pth from huggingface and put them into . V2Pro Release Notes New Features: 1. Slightly higher VRAM usage than v2, surpassing v4's performance, with v2's hardware cost and speed. more details> 2.v1/v2 and the v2Pro series share the same characteristics, while v3/v4 have similar features. For training sets with average audio quality, v1/v2/v2Pro can deliver decent results, but v3/v4 cannot. Additionally, the synthesized tone and timebre of v3/v4 lean more toward the reference audio rather than the overall training set. Use v2Pro from v1/v2/v3/v4 environment: 1. to update some packages 2. Clone the latest codes from github. 3. Download v2Pro pretrained models v2Pro/s2Dv2Pro.pth, v2Pro/s2Gv2Pro.pth, v2Pro/s2Dv2ProPlus.pth, v2Pro/s2Gv2ProPlus.pth, and sv/pretrainederes2netv2w24s4ep4.ckpt from huggingface and put them into . Todo List - x High Priority: - x Localization in Japanese and English. - x User guide. - x Japanese and English dataset fine tune training. - Features: - x Zero-shot voice conversion 5s / few-shot voice conversion 1min. - x TTS speaking speed control. - ~~Enhanced TTS emotion control.~~ Maybe use pretrained finetuned preset GPT models for better emotion. - Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs transformer latent. - x Improve English and Japanese text frontend. - Develop tiny and larger-sized TTS models. - x Colab scripts. - x Try expand training dataset 2k hours -> 10k hours. - x better sovits base model enhanced audio quality - model mix Additional Method for running from the command line Use the command line to open the WebUI for UVR5 <!-- If you can't open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing --> This is how the audio segmentation of the dataset is done using the command line This is how dataset ASR processing is done using the command lineOnly Chinese ASR processing is performed through FasterWhisperASR marking except Chinese No progress bars, GPU performance may cause time delays A custom list save path is enabled Credits Special thanks to the following projects and contributors: Theoretical Research - ar-vits - SoundStorm - vits - TransferTTS - contentvec - hifi-gan - fish-speech - f5-TTS - shortcut flow matching Pretrained Models - Chinese Speech Pretrain - Chinese-Roberta-WWM-Ext-Large - BigVGAN - eresnetv2 Text Frontend for Inference - paddlespeech zhnormalization - split-lang - g2pW - pypinyin-g2pW - paddlespeech g2pw WebUI Tools - ultimatevocalremovergui - audio-slicer - SubFix - FFmpeg - gradio - faster-whisper - FunASR - AP-BWE Thankful to @Naozumi520 for providing the Cantonese training set and for the guidance on Cantonese-related knowledge. Thanks to all contributors for their efforts <a href="https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors" target="blank"> <img src="https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS" /> </a>