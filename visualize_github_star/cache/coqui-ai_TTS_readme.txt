ğŸ¸Coqui.ai News - ğŸ“£ â“TTSv2 is here with 16 languages and better performance across the board. - ğŸ“£ â“TTS fine-tuning code is out. Check the example recipes. - ğŸ“£ â“TTS can now stream with <200ms latency. - ğŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released Blog Post, Demo, Docs - ğŸ“£ ğŸ¶Bark is now available for inference with unconstrained voice cloning. Docs - ğŸ“£ You can use ~1100 Fairseq models with ğŸ¸TTS. - ğŸ“£ ğŸ¸TTS now supports ğŸ¢Tortoise with faster inference. Docs <div align="center"> <img src="https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2" /> <img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png" height="56"/> ğŸ¸TTS is a library for advanced Text-to-Speech generation. ğŸš€ Pretrained models in +1100 languages. ğŸ› ï¸ Tools for training new models and fine-tuning existing models in any language. ğŸ“š Utilities for dataset analysis and curation. !Discordhttps://discord.gg/5eXr5seRrv !Licensehttps://opensource.org/licenses/MPL-2.0 !PyPI versionhttps://badge.fury.io/py/TTS !Covenanthttps://github.com/coqui-ai/TTS/blob/master/CODEOFCONDUCT.md !Downloadshttps://pepy.tech/project/tts !DOIhttps://zenodo.org/badge/latestdoi/265612440 !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !GithubActions !Docshttps://tts.readthedocs.io/en/latest/ </div> ğŸ’¬ Where to ask questions Please use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it. | Type | Platforms | | ------------------------------- | --------------------------------------- | | ğŸš¨ Bug Reports | GitHub Issue Tracker | | ğŸ Feature Requests & Ideas | GitHub Issue Tracker | | ğŸ‘©â€ğŸ’» Usage Questions | GitHub Discussions | | ğŸ—¯ General Discussion | GitHub Discussions or Discord | github issue tracker: https://github.com/coqui-ai/tts/issues github discussions: https://github.com/coqui-ai/TTS/discussions discord: https://discord.gg/5eXr5seRrv Tutorials and Examples: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials ğŸ”— Links and Resources | Type | Links | | ------------------------------- | --------------------------------------- | | ğŸ’¼ Documentation | ReadTheDocs | ğŸ’¾ Installation | TTS/README.md| | ğŸ‘©â€ğŸ’» Contributing | CONTRIBUTING.md| | ğŸ“Œ Road Map | Main Development Plans | ğŸš€ Released Models | TTS Releases and Experimental Models| | ğŸ“° Papers | TTS Papers| ğŸ¥‡ TTS Performance <p align="center"><img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png" width="800" /></p> Underlined "TTS" and "Judy" are internal ğŸ¸TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot .Jofish .Abe and .Janice are real human voices. Features - High-performance Deep Learning models for Text2Speech tasks. - Text2Spec models Tacotron, Tacotron2, Glow-TTS, SpeedySpeech. - Speaker Encoder to compute speaker embeddings efficiently. - Vocoder models MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN - Fast and efficient model training. - Detailed training logs on the terminal and Tensorboard. - Support for Multi-speaker TTS. - Efficient, flexible, lightweight but feature complete . - Released and ready-to-use models. - Tools to curate Text2Speech datasets under. - Utilities to use and test your models. - Modular but not too much code base enabling easy implementation of new ideas. Model Implementations Spectrogram models - Tacotron: paper - Tacotron2: paper - Glow-TTS: paper - Speedy-Speech: paper - Align-TTS: paper - FastPitch: paper - FastSpeech: paper - FastSpeech2: paper - SC-GlowTTS: paper - Capacitron: paper - OverFlow: paper - Neural HMM TTS: paper - Delightful TTS: paper End-to-End Models - â“TTS: blog - VITS: paper - ğŸ¸ YourTTS: paper - ğŸ¢ Tortoise: orig. repo - ğŸ¶ Bark: orig. repo Attention Methods - Guided Attention: paper - Forward Backward Decoding: paper - Graves Attention: paper - Double Decoder Consistency: blog - Dynamic Convolutional Attention: paper - Alignment Network: paper Speaker Encoder - GE2E: paper - Angular Loss: paper Vocoders - MelGAN: paper - MultiBandMelGAN: paper - ParallelWaveGAN: paper - GAN-TTS discriminators: paper - WaveRNN: origin - WaveGrad: paper - HiFiGAN: paper - UnivNet: paper Voice Conversion - FreeVC: paper You can also help us implement more models. Installation ğŸ¸TTS is tested on Ubuntu 18.04 with python >= 3.9, < 3.12.. If you are only interested in synthesizing speech with the released ğŸ¸TTS models, installing from PyPI is the easiest option. If you plan to code or train models, clone ğŸ¸TTS and install it locally. If you are on Ubuntu Debian, you can also run following commands for installation. If you are on Windows, ğŸ‘‘@GuyPaddock wrote installation instructions here. Docker Image You can also try TTS without install with the docker image. Simply run the following command and you will be able to run TTS without installing it. You can then enjoy the TTS server here More details about the docker images like GPU support can be found here Synthesizing speech by ğŸ¸TTS ğŸ Python API Running a multi-speaker and multi-lingual model Running a single speaker model Example voice conversion Converting the voice in to the voice of Example voice cloning together with the voice conversion model. This way, you can clone voices by using any model in ğŸ¸TTS. Example text to speech using Fairseq models in ~1100 languages ğŸ¤¯. For Fairseq models, use the following name format: . You can find the language ISO codes here and learn about the Fairseq models here. Command-line <!-- begin-tts-readme --> Synthesize speech on command line. You can either use your trained model or choose a model from the provided list. If you don't specify any models, then it uses LJSpeech based English model. Single Speaker Models - List provided models: - Get model info for both ttsmodels and vocodermodels: - Query by type/name: The modelinfobyname uses the name as it from the --listmodels. For example: - Query by type/idx: The modelqueryidx uses the corresponding idx from --listmodels. For example: - Query info for model info by full name: - Run TTS with default models: - Run TTS and pipe out the generated TTS wav file data: - Run a TTS model with its default vocoder model: For example: - Run with specific TTS and vocoder models from the list: For example: - Run your own TTS model Using Griffin-Lim Vocoder: - Run your own TTS and Vocoder models: Multi-speaker Models - List the available speakers and choose a <speakerid> among them: - Run the multi-speaker TTS model with the target speaker ID: - Run your own multi-speaker TTS model: Voice Conversion Models <!-- end-tts-readme --> Directory Structure