<a name="top"></a> <h1 align="center"> mistral.rs </h1> <h3 align="center"> Blazingly fast LLM inference. </h3> <p align="center"> | <a href="https://ericlbuehler.github.io/mistral.rs/mistralrs/"><b>Rust Documentation</b></a> | <a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/API.md"><b>Python Documentation</b></a> | <a href="https://discord.gg/SZrecqK8qw"><b>Discord</b></a> | <a href="https://matrix.to//mistral.rs:matrix.org"><b>Matrix</b></a> | </p> <p align="center"> <a href="https://github.com/EricLBuehler/mistral.rs/stargazers"> <img src="https://img.shields.io/github/stars/EricLBuehler/mistral.rs?style=social&label=Star" alt="GitHub stars"> </a> </p> Mistral.rs is a cross-platform, highly-multimodal inference engine that brings you: - All-in-one multimodal workflow: text↔text, text+vision↔text, text+vision+audio↔text, text→speech, text→image - APIs: Rust, Python, OpenAI HTTP server with Chat Completions, Responses API, MCP server - 🔗 MCP Client: Connect to external tools and services automatically file systems, web search, databases, APIs - Performance: ISQ, PagedAttention, FlashAttention Please submit requests for new models here. Get started fast 🚀 1 Install 2 Get models 3 Deploy with our easy to use APIs - Python - Rust - OpenAI-compatible HTTP server - Interactive mode - 🔗 MCP Client - Connect to external tools automatically 4 Try the web chat app for local in-browser conversation text, vision, and speech support: - Quickstart here - Run the server and visit http://localhost:8080 by default. <br> <!-- Web Chat App --> <details open> <summary>🖥️ <strong>Web Chat App</strong></summary> <br> <img src="./res/chat.gif" alt="Web Chat UI Demo" /> <br> Try our modern in-browser chat with text, vision, and speech support TTS generation. </details> <!-- Interactive Mode --> <details> <summary>💻 <strong>Terminal Interactive Mode</strong></summary> <br> <img src="./res/demo.gif" alt="Terminal Interactive Mode" /> <br> Prefer the terminal? Use interactive mode for a classic CLI experience. </details> <br> Quick examples After following installation instructions - 💎🪆💎🪆💎 Run the Gemma 3n family E2B, E4B with vision, audio, and MatFormer support: documentation <details> <summary>Show commands</summary> Normal use, run the full model E4B or E2B: Use MatFormer to get a balanced smaller model: </details> - 🤗🤗🤗 Run the SmolLM 3 long-context hybrid-reasoning model with full tool-calling support: documentation <details> <summary>Show command</summary> Default, easiest: UQFF prequantized: </details> - 🔊 Run the Dia 1.6b model for highly-realistic dialogue generation: documentation <details> <summary>Show command</summary> </details> - 🦙 Run the Llama 3.\ and Llama 4 models with long context & vision support: docs llama 3.2, docs llama 4 <details> <summary>Show commands</summary> Llama 4: Llama 3.1/3.2/3.3: Llama 3.2 vision: </details> - 💎💎💎 Run the Gemma 3 family 1b, 4b, 12b, 27b with 128k context & vision support: documentation <details> <summary>Show command</summary> </details> - 🌲📷 Run the FLUX.1 diffusion model: documentation <details> <summary>Show command</summary> </details> - 🧠 Run the Qwen 3 hybrid-reasoning model with full tool-calling support: documentation <details> <summary>Show command</summary> </details> - 🔗 MCP Client - Connect to external tools and services automatically: Quick Start Guide <details> <summary>Show examples</summary> 1. Create config file : 2. Start server with tools: 3. Tools work automatically: Python API: Rust API: </details> Description mistral.rs is a blazing-fast, cross-platform LLM inference engine with support for text, vision, image generation, and speech. Key Benefits: 1. Ease of Use - OpenAI-compatible HTTP server - Rust API & Python API - Automatic device mapping multi-GPU, CPU - Chat templates & tokenizer auto-detection - MCP server for structured, realtime tool calls - ⭐ MCP client to connect to external tools and services automatically 2. Performance - CPU acceleration MKL, AVX, NEON, Accelerate - GPU acceleration CUDA with FlashAttention & cuDNN, Metal - Automatic tensor parallelism for splitting models across multiple devices - CUDA-specialized NCCL - Heterogeneous, flexible Ring backend 3. Quantization - In-place quantization ISQ of Hugging Face models - GGML & GGUF support: 2–8 bit - GPTQ, AWQ, AFQ, HQQ, FP8, BNB int8/fp4/nf4 - ⭐ Auto-select the fastest quant method - KV cache quantization 4. Flexibility - LoRA & X-LoRA adapters with weight merging - AnyMoE: create MoE models on any base model - Sampling & penalty options - Prompt chunking for large inputs - Integrated tool calling with customizable Python/Rust native tool and search callbacks 5. Advanced Features - High-throughput with PagedAttention & FlashAttention V2/V3 - Prefix caching including multimodal - Customizable quantization with topology & UQFF format - Speculative decoding across models - ⭐ Agentic web search integration APIs and Integrations Rust Crate Rust multithreaded/async API for easy integration into any application. - Docs - Examples including MCP client integration - To use: add to your Cargo.toml - MCP Client: Connect to external tools automatically - Quick Start Python API Python API for mistral.rs. - Installation including PyPI - Docs - Examples including MCP client usage - Cookbook - MCP Client: Full MCP integration - Quick Start HTTP Server OpenAI API compatible API server - API Docs - includes chat completions, completions, and Responses API for stateful conversations - Launching the server or use the CLI - Example - Responses API examples - maintain conversation context without resending history - Use or extend the server in other axum projects - MCP Client: Configure via flag for automatic tool integration - Quick Start MCP Protocol Serve the same models over the open MCP Model Context Protocol in parallel to the HTTP API: See the docs for feature flags, examples and limitations. Llama Index integration - Docs: https://docs.llamaindex.ai/en/stable/examples/llm/mistralrs/ --- Supported accelerators | Accelerator | Feature Flag | Additional Flags | |--------------------------|---------------|------------------------| | NVIDIA GPUs CUDA | | , , | | Apple Silicon GPU Metal| | | | CPU Intel | | | | CPU Apple Accelerate | | | | Generic CPU ARM/AVX | none | ARM NEON / AVX enabled by default | To enable one or more features, pass them to Cargo. For example: > Note for Linux users: The feature is macOS-only and should not be used on Linux. Use for NVIDIA GPUs or for Intel CPUs instead of . Installation and Build > Note: You can use our Docker containers here. > Learn more about running Docker containers: https://docs.docker.com/engine/reference/run/ - Install the Python package here. - The Python package has wheels on PyPi! 1 Install required packages: - Example on Ubuntu: - <b>Linux only:</b> Example on Ubuntu: 2 Install Rust: https://rustup.rs/ Example on Ubuntu: 3 <b>Optional:</b> Set HF token correctly skip if already set or your model is not gated, or if you want to use the parameters in Python or the command line. - Note: you can install as documented here. 4 Download the code: 5 Build or install : - Build the binary, which can be found at . - Install with for easy command line usage Pass the same values to as you would for 6 If you used The build process will output a binary at . We can switch to that directory so that the binary can be accessed as with the following command: Example on Ubuntu: 7 Use our APIs and integrations: APIs and integrations list Getting models <details> <summary>Show: How to get models Hub, local, GGUF, adapters, etc.</summary> Getting models from Hugging Face Hub - Default: Downloads from Hugging Face Hub. - For gated models, you can optionally set token source: - CLI: - Python: See examples/python/tokensource.py - If no token is found, tries or runs with no token. Loading models from local files - Pass a path to a downloaded model from Hugging Face hub: - Example: Running GGUF models - Minimal example: - Specify tokenizer if needed: Or use the built-in GGUF tokenizer. Adapters, X-LoRA, LoRA, Chat Templates - Use the correct subcommand , , pass model, adapter, or quant file as needed. - See docs/ADAPTERMODELS.md for details. - For chat templates: usually auto-detected, override with . See docs/CHATTOK.md. More model CLI examples - See Run with the CLI below or full documentation. </details> Using the CLI Mistral.rs uses subcommands to control the model type. Please run to see the subcommands which categorize the models by kind. > 🚨 Important: The subcommand alias for / only auto-detects and runs text and vision models. It does not support diffusion or speech models. > To run a diffusion model e.g. FLUX series, use the subcommand: > > To run a speech model e.g. Dia, use the subcommand: > > If you attempt to use with diffusion or speech models, model loading will fail. Interactive mode Llama 3.2 3B running on an M3 Max with 8-bit ISQ: <img src="./res/demo.gif" alt="Interactive demo" /> You can launch interactive mode, a simple chat application running in the terminal, by passing : Vision models work seamlessly: Diffusion models can be run too quantization and adapters are not yet supported: And you can run speech generation in your terminal! OpenAI HTTP server You can launch an HTTP server by replacing with . For instance: You can find documentation about the server itself here. Multi-model support Serve multiple models simultaneously from a single server instance. Perfect for comparing models, A/B testing, or serving different models for different use cases. Select models in your requests using the parameter: 📖 Complete multi-model documentation → Structured selection with a file We provide a method to select models with a file. The keys are the same as the command line, with and being "global" keys. Example: Architecture for plain models > Note: for plain models, you can specify the data type to load and run in. This must be one of , , or to choose based on the device. This is specified in the / parameter after the model architecture . For quantized models gguf/ggml, you may specify data type of or is not recommended due to its lower precision in quantized inference. If you do not specify the architecture, an attempt will be made to use the model's config. If this fails, please raise an issue. <details> <summary>Show plain architectures</summary> - - - - - - - - - - - - - - - - </details> Architecture for vision models > Note: for vision models, you can specify the data type to load and run in. This must be one of , , or to choose based on the device. This is specified in the / parameter after the model architecture . <details> <summary>Show vision architectures</summary> - - - - - - - - - - - - - - </details> Supported GGUF architectures <details> <summary>Show supported GGUF architectures</summary> Plain: - llama - phi2 - phi3 - starcoder2 - qwen2 - qwen3 With adapters: - llama - phi3 </details> --- Please submit more benchmarks via raising an issue! Supported models <details> <summary>Show quantization support</summary> Quantization support |Model|GGUF|GGML|ISQ| |--|--|--|--| |Mistral|✅| |✅| |Gemma| | |✅| |Llama|✅|✅|✅| |Mixtral|✅| |✅| |Phi 2|✅| |✅| |Phi 3|✅| |✅| |Phi 3.5 MoE| | |✅| |Qwen 2.5| | |✅| |Phi 3 Vision| | |✅| |Idefics 2| | |✅| |Gemma 2| | |✅| |GLM4| | |✅| |Starcoder 2| |✅|✅| |LLaVa Next| | |✅| |LLaVa| | |✅| |Llama 3.2 Vision| | |✅| |Qwen2-VL| | |✅| |Idefics 3| | |✅| |Deepseek V2| | |✅| |Deepseek V3| | |✅| |MiniCPM-O 2.6| | |✅| |Qwen2.5-VL| | |✅| |Gemma 3| | |✅| |Mistral 3| | |✅| |Llama 4| | |✅| |Qwen 3|✅| |✅| |SmolLM3| | |✅| |Dia 1.6b| | |✅| |Gemma 3n| | |✅| </details> <details> <summary>Show device mapping support</summary> Device mapping support |Model category|Supported| |--|--| |Plain|✅| |GGUF|✅| |GGML| | |Vision Plain|✅| </details> <details> <summary>Show X-LoRA and LoRA support</summary> X-LoRA and LoRA support |Model|X-LoRA|X-LoRA+GGUF|X-LoRA+GGML| |--|--|--|--| |Mistral|✅|✅| | |Gemma|✅| | | |Llama|✅|✅|✅| |Mixtral|✅|✅| | |Phi 2|✅| | | |Phi 3|✅|✅| | |Phi 3.5 MoE| | | | |Qwen 2.5| | | | |Phi 3 Vision| | | | |Idefics 2| | | | |Gemma 2|✅| | | |GLM4|✅| | | |Starcoder 2|✅| | | |LLaVa Next| | | | |LLaVa| | | | |Qwen2-VL| | | | |Idefics 3| | | | |Deepseek V2| | | | |Deepseek V3| | | | |MiniCPM-O 2.6| | | | |Qwen2.5-VL| | | | |Gemma 3| | | | |Mistral 3| | | | |Llama 4| | | | |Qwen 3| | | | |SmolLM3|✅| | | |Gemma 3n| | | | </details> <details> <summary>Show AnyMoE support</summary> AnyMoE support |Model|AnyMoE| |--|--| |Mistral 7B|✅| |Gemma|✅| |Llama|✅| |Mixtral| | |Phi 2|✅| |Phi 3|✅| |Phi 3.5 MoE| | |Qwen 2.5|✅| |Phi 3 Vision| | |Idefics 2| | |Gemma 2|✅| |Starcoder 2|✅| |LLaVa Next|✅| |LLaVa|✅| |Llama 3.2 Vision| | |Qwen2-VL| | |Idefics 3|✅| |Deepseek V2| | |Deepseek V3| | |MiniCPM-O 2.6| | |Qwen2.5-VL| | |Gemma 3|✅| |Mistral 3|✅| |Llama 4| | |Qwen 3| | |SmolLM3|✅| |Gemma 3n| | | | </details> Using derivative and adapter models To use a derivative or adapter model e.g., quantized, LoRA, X-LoRA, vision, etc., select the correct architecture subcommand and pass the required arguments—typically model id, and for quantized/adapters, also the quantization filename, tokenizer, or adapter ordering if needed. - See all options: Run - Docs: Adapter models, Chat templates <details> <summary>Arguments by model type</summary> | Model Type | Required Arguments | |---------------------|-----------------------------------------------------------------------| | Plain | model id | | Quantized | model id, quantized filename, tokenizer id | | X-LoRA | model id, X-LoRA ordering if not default | | X-LoRA quantized | model id, quantized filename, tokenizer id, X-LoRA ordering | | LoRA | model id, LoRA ordering if not default | | LoRA quantized | model id, quantized filename, tokenizer id, LoRA ordering | | Vision Plain | model id | </details> <details> <summary>Example: Zephyr GGUF model</summary> </details> Chat template and tokenizer are usually auto-detected. If you need to override, see the chat templates doc. An adapter model is a model with X-LoRA or LoRA. X-LoRA support is provided by selecting the architecture, and LoRA support by selecting the architecture. Please find docs for adapter models here. Examples may be found here. Chat Templates and Tokenizer Mistral.rs will attempt to automatically load a chat template and tokenizer. This enables high flexibility across models and ensures accurate and flexible chat templating. However, this behavior can be customized. Please find detailed documentation here. Contributing Thank you for contributing! If you have any problems or want to contribute something, please raise an issue or pull request. If you want to add a new model, please contact us via an issue and we can coordinate how to do this. FAQ - Debugging with the environment variable causes the following things - If loading a GGUF or GGML model, this will output a file containing the names, shapes, and types of each tensor. - or - More logging. - Setting the CUDA compiler path: - Set the environment variable during build. - Error: : - Some Linux distributions require compiling with . - Set the environment variable to during build: - Error or symbol not found when using a normal or vison model: - For non-quantized models, you can specify the data type to load and run in. This must be one of , , or to choose based on the device. - What is the minimum supported CUDA compute cap? - The minimum CUDA compute cap is 5.3. - Metal not found error: unable to find utility "metal", not a developer tool or in PATH 1 Install Xcode: 2 Set the active developer directory: - Disabling Metal kernel precompilation: - By default, Metal kernels are precompiled during build time for better performance - To skip Metal kernel precompilation useful for CI or when Metal is not needed, set or - Example: Credits This project would not be possible without the excellent work at https://github.com/huggingface/candle. Additionally, thank you to all contributors! Contributing can range from raising an issue or suggesting a feature to adding some new functionality. <p align="right"> <a href="top">⬆️ Back to Top</a> </p>