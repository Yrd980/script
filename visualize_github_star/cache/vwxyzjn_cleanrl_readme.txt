CleanRL Clean Implementation of RL Algorithms <img src="https://img.shields.io/badge/license-MIT-blue"> !testshttps://github.com/vwxyzjn/cleanrl/actions/workflows/tests.yaml !docshttps://docs.cleanrl.dev/ <img src="https://img.shields.io/discord/767863440248143916?label=discord"> <img src="https://img.shields.io/youtube/channel/views/UCDdC6BIFRI0jvcwuhi3aI6w?style=social"> !Code style: blackhttps://github.com/psf/black !Imports: isorthttps://pycqa.github.io/isort/ <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Models-Huggingface-F8D521"> !Open In Colabhttps://colab.research.google.com/github/vwxyzjn/cleanrl/blob/master/docs/get-started/CleanRLHuggingfaceIntegrationDemo.ipynb CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: 📜 Single-file implementation Every detail about an algorithm variant is put into a single standalone file. For example, our only has 340 lines of code but contains all implementation details on how PPO works with Atari games, so it is a great reference implementation to read for folks who do not wish to read an entire modular library. 📊 Benchmarked Implementation 7+ algorithms and 34+ games at https://benchmark.cleanrl.dev 📈 Tensorboard Logging 🪛 Local Reproducibility via Seeding 🎮 Videos of Gameplay Capturing 🧫 Experiment Management with Weights and Biases 💸 Cloud Integration with docker and AWS You can read more about CleanRL in our JMLR paper and documentation. Notable CleanRL-related projects: corl-team/CORL: Offline RL algorithm implemented in CleanRL style pytorch-labs/LeanRL: Fast optimized PyTorch implementation of CleanRL RL algorithms using CUDAGraphs. > ℹ️ Support for Gymnasium: Farama-Foundation/Gymnasium is the next generation of https://github.com/openai/gym that will continue to be maintained and introduce new features. Please see their announcement for further detail. We are migrating to and the progress can be tracked in vwxyzjn/cleanrl277. > ⚠️ NOTE: CleanRL is not a modular library and therefore it is not meant to be imported. At the cost of duplicate code, we make all implementation details of a DRL algorithm variant easy to understand, so CleanRL comes with its own pros and cons. You should consider using CleanRL if you want to 1 understand all implementation details of an algorithm's variant or 2 prototype advanced features that other modular DRL libraries do not support CleanRL has minimal lines of code so it gives you great debugging experience and you don't have do a lot of subclassing like sometimes in modular DRL libraries. Get started Prerequisites: Python >=3.7.1,<3.11 uv 0.7.9+ To run experiments locally, give the following a try: To use experiment tracking with wandb, run If you are not using , you can install CleanRL with : To run training scripts in other games: You may also use a prebuilt development environment hosted in Gitpod: !Open in Gitpodhttps://gitpod.io/https://github.com/vwxyzjn/cleanrl Algorithms Implemented | Algorithm | Variants Implemented | | ----------- | ----------- | | ✅ Proximal Policy Gradient PPO | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatari.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppocontinuousaction.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatarilstm.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatarienvpool.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatarienvpoolxlajax.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatarienvpoolxlajaxscan.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoprocgen.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppoatarimultigpu.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppopettingzoomaatari.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppocontinuousactionisaacgym/ppocontinuousactionisaacgym.py, docs | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppotrxl/ppotrxl.py, docs | ✅ Deep Q-Learning DQN | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqnatari.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqnjax.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqnatarijax.py, docs | | ✅ Categorical DQN C51 | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/c51.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/c51atari.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/c51jax.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/c51atarijax.py, docs | | ✅ Soft Actor-Critic SAC | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/saccontinuousaction.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sacatari.py, docs | | ✅ Deep Deterministic Policy Gradient DDPG | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ddpgcontinuousaction.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ddpgcontinuousactionjax.py, docs | ✅ Twin Delayed Deep Deterministic Policy Gradient TD3 | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/td3continuousaction.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/td3continuousactionjax.py, docs | | ✅ Phasic Policy Gradient PPG | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppgprocgen.py, docs | | ✅ Random Network Distillation RND | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/pporndenvpool.py, docs | | ✅ Qdagger | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/qdaggerdqnatariimpalacnn.py, docs | | | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/qdaggerdqnatarijaximpalacnn.py, docs | Open RL Benchmark To make our experimental data transparent, CleanRL participates in a related project called Open RL Benchmark, which contains tracked experiments from popular DRL libraries such as ours, Stable-baselines3, openai/baselines, jaxrl, and others. Check out https://benchmark.cleanrl.dev/ for a collection of Weights and Biases reports showcasing tracked DRL experiments. The reports are interactive, and researchers can easily query information such as GPU utilization and videos of an agent's gameplay that are normally hard to acquire in other RL benchmarks. In the future, Open RL Benchmark will likely provide an dataset API for researchers to easily access the data see repo. !docs/static/o1.png !docs/static/o2.png !docs/static/o3.png Support and get involved We have a Discord Community for support. Feel free to ask questions. Posting in Github Issues and PRs are also welcome. Also our past video recordings are available at YouTube Citing CleanRL If you use CleanRL in your work, please cite our technical paper: Acknowledgement CleanRL is a community-powered by project and our contributors run experiments on a variety of hardware. We thank many contributors for using their own computers to run experiments We thank Google's TPU research cloud for providing TPU resources. We thank Hugging Face's cluster for providing GPU resources.