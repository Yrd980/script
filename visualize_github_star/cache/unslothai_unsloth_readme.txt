<div align="center"> <a href="https://unsloth.ai"><picture> <source media="prefers-color-scheme: dark" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png"> <source media="prefers-color-scheme: light" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"> <img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;"> </picture></a> <a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.18B-Alpaca.ipynb"><img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png" width="154"></a> <a href="https://discord.com/invite/unsloth"><img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png" width="165"></a> <a href="https://docs.unsloth.ai"><img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137"></a> Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, & Mistral 2x faster with 80% less VRAM! !https://i.ibb.co/sJ7RhGG/image-41.png </div> ‚ú® Finetune for Free Notebooks are beginner friendly. Read our guide. Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face. | Unsloth supports | Free Notebooks | Performance | Memory use | |-----------|---------|--------|----------| | gpt-oss 20B | ‚ñ∂Ô∏è Start for free-Fine-tuning.ipynb | 1.5x faster | 70% less | | Gemma 3n 4B | ‚ñ∂Ô∏è Start for free-Conversational.ipynb | 1.5x faster | 50% less | | Qwen3 14B | ‚ñ∂Ô∏è Start for free-Reasoning-Conversational.ipynb | 2x faster | 70% less | | Qwen3 4B: GRPO | ‚ñ∂Ô∏è Start for free-GRPO.ipynb | 2x faster | 80% less | | Gemma 3 4B | ‚ñ∂Ô∏è Start for free.ipynb | 1.6x faster | 60% less | | Phi-4 14B | ‚ñ∂Ô∏è Start for free | 2x faster | 70% less | | Llama 3.2 Vision 11B | ‚ñ∂Ô∏è Start for free-Vision.ipynb | 2x faster | 50% less | | Llama 3.1 8B | ‚ñ∂Ô∏è Start for free-Alpaca.ipynb | 2x faster | 70% less | | Mistral v0.3 7B | ‚ñ∂Ô∏è Start for free-Conversational.ipynb | 2.2x faster | 75% less | | Orpheus-TTS 3B | ‚ñ∂Ô∏è Start for free-TTS.ipynb | 1.5x faster | 50% less | - See all our notebooks for: Kaggle, GRPO, TTS & Vision - See all our models and all our notebooks - See detailed documentation for Unsloth here ‚ö° Quickstart - Install with pip recommended for Linux devices: For Windows install instructions, see here. ü¶• Unsloth.ai News - üì£ gpt-oss by OpenAI: For details on our bug fixes, Read our Guide. 20B works on a 14GB GPU and 120B on 65GB VRAM. gpt-oss uploads. - üì£ Gemma 3n by Google: Read Blog. We uploaded GGUFs, 4-bit models. - üì£ Text-to-Speech TTS is now supported, including and STT . - üì£ Qwen3 is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM. - üì£ Introducing Dynamic 2.0 quants that set new benchmarks on 5-shot MMLU & KL Divergence. - üì£ EVERYTHING is now supported - all models BERT, diffusion, Cohere, Mamba, FFT, etc. MultiGPU coming soon. Enable FFT with , 8-bit with . - üì£ Introducing Long-context Reasoning GRPO in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs! - üì£ DeepSeek-R1 - run or fine-tune them with our guide. All model uploads: here. <details> <summary>Click for more news</summary> - üì£ Introducing Unsloth Dynamic 4-bit Quantization! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on Hugging Face here. - üì£ Llama 4 by Meta, including Scout & Maverick are now supported. - üì£ Phi-4 by Microsoft: We also fixed bugs in Phi-4 and uploaded GGUFs, 4-bit. - üì£ Vision models now supported! Llama 3.2 Vision 11B-Vision.ipynb, Qwen 2.5 VL 7B-Vision.ipynb and Pixtral 12B 2409-Vision.ipynb - üì£ Llama 3.3 70B, Meta's latest model is supported. - üì£ We worked with Apple to add Cut Cross Entropy. Unsloth now supports 89K context for Meta's Llama 3.3 70B on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 8B, Unsloth enables 342K context, surpassing its native 128K support. - üì£ We found and helped fix a gradient accumulation bug! Please update Unsloth and transformers. - üì£ We cut memory usage by a further 30% and now support 4x longer context windows! </details> üîó Links and Resources | Type | Links | | ------------------------------- | --------------------------------------- | | üìö Documentation & Wiki | Read Our Docs | | <img width="16" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/LogoofTwitter.svg" />&nbsp; Twitter aka X | Follow us on X| | üíæ Installation | Pip install| | üîÆ Our Models | Unsloth Releases| | ‚úçÔ∏è Blog | Read our Blogs| | <img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/RedditLogo.png" />&nbsp; Reddit | Join our Reddit| ‚≠ê Key Features - Supports full-finetuning, pretraining, 4b-bit, 16-bit and 8-bit training - Supports all transformer-style models including TTS, STT, multimodal, diffusion, BERT and more! - All kernels written in OpenAI's Triton language. Manual backprop engine. - 0% loss in accuracy - no approximation methods - all exact. - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc Check your GPU! GTX 1070, 1080 works, but is slow. - Works on Linux and Windows - If you trained a model with ü¶•Unsloth, you can use this cool sticker! &nbsp; <img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" width="200" align="center" /> üíæ Install Unsloth You can also see our documentation for more detailed installation and updating instructions here. Pip Installation Install with pip recommended for Linux devices: To update Unsloth: See here for advanced pip install instructions. Windows Installation > !warning > Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10 1. Install NVIDIA Video Driver: You should install the latest version of your GPUs driver. Download drivers here: NVIDIA GPU Drive. 3. Install Visual Studio C++: You will need Visual Studio, with C++ installed. By default, C++ is not installed with Visual Studio, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see here. 5. Install CUDA Toolkit: Follow the instructions to install CUDA Toolkit. 6. Install PyTorch: You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. Install PyTorch. 7. Install Unsloth: Notes To run Unsloth directly on Windows: - Install Triton from this Windows fork and follow the instructions here be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12 - In the , set to avoid a crashing issue: Advanced/Troubleshooting For advanced installation instructions or if you see weird errors during installations: 1. Install and . Go to https://pytorch.org to install it. For example 2. Confirm if CUDA is installed correctly. Try . If that fails, you need to install or CUDA drivers. 3. Install manually. You can try installing and seeing if succeeds. Check if succeeded with Go to https://github.com/facebookresearch/xformers. Another option is to install for Ampere GPUs. 4. Double check that your versions of Python, CUDA, CUDNN, , , and are compatible with one another. The PyTorch Compatibility Matrix may be useful. 5. Finally, install and check it with Conda Installation Optional . Select either for CUDA 11.8 or CUDA 12.1. We support . <details> <summary>If you're looking to install Conda in a Linux environment, <a href="https://docs.anaconda.com/miniconda/">read here</a>, or run the below üîΩ</summary> </details> Advanced Pip Installation Pip is a bit more complex since there are dependency issues. The pip command is different for and CUDA versions. For other torch versions, we support , , , , and for CUDA versions, we support and and . For Ampere devices A100, H100, RTX3090 and above, use or or . For example, if you have and , use: Another example, if you have and , use: And other examples: Or, run the below in a terminal to get the optimal pip installation command: Or, run the below manually in a Python REPL: üìú Documentation - Go to our official Documentation for saving to GGUF, checkpointing, evaluation and more! - We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code! - We're in ü§óHugging Face's official docs! Check out the SFT docs and DPO docs! - If you want to download models from the ModelScope community, please use an environment variable: , and install the modelscope library by: . > unslothcli.py also supports to download models and datasets. please remember to use the model and dataset id in the ModelScope community. <a name="RL"></a> üí° Reinforcement Learning RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in ü§óHugging Face's official docs! We're on the GRPO docs and the DPO docs! List of RL notebooks: - Advanced Qwen3 GRPO notebook: Link-GRPO.ipynb - ORPO notebook: Link-ORPO.ipynb - DPO Zephyr notebook: Link-DPO.ipynb - KTO notebook: Link - SimPO notebook: Link <details> <summary>Click for DPO code</summary> </details> ü•á Performance Benchmarking - For our most detailed benchmarks, read our Llama 3.3 Blog. - Benchmarking of Unsloth was also conducted by ü§óHugging Face. We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers q, k, v, o, gate, up, down: | Model | VRAM | ü¶• Unsloth speed | ü¶• VRAM reduction | ü¶• Longer context | üòä Hugging Face + FA2 | |----------------|-------|-----------------|----------------|----------------|--------------------| | Llama 3.3 70B| 80GB | 2x | >75% | 13x longer | 1x | | Llama 3.1 8B | 80GB | 2x | >70% | 12x longer | 1x | Context length benchmarks Llama 3.1 8B max. context length We tested Llama 3.1 8B Instruct and did 4bit QLoRA on all linear layers Q, K, V, O, gate, up and down with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads. | GPU VRAM | ü¶•Unsloth context length | Hugging Face + FA2 | |----------|-----------------------|-----------------| | 8 GB | 2,972 | OOM | | 12 GB | 21,848 | 932 | | 16 GB | 40,724 | 2,551 | | 24 GB | 78,475 | 5,789 | | 40 GB | 153,977 | 12,264 | | 48 GB | 191,728 | 15,502 | | 80 GB | 342,733 | 28,454 | Llama 3.3 70B max. context length We tested Llama 3.3 70B Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers Q, K, V, O, gate, up and down with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads. | GPU VRAM | ü¶•Unsloth context length | Hugging Face + FA2 | |----------|------------------------|------------------| | 48 GB | 12,106 | OOM | | 80 GB | 89,389 | 6,916 | <br> !https://i.ibb.co/sJ7RhGG/image-41.png <br> Citation You can cite the Unsloth repo as follows: Thank You to - The llama.cpp library that lets users save models with Unsloth - The Hugging Face team and their TRL library - Erik for his help adding Apple's ML Cross Entropy in Unsloth - Etherl for adding support for TTS, diffusion and BERT models - And of course for every single person who has contributed or has used Unsloth!