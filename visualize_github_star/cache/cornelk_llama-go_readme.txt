llama-go Inference of Facebook's LLaMA model in Golang with embedded C/C++. Description This project embeds the work of llama.cpp in a Golang binary. The main goal is to run the model using 4-bit quantization using CPU on Consumer-Grade hardware. At startup, the model is loaded and a prompt is offered to enter a prompt, after the results have been printed another prompt can be entered. The program can be quit using ctrl+c. This project was tested on Linux but should be able to get to work on macOS as well. Requirements The memory requirements for the models are approximately: Installation Obtain the original LLaMA model weights and place them in ./models - for example by using the https://github.com/shawwn/llama-dl script to download them. Use the following steps to convert the LLaMA-7B model to a format that is compatible: When running the larger models, make sure you have enough disk space to store all the intermediate files. Usage The settings can be changed at runtime, multiple values are possible: