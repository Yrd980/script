Smol Models ü§è Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance. NEW SmolLM3 Language Model !image Our 3B model outperforms Llama 3.2 3B and Qwen2.5 3B while staying competitive with larger 4B alternatives Qwen3 & Gemma3. Beyond the performance numbers, we're sharing exactly how we built it using public datasets and training frameworks. Ressources: - SmolLM3-Base - SmolLM3 - blog Summary: - 3B model trained on 11T tokens, SoTA at the 3B scale and competitive with 4B models - Fully open model, open weights + full training details including public data mixture and training configs - Instruct model with dual mode reasoning, supporting think/nothink modes - Multilingual support for 6 languages: English, French, Spanish, German, Italian, and Portuguese - Long context up to 128k with NoPE and using YaRN !image üëÅÔ∏è SmolVLM Vision Language Model SmolVLM is our compact multimodal model that can: - Process both images and text and perform tasks like visual QA, image description, and visual storytelling - Handle multiple images in a single conversation - Run efficiently on-device Repository Structure Getting Started SmolLM3 SmolVLM Ecosystem <div align="center"> <img src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/RvHjdlRT5gGQt5mJuhXH9.png" width="700"/> </div> Resources Documentation - SmolLM3 Documentation - SmolLM2 paper - SmolVLM Documentation - Local Inference Guide Pretrained Models - SmolLM3 Models Collection - SmolLM2 Models Collection - SmolVLM Model Datasets - SmolLM3 Pretraining dataset - SmolTalk - Our instruction-tuning dataset - FineMath - Mathematics pretraining dataset - FineWeb-Edu - Educational content pretraining dataset