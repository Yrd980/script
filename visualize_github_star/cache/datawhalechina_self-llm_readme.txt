<div align=center> <img src="./images/head-img.png" > <h1>开源大模型食用指南</h1> </div> <div align="center"> 中文 | English </div> &emsp;&emsp;本项目是一个围绕开源大模型、针对国内初学者、基于 Linux 平台的中国宝宝专属大模型教程，针对各类开源大模型提供包括环境配置、本地部署、高效微调等技能在内的全流程指导，简化开源大模型的部署、使用和应用流程，让更多的普通学生、研究者更好地使用开源大模型，帮助开源、自由的大模型更快融入到普通学习者的生活中。 &emsp;&emsp;本项目的主要内容包括： 1. 基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤； 2. 针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等； 3. 开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等； 4. 开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。 &emsp;&emsp;项目的主要内容就是教程，让更多的学生和未来的从业者了解和熟悉开源大模型的食用方法！任何人都可以提出issue或是提交PR，共同构建维护这个项目。 &emsp;&emsp;想要深度参与的同学可以联系我们，我们会将你加入到项目的维护者中。 > &emsp;&emsp;学习建议：本项目的学习建议是，先学习环境配置，然后再学习模型的部署使用，最后再学习微调。因为环境配置是基础，模型的部署使用是基础，微调是进阶。初学者可以选择Qwen1.5，InternLM2，MiniCPM等模型优先学习。 > &emsp;&emsp;进阶学习推荐 ：如果您在学习完本项目后，希望更深入地理解大语言模型的核心原理，并渴望亲手从零开始训练属于自己的大模型，我们强烈推荐关注 Datawhale 的另一个开源项目—— Happy-LLM 从零开始的大语言模型原理与实践教程 。该项目将带您深入探索大模型的底层机制，掌握完整的训练流程。 > 注：如果有同学希望了解大模型的模型构成，以及从零手写RAG、Agent和Eval等任务，可以学习Datawhale的另一个项目Tiny-Universe，大模型是当下深度学习领域的热点，但现有的大部分大模型教程只在于教给大家如何调用api完成大模型的应用，而很少有人能够从原理层面讲清楚模型结构、RAG、Agent 以及 Eval。所以该仓库会提供全部手写，不采用调用api的形式，完成大模型的 RAG 、 Agent 、Eval 任务。 > 注：考虑到有同学希望在学习本项目之前，希望学习大模型的理论部分，如果想要进一步深入学习 LLM 的理论基础，并在理论的基础上进一步认识、应用 LLM，可以参考 Datawhale 的 so-large-llm课程。 > 注：如果有同学在学习本课程之后，想要自己动手开发大模型应用。同学们可以参考 Datawhale 的 动手学大模型应用开发 课程，该项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，向同学们完整的呈现大模型应用开发流程。 项目意义 &emsp;&emsp;什么是大模型？ >大模型（LLM）狭义上指基于深度学习算法进行训练的自然语言处理（NLP）模型，主要应用于自然语言理解和生成等领域，广义上还包括机器视觉（CV）大模型、多模态大模型和科学计算大模型等。 &emsp;&emsp;百模大战正值火热，开源 LLM 层出不穷。如今国内外已经涌现了众多优秀开源 LLM，国外如 LLaMA、Alpaca，国内如 ChatGLM、BaiChuan、InternLM（书生·浦语）等。开源 LLM 支持用户本地部署、私域微调，每一个人都可以在开源 LLM 的基础上打造专属于自己的独特大模型。 &emsp;&emsp;然而，当前普通学生和用户想要使用这些大模型，需要具备一定的技术能力，才能完成模型的部署和使用。对于层出不穷又各有特色的开源 LLM，想要快速掌握一个开源 LLM 的应用方法，是一项比较有挑战的任务。 &emsp;&emsp;本项目旨在首先基于核心贡献者的经验，实现国内外主流开源 LLM 的部署、使用与微调教程；在实现主流 LLM 的相关部分之后，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。 &emsp;&emsp;我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。 项目受众 &emsp;&emsp;本项目适合以下学习者： 想要使用或体验 LLM，但无条件获得或使用相关 API； 希望长期、低成本、大量应用 LLM； 对开源 LLM 感兴趣，想要亲自上手开源 LLM； NLP 在学，希望进一步学习 LLM； 希望结合开源 LLM，打造领域特色的私域 LLM； 以及最广大、最普通的学生群体。 项目规划及进展 &emsp;&emsp; 本项目拟围绕开源 LLM 应用全流程组织，包括环境配置及使用、部署应用、微调等，每个部分覆盖主流及特点开源 LLM： Example 系列 - Chat-嬛嬛： Chat-甄嬛是利用《甄嬛传》剧本中所有关于甄嬛的台词和语句，基于LLM进行LoRA微调得到的模仿甄嬛语气的聊天语言模型。 - Tianji-天机：天机是一款基于人情世故社交场景，涵盖提示词工程 、智能体制作、 数据获取与模型微调、RAG 数据清洗与使用等全流程的大语言模型系统应用教程。 - AMChat: AM Advanced Mathematics chat 是一个集成了数学知识和高等数学习题及其解答的大语言模型。该模型使用 Math 和高等数学习题及其解析融合的数据集，基于 InternLM2-Math-7B 模型，通过 xtuner 微调，专门设计用于解答高等数学问题。 - 数字生命: 本项目将以我为原型，利用特制的数据集对大语言模型进行微调，致力于创造一个能够真正反映我的个性特征的AI数字人——包括但不限于我的语气、表达方式和思维模式等等，因此无论是日常聊天还是分享心情，它都以一种既熟悉又舒适的方式交流，仿佛我在他们身边一样。整个流程是可迁移复制的，亮点是数据集的制作。 已支持模型 - gpt-oss-20b - gpt-oss-20b vllm 部署调用 - gpt-oss-20b EvalScope 并发评测 - gpt-oss-20b lmstudio 本地部署调用 - gpt-oss-20b Lora 微调及 SwanLab 可视化记录 - gpt-oss-20b DPO 微调及 SwanLab 可视化记录 - GLM-4.1-Thinking - x GLM-4.1V-Thinking vLLM 部署调用 @林恒宇 - x GLM-4.1V-Thinking Gradio部署 @林恒宇 - x GLM-4.1V-Thinking Lora 微调及 SwanLab 可视化记录 @林恒宇 - x GLM-4.1V-Thinking Docker 镜像 @林恒宇 - GLM-4.5-Air - x GLM-4.5-Air vLLM 部署调用 @不要葱姜蒜 - x GLM-4.5-Air EvalScope 智商情商 && 并发评测 @不要葱姜蒜 - x GLM-4.5-Air Lora 微调 @不要葱姜蒜 - x GLM-4.5-Air Ucloud Docker 镜像 @不要葱姜蒜 - ERNIE-4.5 - x ERNIE-4.5-0.3B-PT Lora 微调及 SwanLab 可视化记录 @不要葱姜蒜 - x ERNIE-4.5-0.3B-PT Lora Docker 镜像 @不要葱姜蒜 - Hunyuan-A13B-Instruct - x Hunyuan-A13B-Instruct 模型架构解析 Blog @卓堂越 - x Hunyuan-A13B-Instruct SGLang 部署调用 @fancy - x Hunyuan-A13B-Instruct Lora SwanLab 可视化微调 @谢好冉 - x Hunyuan-A13B-Instruct Lora Docker 镜像 @谢好冉 - Qwen3 - x Qwen3 模型结构解析 Blog @王泽宇 - x Qwen3-8B vllm 部署调用 @李娇娇 - x Qwen3-8B Windows LMStudio 部署调用 @王熠明 - x Qwen3-8B Evalscope 智商情商评测 @李娇娇 - x Qwen3-8B Lora 微调及SwanLab 可视化记录 @姜舒凡 - x Qwen3-30B-A3B 微调及SwanLab 可视化记录 @高立业 - x Qwen3 Think 解密 Blog @樊奇 - x Qwen3-8B Docker 镜像 @高立业 - x Qwen3-0.6B 的小模型有什么用 @不要葱姜蒜 - x Qwen3-1.7B 医学推理式对话微调 及 SwanLab 可视化记录 @林泽毅 - x Qwen3-8B GRPO微调及通过swanlab可视化 @郭宣伯 - Kimi - x Kimi-VL-A3B 技术报告解读 @王泽宇 - x Kimi-VL-A3B-Thinking WebDemo 部署（网页对话助手） @姜舒凡 - Llama4 - x Llama4 对话助手 @姜舒凡 - SpatialLM - x SpatialLM 3D点云理解与目标检测模型部署 @王泽宇 - Hunyuan3D-2 - x Hunyuan3D-2 系列模型部署 @林恒宇 - x Hunyuan3D-2 系列模型代码调用 @林恒宇 - x Hunyuan3D-2 系列模型Gradio部署 @林恒宇 - x Hunyuan3D-2 系列模型API Server @林恒宇 - x Hunyuan3D-2 Docker 镜像 @林恒宇 - Gemma3 - x gemma-3-4b-it FastApi 部署调用 @杜森 - x gemma-3-4b-it ollama + open-webui部署 @孙超 - x gemma-3-4b-it evalscope 智商情商评测 @张龙斐 - x gemma-3-4b-it Lora 微调 @荞麦 - x gemma-3-4b-it Docker 镜像 @姜舒凡 - x gemma-3-4b-it GRPO微调及通过swanlab可视化 @郭宣伯 - DeepSeek-R1-Distill - x DeepSeek-R1-Distill-Qwen-7B FastApi 部署调用 @骆秀韬 - x DeepSeek-R1-Distill-Qwen-7B Langchain 接入 @骆秀韬 - x DeepSeek-R1-Distill-Qwen-7B WebDemo 部署 @骆秀韬 - x DeepSeek-R1-Distill-Qwen-7B vLLM 部署调用 @骆秀韬 - x DeepSeek-R1-0528-Qwen3-8B-GRPO及swanlab可视化 @郭宣伯 - MiniCPM-o-26 - x minicpm-o-2.6 FastApi 部署调用 @林恒宇 - x minicpm-o-2.6 WebDemo 部署 @程宏 - x minicpm-o-2.6 多模态语音能力 @邓恺俊 - x minicpm-o-2.6 可视化 LaTeXOCR Lora 微调 @林泽毅 - InternLM3 - x internlm3-8b-instruct FastApi 部署调用 @苏向标 - x internlm3-8b-instruct Langchian接入 @赵文恺 - x internlm3-8b-instruct WebDemo 部署 @王泽宇 - x internlm3-8b-instruct Lora 微调 @程宏 - x internlm3-8b-instruct o1-like推理链实现 @陈睿 - phi4 - x phi4 FastApi 部署调用 @杜森 - x phi4 langchain 接入 @小罗 - x phi4 WebDemo 部署 @杜森 - x phi4 Lora 微调 @郑远婧 - x phi4 Lora 微调 NER任务 SwanLab 可视化记录版 @林泽毅 - x phi4 GRPO微调及通过swanlab可视化 @郭宣伯 - Qwen2.5-Coder - x Qwen2.5-Coder-7B-Instruct FastApi部署调用 @赵文恺 - x Qwen2.5-Coder-7B-Instruct Langchian接入 @杨晨旭 - x Qwen2.5-Coder-7B-Instruct WebDemo 部署 @王泽宇 - x Qwen2.5-Coder-7B-Instruct vLLM 部署 @王泽宇 - x Qwen2.5-Coder-7B-Instruct Lora 微调 @荞麦 - x Qwen2.5-Coder-7B-Instruct Lora 微调 SwanLab 可视化记录版 @杨卓 - Qwen2-vl - x Qwen2-vl-2B FastApi 部署调用 @姜舒凡 - x Qwen2-vl-2B WebDemo 部署 @赵伟 - x Qwen2-vl-2B vLLM 部署 @荞麦 - x Qwen2-vl-2B Lora 微调 @李柯辰 - x Qwen2-vl-2B Lora 微调 SwanLab 可视化记录版 @林泽毅 - x Qwen2-vl-2B Lora 微调案例 - LaTexOCR @林泽毅 - Qwen2.5 - x Qwen2.5-7B-Instruct FastApi 部署调用 @娄天奥 - x Qwen2.5-7B-Instruct langchain 接入 @娄天奥 - x Qwen2.5-7B-Instruct vLLM 部署调用 @姜舒凡 - x Qwen2.5-7B-Instruct WebDemo 部署 @高立业 - x Qwen2.5-7B-Instruct Lora 微调 @左春生 - x Qwen2.5-7B-Instruct o1-like 推理链实现 @姜舒凡 - x Qwen2.5-7B-Instruct Lora 微调 SwanLab 可视化记录版 @林泽毅 - Apple OpenELM - x OpenELM-3B-Instruct FastApi 部署调用 @王泽宇 - x OpenELM-3B-Instruct Lora 微调 @王泽宇 - Llama31-8B-Instruct - x Llama31-8B-Instruct FastApi 部署调用 @不要葱姜蒜 - x Llama31-8B-Instruct langchain 接入 @张晋 - x Llama31-8B-Instruct WebDemo 部署 @张晋 - x Llama31-8B-Instruct Lora 微调 @不要葱姜蒜 - x 动手转换GGUF模型并使用Ollama本地部署 @Gaoboy - Gemma-2-9b-it - x Gemma-2-9b-it FastApi 部署调用 @不要葱姜蒜 - x Gemma-2-9b-it langchain 接入 @不要葱姜蒜 - x Gemma-2-9b-it WebDemo 部署 @不要葱姜蒜 - x Gemma-2-9b-it Peft Lora 微调 @不要葱姜蒜 - Yuan2.0 - x Yuan2.0-2B FastApi 部署调用 @张帆 - x Yuan2.0-2B Langchain 接入 @张帆 - x Yuan2.0-2B WebDemo部署 @张帆 - x Yuan2.0-2B vLLM部署调用 @张帆 - x Yuan2.0-2B Lora微调 @张帆 - Yuan2.0-M32 - x Yuan2.0-M32 FastApi 部署调用 @张帆 - x Yuan2.0-M32 Langchain 接入 @张帆 - x Yuan2.0-M32 WebDemo部署 @张帆 - DeepSeek-Coder-V2 - x DeepSeek-Coder-V2-Lite-Instruct FastApi 部署调用 @姜舒凡 - x DeepSeek-Coder-V2-Lite-Instruct langchain 接入 @姜舒凡 - x DeepSeek-Coder-V2-Lite-Instruct WebDemo 部署 @Kailigithub - x DeepSeek-Coder-V2-Lite-Instruct Lora 微调 @余洋 - 哔哩哔哩 Index-1.9B - x Index-1.9B-Chat FastApi 部署调用 @邓恺俊 - x Index-1.9B-Chat langchain 接入 @张友东 - x Index-1.9B-Chat WebDemo 部署 @程宏 - x Index-1.9B-Chat Lora 微调 @姜舒凡 - Qwen2 - x Qwen2-7B-Instruct FastApi 部署调用 @康婧淇 - x Qwen2-7B-Instruct langchain 接入 @不要葱姜蒜 - x Qwen2-7B-Instruct WebDemo 部署 @三水 - x Qwen2-7B-Instruct vLLM 部署调用 @姜舒凡 - x Qwen2-7B-Instruct Lora 微调 @散步 - GLM-4 - x GLM-4-9B-chat FastApi 部署调用 @张友东 - x GLM-4-9B-chat langchain 接入 @谭逸珂 - x GLM-4-9B-chat WebDemo 部署 @何至轩 - x GLM-4-9B-chat vLLM 部署 @王熠明 - x GLM-4-9B-chat Lora 微调 @肖鸿儒 - x GLM-4-9B-chat-hf Lora 微调 @付志远 - Qwen 1.5 - x Qwen1.5-7B-chat FastApi 部署调用 @颜鑫 - x Qwen1.5-7B-chat langchain 接入 @颜鑫 - x Qwen1.5-7B-chat WebDemo 部署 @颜鑫 - x Qwen1.5-7B-chat Lora 微调 @不要葱姜蒜 - x Qwen1.5-72B-chat-GPTQ-Int4 部署环境 @byx020119 - x Qwen1.5-MoE-chat Transformers 部署调用 @丁悦 - x Qwen1.5-7B-chat vLLM推理部署 @高立业 - x Qwen1.5-7B-chat Lora 微调 接入SwanLab实验管理平台 @黄柏特 - 谷歌-Gemma - x gemma-2b-it FastApi 部署调用 @东东 - x gemma-2b-it langchain 接入 @东东 - x gemma-2b-it WebDemo 部署 @东东 - x gemma-2b-it Peft Lora 微调 @东东 - phi-3 - x Phi-3-mini-4k-instruct FastApi 部署调用 @郑皓桦 - x Phi-3-mini-4k-instruct langchain 接入 @郑皓桦 - x Phi-3-mini-4k-instruct WebDemo 部署 @丁悦 - x Phi-3-mini-4k-instruct Lora 微调 @丁悦 - CharacterGLM-6B - x CharacterGLM-6B Transformers 部署调用 @孙健壮 - x CharacterGLM-6B FastApi 部署调用 @孙健壮 - x CharacterGLM-6B webdemo 部署 @孙健壮 - x CharacterGLM-6B Lora 微调 @孙健壮 - LLaMA3-8B-Instruct - x LLaMA3-8B-Instruct FastApi 部署调用 @高立业 - X LLaMA3-8B-Instruct langchain 接入 @不要葱姜蒜 - x LLaMA3-8B-Instruct WebDemo 部署 @不要葱姜蒜 - x LLaMA3-8B-Instruct Lora 微调 @高立业 - XVERSE-7B-Chat - x XVERSE-7B-Chat transformers 部署调用 @郭志航 - x XVERSE-7B-Chat FastApi 部署调用 @郭志航 - x XVERSE-7B-Chat langchain 接入 @郭志航 - x XVERSE-7B-Chat WebDemo 部署 @郭志航 - x XVERSE-7B-Chat Lora 微调 @郭志航 - TransNormerLLM - X TransNormerLLM-7B-Chat FastApi 部署调用 @王茂霖 - X TransNormerLLM-7B-Chat langchain 接入 @王茂霖 - X TransNormerLLM-7B-Chat WebDemo 部署 @王茂霖 - x TransNormerLLM-7B-Chat Lora 微调 @王茂霖 - BlueLM Vivo 蓝心大模型 - x BlueLM-7B-Chat FatApi 部署调用 @郭志航 - x BlueLM-7B-Chat langchain 接入 @郭志航 - x BlueLM-7B-Chat WebDemo 部署 @郭志航 - x BlueLM-7B-Chat Lora 微调 @郭志航 - InternLM2 - x InternLM2-7B-chat FastApi 部署调用 @不要葱姜蒜 - x InternLM2-7B-chat langchain 接入 @不要葱姜蒜 - x InternLM2-7B-chat WebDemo 部署 @郑皓桦 - x InternLM2-7B-chat Xtuner Qlora 微调 @郑皓桦 - DeepSeek 深度求索 - x DeepSeek-7B-chat FastApi 部署调用 @不要葱姜蒜 - x DeepSeek-7B-chat langchain 接入 @不要葱姜蒜 - x DeepSeek-7B-chat WebDemo @不要葱姜蒜 - x DeepSeek-7B-chat Lora 微调 @不要葱姜蒜 - x DeepSeek-7B-chat 4bits量化 Qlora 微调 @不要葱姜蒜 - x DeepSeek-MoE-16b-chat Transformers 部署调用 @Kailigithub - x DeepSeek-MoE-16b-chat FastApi 部署调用 @Kailigithub - x DeepSeek-coder-6.7b finetune colab @Swiftie - x Deepseek-coder-6.7b webdemo colab @Swiftie - MiniCPM - x MiniCPM-2B-chat transformers 部署调用 @Kailigithub - x MiniCPM-2B-chat FastApi 部署调用 @Kailigithub - x MiniCPM-2B-chat langchain 接入 @不要葱姜蒜 - x MiniCPM-2B-chat webdemo 部署 @Kailigithub - x MiniCPM-2B-chat Lora && Full 微调 @不要葱姜蒜 - x 官方友情链接：面壁小钢炮MiniCPM教程 @OpenBMB - x 官方友情链接：MiniCPM-Cookbook @OpenBMB - Qwen-Audio - x Qwen-Audio FastApi 部署调用 @陈思州 - x Qwen-Audio WebDemo @陈思州 - Qwen - x Qwen-7B-chat Transformers 部署调用 @李娇娇 - x Qwen-7B-chat FastApi 部署调用 @李娇娇 - x Qwen-7B-chat WebDemo @李娇娇 - x Qwen-7B-chat Lora 微调 @不要葱姜蒜 - x Qwen-7B-chat ptuning 微调 @肖鸿儒 - x Qwen-7B-chat 全量微调 @不要葱姜蒜 - x Qwen-7B-Chat 接入langchain搭建知识库助手 @李娇娇 - x Qwen-7B-chat 低精度训练 @肖鸿儒 - x Qwen-18B-chat CPU 部署 @散步 - Yi 零一万物 - x Yi-6B-chat FastApi 部署调用 @李柯辰 - x Yi-6B-chat langchain接入 @李柯辰 - x Yi-6B-chat WebDemo @肖鸿儒 - x Yi-6B-chat Lora 微调 @李娇娇 - Baichuan 百川智能 - x Baichuan2-7B-chat FastApi 部署调用 @惠佳豪 - x Baichuan2-7B-chat WebDemo @惠佳豪 - x Baichuan2-7B-chat 接入 LangChain 框架 @惠佳豪 - x Baichuan2-7B-chat Lora 微调 @惠佳豪 - InternLM - x InternLM-Chat-7B Transformers 部署调用 @小罗 - x InternLM-Chat-7B FastApi 部署调用 @不要葱姜蒜 - x InternLM-Chat-7B WebDemo @不要葱姜蒜 - x Lagent+InternLM-Chat-7B-V1.1 WebDemo @不要葱姜蒜 - x 浦语灵笔图文理解&创作 WebDemo @不要葱姜蒜 - x InternLM-Chat-7B 接入 LangChain 框架 @Logan Zou - Atom llama2 - x Atom-7B-chat WebDemo @Kailigithub - x Atom-7B-chat Lora 微调 @Logan Zou - x Atom-7B-Chat 接入langchain搭建知识库助手 @陈思州 - x Atom-7B-chat 全量微调 @Logan Zou - ChatGLM3 - x ChatGLM3-6B Transformers 部署调用 @丁悦 - x ChatGLM3-6B FastApi 部署调用 @丁悦 - x ChatGLM3-6B chat WebDemo @不要葱姜蒜 - x ChatGLM3-6B Code Interpreter WebDemo @不要葱姜蒜 - x ChatGLM3-6B 接入 LangChain 框架 @Logan Zou - x ChatGLM3-6B Lora 微调 @肖鸿儒 通用环境配置 - x pip、conda 换源 @不要葱姜蒜 - x AutoDL 开放端口 @不要葱姜蒜 - 模型下载 - x hugging face @不要葱姜蒜 - x hugging face 镜像下载 @不要葱姜蒜 - x modelscope @不要葱姜蒜 - x git-lfs @不要葱姜蒜 - x Openxlab - Issue && PR - x Issue 提交 @肖鸿儒 - x PR 提交 @肖鸿儒 - x fork更新 @肖鸿儒 致谢 核心贡献者 - 宋志学不要葱姜蒜-项目负责人 （Datawhale成员-中国矿业大学北京） - 邹雨衡-项目负责人 （Datawhale成员-对外经济贸易大学） - 姜舒凡（内容创作者-Datawhale成员） - 肖鸿儒 （Datawhale成员-同济大学） - 郭志航（内容创作者） - 林泽毅（内容创作者-SwanLab产品负责人） - 张帆（内容创作者-Datawhale成员） - 王泽宇（内容创作者-太原理工大学-鲸英助教） - 李娇娇 （Datawhale成员） - 高立业（内容创作者-DataWhale成员） - 丁悦 （Datawhale-鲸英助教） - 林恒宇（内容创作者-清华大学-鲸英助教） - 惠佳豪 （Datawhale-宣传大使） - 王茂霖（内容创作者-Datawhale成员） - 孙健壮（内容创作者-对外经济贸易大学） - 东东（内容创作者-谷歌开发者机器学习技术专家） - 荞麦（内容创作者-Datawhale成员） - Kailigithub （Datawhale成员） - 郑皓桦 （内容创作者） - 李柯辰 （Datawhale成员） - 程宏（内容创作者-Datawhale意向成员） - 骆秀韬（内容创作者-Datawhale成员-似然实验室） - 郭宣伯（内容创作者-北京航空航天大学） - 谢好冉（内容创作者-鲸英助教） - 陈思州 （Datawhale成员） - 散步 （Datawhale成员） - 颜鑫 （Datawhale成员） - 杜森（内容创作者-Datawhale成员-南阳理工学院） - Swiftie （小米NLP算法工程师） - 黄柏特（内容创作者-西安电子科技大学） - 张友东（内容创作者-Datawhale成员） - 余洋（内容创作者-Datawhale成员） - 张晋（内容创作者-Datawhale成员） - 娄天奥（内容创作者-中国科学院大学-鲸英助教） - 左春生（内容创作者-Datawhale成员） - 杨卓（内容创作者-西安电子科技大学-鲸英助教） - 小罗 （内容创作者-Datawhale成员） - 邓恺俊（内容创作者-Datawhale成员） - 赵文恺（内容创作者-太原理工大学-鲸英助教） - 付志远（内容创作者-海南大学） - 郑远婧（内容创作者-鲸英助教-福州大学） - 王熠明（内容创作者-Datawhale成员） - 谭逸珂（内容创作者-对外经济贸易大学） - 何至轩（内容创作者-鲸英助教） - 康婧淇（内容创作者-Datawhale成员） - 三水（内容创作者-鲸英助教） - 杨晨旭（内容创作者-太原理工大学-鲸英助教） - 赵伟（内容创作者-鲸英助教） - 苏向标（内容创作者-广州大学-鲸英助教） - 陈睿（内容创作者-西交利物浦大学-鲸英助教） - 张龙斐（内容创作者-鲸英助教） - 孙超（内容创作者-Datawhale成员） - 樊奇（内容创作者-上海交通大学） - 卓堂越（内容创作者-鲸英助教） - fancy（内容创作者-鲸英助教） > 注：排名根据贡献程度排序 其他 - 特别感谢@Sm1les对本项目的帮助与支持 - 部分lora代码和讲解参考仓库：https://github.com/zyds/transformers-code.git - 如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue - 特别感谢以下为教程做出贡献的同学！ <div align=center style="margin-top: 30px;"> <a href="https://github.com/datawhalechina/self-llm/graphs/contributors"> <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" /> </a> </div> Star History <div align=center style="margin-top: 30px;"> <img src="./images/star-history-202572.png"/> </div>