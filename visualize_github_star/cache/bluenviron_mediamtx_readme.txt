<h1 align="center"> <img src="logo.png" alt="MediaMTX / rtsp-simple-server"> <br> <br> !Testhttps://github.com/bluenviron/mediamtx/actions/workflows/codetest.yml !Linthttps://github.com/bluenviron/mediamtx/actions/workflows/codelint.yml !CodeCovhttps://app.codecov.io/gh/bluenviron/mediamtx/tree/main !Releasehttps://github.com/bluenviron/mediamtx/releases !Docker Hubhttps://hub.docker.com/r/bluenviron/mediamtx !API Documentationhttps://bluenviron.github.io/mediamtx </h1> <br> MediaMTX is a ready-to-use and zero-dependency real-time media server and media proxy that allows to publish, read, proxy, record and playback video and audio streams. It has been conceived as a "media router" that routes media streams from one end to the other. Live streams can be published to the server with: |protocol|variants|video codecs|audio codecs| |--------|--------|------------|------------| |SRT clients||H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3| |SRT cameras and servers||H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3| |WebRTC clients|WHIP|AV1, VP9, VP8, H265, H264|Opus, G722, G711 PCMA, PCMU| |WebRTC servers|WHEP|AV1, VP9, VP8, H265, H264|Opus, G722, G711 PCMA, PCMU| |RTSP clients|UDP, TCP, RTSPS|AV1, VP9, VP8, H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video, M-JPEG and any RTP-compatible codec|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G726, G722, G711 PCMA, PCMU, LPCM and any RTP-compatible codec| |RTSP cameras and servers|UDP, UDP-Multicast, TCP, RTSPS|AV1, VP9, VP8, H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video, M-JPEG and any RTP-compatible codec|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G726, G722, G711 PCMA, PCMU, LPCM and any RTP-compatible codec| |RTMP clients|RTMP, RTMPS, Enhanced RTMP|AV1, VP9, H265, H264|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G711 PCMA, PCMU, LPCM| |RTMP cameras and servers|RTMP, RTMPS, Enhanced RTMP|AV1, VP9, H265, H264|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G711 PCMA, PCMU, LPCM| |HLS cameras and servers|Low-Latency HLS, MP4-based HLS, legacy HLS|AV1, VP9, H265, H264|Opus, MPEG-4 Audio AAC| |MPEG-TS|MPEG-TS over UDP, MPEG-TS over Unix sockets|H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3| |RTP|RTP over UDP, RTP over Unix sockets|AV1, VP9, VP8, H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video, M-JPEG and any RTP-compatible codec|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G726, G722, G711 PCMA, PCMU, LPCM and any RTP-compatible codec| |Raspberry Pi Cameras||H264|| Live streams can be read from the server with: |protocol|variants|video codecs|audio codecs| |--------|--------|------------|------------| |SRT||H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3| |WebRTC|WHEP|AV1, VP9, VP8, H265, H264|Opus, G722, G711 PCMA, PCMU| |RTSP|UDP, UDP-Multicast, TCP, RTSPS|AV1, VP9, VP8, H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video, M-JPEG and any RTP-compatible codec|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G726, G722, G711 PCMA, PCMU, LPCM and any RTP-compatible codec| |RTMP|RTMP, RTMPS, Enhanced RTMP|H264|MPEG-4 Audio AAC, MPEG-1/2 Audio MP3| |HLS|Low-Latency HLS, MP4-based HLS, legacy HLS|AV1, VP9, H265, H264|Opus, MPEG-4 Audio AAC| Live streams be recorded and played back with: |format|video codecs|audio codecs| |------|------------|------------| |fMP4|AV1, VP9, H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video, M-JPEG|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3, G711 PCMA, PCMU, LPCM| |MPEG-TS|H265, H264, MPEG-4 Video H263, Xvid, MPEG-1/2 Video|Opus, MPEG-4 Audio AAC, MPEG-1/2 Audio MP3, AC-3| Features Publish live streams to the server Read live streams from the server Streams are automatically converted from a protocol to another Serve several streams at once in separate paths Record streams to disk Playback recorded streams Authenticate users Redirect readers to other RTSP servers load balancing Control the server through the Control API Reload the configuration without disconnecting existing clients hot reloading Read Prometheus-compatible metrics Run hooks external commands when clients connect, disconnect, read or publish streams Compatible with Linux, Windows and macOS, does not require any dependency or interpreter, it's a single executable Note about rtsp-simple-server rtsp-simple-server has been rebranded as MediaMTX. The reason is pretty obvious: this project started as a RTSP server but has evolved into a much more versatile product that is not tied to the RTSP protocol anymore. Nothing will change regarding license, features and backward compatibility. Table of contents Installation Standalone binary Docker image Arch Linux package FreeBSD OpenWrt binary Basic usage Publish to the server By software FFmpeg GStreamer OBS Studio OpenCV Unity Web browsers By device Generic webcam Raspberry Pi Cameras Adding audio Secondary stream By protocol SRT clients SRT cameras and servers WebRTC clients WebRTC servers RTSP clients RTSP cameras and servers RTMP clients RTMP cameras and servers HLS cameras and servers MPEG-TS RTP Read from the server By software FFmpeg GStreamer VLC Unity Web browsers By protocol SRT WebRTC RTSP RTMP HLS Other features Configuration Authentication Internal HTTP-based JWT-based Encrypt the configuration Remuxing, re-encoding, compression Record streams to disk Playback recorded streams Forward streams to other servers Proxy requests to other servers On-demand publishing Route absolute timestamps Expose the server in a subfolder Start on boot Linux OpenWrt Windows Hooks Control API Metrics pprof SRT-specific features Standard stream ID syntax WebRTC-specific features Authenticating with WHIP/WHEP Solving WebRTC connectivity issues Supported browsers HLS-specific features Supported browsers RTSP-specific features Transport protocols Encryption Corrupted frames RTMP-specific features Encryption Compile from source Standard OpenWrt Custom libcamera Cross compile Compile for all supported platforms Docker image License Specifications Related projects Installation There are several installation methods available: standalone binary, Docker image, Arch Linux package, FreeBSD Ports Collection or package and OpenWrt binary. Standalone binary 1. Download and extract a standalone binary from the release page that corresponds to your operating system and architecture. 2. Start the server: Docker image Download and launch the image: Available images: |name|FFmpeg included|RPI Camera support| |----|---------------|------------------| |bluenviron/mediamtx:latest|:x:|:x:| |bluenviron/mediamtx:latest-ffmpeg|:heavycheckmark:|:x:| |bluenviron/mediamtx:latest-rpi|:x:|:heavycheckmark:| |bluenviron/mediamtx:latest-ffmpeg-rpi|:heavycheckmark:|:heavycheckmark:| The flag is mandatory for RTSP to work, since Docker can change the source port of UDP packets for routing reasons, and this doesn't allow the server to identify the senders of the packets. If the cannot be used for instance, it is not compatible with Windows or Kubernetes, you can disable the RTSP UDP transport protocol, add the server IP to and expose ports manually: Arch Linux package If you are running the Arch Linux distribution, run: FreeBSD Available via ports tree or using packages 2025Q2 and later as listed below: OpenWrt binary If the architecture of the OpenWrt device is amd64, armv6, armv7 or arm64, use the standalone binary method and download a Linux binary that corresponds to your architecture. Otherwise, compile the server from source. Basic usage 1. Publish a stream. For instance, you can publish a video/audio file with FFmpeg: or GStreamer: 2. Open the stream. For instance, you can open the stream with VLC: or GStreamer: or FFmpeg: Publish to the server By software FFmpeg FFmpeg can publish a stream to the server in several ways SRT client, SRT server, RTSP client, RTMP client, MPEG-TS over UDP, MPEG-TS over Unix sockets, WebRTC with WHIP, RTP over UDP, rtp over Unix sockets. The recommended one consists in publishing as a RTSP client: The RTSP protocol supports several underlying transport protocols, each with its own characteristics see RTSP-specific features. You can set the transport protocol by using the flag, for instance, in order to use TCP: The resulting stream is available in path . GStreamer GStreamer can publish a stream to the server in several ways SRT client, SRT server, RTSP client, RTMP client, MPEG-TS over UDP, WebRTC with WHIP, RTP over UDP. The recommended one consists in publishing as a RTSP client: If the stream is video only: The RTSP protocol supports several underlying transport protocols, each with its own characteristics see RTSP-specific features. You can set the transport protocol by using the flag: If encryption is enabled, the and options must be specified too: The resulting stream is available in path . GStreamer can also publish a stream by using the WebRTC / WHIP protocol. Make sure that GStreamer version is at least 1.22, and that if the codec is H264, the profile is baseline. Use the element: OBS Studio OBS Studio can publish to the server in several ways SRT client, RTMP client, WebRTC client. The recommended one consists in publishing as a RTMP client. In or in the Auto-configuration Wizard, use the following parameters: Service: Server: Stream key: empty If credentials are in use, use the following parameters: Service: Server: Stream key: empty Save the configuration and click . If you want to generate a stream that can be read with WebRTC, open and use the following parameters: FFmpeg output type: File path or URL: Container format: Check Video encoder: Video encoder settings if any: Audio track: Audio encoder: Then use the button instead of to start streaming. Recent versions of OBS Studio can also publish to the server with the WebRTC / WHIP protocol. Use the following parameters: Service: Server: Bearer Token: when internal authentication is enabled or when JWT-based authentication is enabled Save the configuration and click . The resulting stream is available in path . OpenCV Software which uses the OpenCV library can publish to the server through its GStreamer plugin, as a RTSP client. It must be compiled with GStreamer support, by following this procedure: You can check that OpenCV has been installed correctly by running: Check that the output contains . Videos can be published with : The resulting stream is available in path . Unity Software written with the Unity Engine can publish a stream to the server by using the WebRTC protocol. Create a new Unity project or open an existing open. Open Window -> Package Manager, click on the plus sign, Add Package by name... and insert . Wait for the package to be installed. In the Project window, under , create a new C Script called with this content: In the Hierarchy window, find or create a scene and a camera, then add the script as component of the camera, by dragging it inside the Inspector window. then Press the Play button at the top of the page. The resulting stream is available in path . Web browsers Web browsers can publish a stream to the server by using the WebRTC protocol. Start the server and open the web page: The resulting stream is available in path . This web page can be embedded into another web page by using an iframe: For more advanced setups, you can create and serve a custom web page by starting from the source code of the WebRTC publish page. In particular, there's a ready-to-use, standalone JavaScript class for publishing streams with WebRTC, available in publisher.js. By device Generic webcam If the operating system is Linux-based, edit and replace everything inside section with the following content: If the operating system is Windows: Where is the name of a webcam, that can be obtained with: The resulting stream is available in path . Raspberry Pi Cameras MediaMTX natively supports most of the Raspberry Pi Camera models, enabling high-quality and low-latency video streaming from the camera to any user, for any purpose. There are a couple of requirements: 1. The server must run on a Raspberry Pi, with one of the following operating systems: Raspberry Pi OS Bookworm Raspberry Pi OS Bullseye Both 32 bit and 64 bit architectures are supported. 2. If you are using Raspberry Pi OS Bullseye, make sure that the legacy camera stack is disabled. Type , then go to , , choose . Reboot the system. If you want to run the standard non-Docker version of the server: 1. Download the server executable. If you're using 64-bit version of the operative system, make sure to pick the variant. 2. Edit and replace everything inside section with the following content: The resulting stream is available in path . If you want to run the server inside Docker, you need to use the image and launch the container with some additional flags: Be aware that precompiled binaries and Docker images are not compatible with cameras that require a custom like some ArduCam products, since they come with a bundled . If you want to use a custom one, you can compile from source. Camera settings can be changed by using the parameters: All available parameters are listed in the sample configuration file. Adding audio In order to add audio from a USB microfone, install GStreamer and alsa-utils: list available audio cards with: Sample output: Find the audio card of the microfone and take note of its name, for instance . Then create a new path that takes the video stream from the camera and audio from the microphone: The resulting stream is available in path . Secondary stream It is possible to enable a secondary stream from the same camera, with a different resolution, FPS and codec. Configuration is the same of a primary stream, with set to and parameters adjusted accordingly: The secondary stream is available in path . By protocol SRT clients SRT is a protocol that allows to publish and read live data stream, providing encryption, integrity and a retransmission mechanism. It is usually used to transfer media streams encoded with MPEG-TS. In order to publish a stream to the server with the SRT protocol, use this URL: Replace with any name you want. The resulting stream is available in path . If credentials are enabled, append username and password to : If you need to use the standard stream ID syntax instead of the custom one in use by this server, see Standard stream ID syntax. If you want to publish a stream by using a client in listening mode i.e. with appended to the URL, read the next section. Known clients that can publish with SRT are FFmpeg, GStreamer, OBS Studio. SRT cameras and servers In order to ingest into the server a SRT stream from an existing server, camera or client in listening mode i.e. with appended to the URL, add the corresponding URL into the parameter of a path: WebRTC clients WebRTC is an API that makes use of a set of protocols and methods to connect two clients together and allow them to exchange real-time media or data streams. You can publish a stream with WebRTC and a web browser by visiting: The resulting stream is available in path . WHIP is a WebRTC extensions that allows to publish streams by using a URL, without passing through a web page. This allows to use WebRTC as a general purpose streaming protocol. If you are using a software that supports WHIP for instance, latest versions of OBS Studio, you can publish a stream to the server by using this URL: Regarding authentication, read Authenticating with WHIP/WHEP. Depending on the network it may be difficult to establish a connection between server and clients, read Solving WebRTC connectivity issues. Known clients that can publish with WebRTC and WHIP are FFmpeg, GStreamer, OBS Studio, Unity and Web browsers. WebRTC servers In order to ingest into the server a WebRTC stream from an existing server, add the corresponding WHEP URL into the parameter of a path: RTSP clients RTSP is a protocol that allows to publish and read streams. It supports different underlying transport protocols and allows to encrypt streams in transit see RTSP-specific features. In order to publish a stream to the server with the RTSP protocol, use this URL: The resulting stream is available in path . Known clients that can publish with RTSP are FFmpeg, GStreamer, OBS Studio. RTSP cameras and servers Most IP cameras expose their video stream by using a RTSP server that is embedded into the camera itself. In particular, cameras that are compliant with ONVIF profile S or T meet this requirement. You can use MediaMTX to connect to one or several existing RTSP servers and read their video streams: The resulting stream is available in path . The server supports any number of source streams count is just limited by available hardware resources it's enough to add additional entries to the paths section: RTMP clients RTMP is a protocol that allows to read and publish streams, but is less versatile and less efficient than RTSP and WebRTC doesn't support UDP, doesn't support most RTSP codecs, doesn't support feedback mechanism. Streams can be published to the server by using the URL: The resulting stream is available in path . In case authentication is enabled, credentials can be passed to the server by using the and query parameters: Known clients that can publish with RTMP are FFmpeg, GStreamer, OBS Studio. RTMP cameras and servers You can use MediaMTX to connect to one or several existing RTMP servers and read their video streams: The resulting stream is available in path . HLS cameras and servers HLS is a streaming protocol that works by splitting streams into segments, and by serving these segments and a playlist with the HTTP protocol. You can use MediaMTX to connect to one or several existing HLS servers and read their video streams: The resulting stream is available in path . MPEG-TS The server supports ingesting MPEG-TS streams, shipped in two different ways UDP packets or Unix sockets. In order to read a UDP MPEG-TS stream, edit and replace everything inside section with the following content: Where is the IP for listening packets, in this case a multicast IP. You can generate a UDP multicast MPEG-TS stream with GStreamer: or FFmpeg: The resulting stream is available in path . If the listening IP is a multicast IP, MediaMTX will listen for incoming packets on the default multicast interface, picked by the operating system. It is possible to specify the interface manually by using the parameter: It is possible to restrict who can send packets by using the parameter: Known clients that can publish with UDP and MPEG-TS are FFmpeg and GStreamer. Unix sockets are more efficient than UDP packets and can be used as transport by specifying the scheme: FFmpeg can generate such streams: RTP The server supports ingesting RTP streams, shipped in two different ways UDP packets or Unix sockets. In order to read a UDP RTP stream, edit and replace everything inside section with the following content: must contain a valid SDP, that is a description of the RTP session. FFmpeg can generate a RTP over UDP stream: The stream is available on path . Known clients that can publish with UDP and MPEG-TS are FFmpeg and GStreamer. Unix sockets are more efficient than UDP packets and can be used as transport by specifying the scheme: FFmpeg can generate such streams: Read from the server By software FFmpeg FFmpeg can read a stream from the server in several ways RTSP, RTMP, HLS, WebRTC with WHEP, SRT. The recommended one consists in reading with RTSP: The RTSP protocol supports several underlying transport protocols, each with its own characteristics see RTSP-specific features. You can set the transport protocol by using the flag: GStreamer GStreamer can read a stream from the server in several ways RTSP, RTMP, HLS, WebRTC with WHEP, SRT. The recommended one consists in reading with RTSP: The RTSP protocol supports several underlying transport protocols, each with its own characteristics see RTSP-specific features. You can change the transport protocol by using the flag: If encryption is enabled, set to : GStreamer also supports reading streams with WebRTC/WHEP, although track codecs must be specified in advance through the and parameters. Furthermore, if audio is not present, must be set anyway and must point to a PCMU codec. For instance, the command for reading a video-only H264 stream is: While the command for reading an audio-only Opus stream is: While the command for reading a H264 and Opus stream is: VLC VLC can read a stream from the server in several ways RTSP, RTMP, HLS, SRT. The recommended one consists in reading with RTSP: The RTSP protocol supports several underlying transport protocols, each with its own characteristics see RTSP-specific features. In order to use the TCP transport protocol, use the flag: In order to use the UDP-multicast transport protocol, append to the URL: Ubuntu bug The VLC shipped with Ubuntu 21.10 doesn't support playing RTSP due to a license issue see here and here. To fix the issue, remove the default VLC instance and install the snap version: Encrypted streams At the moment VLC doesn't support reading encrypted RTSP streams. However, you can use a proxy like stunnel or nginx or a local MediaMTX instance to decrypt streams before reading them. Unity Software written with the Unity Engine can read a stream from the server by using the WebRTC protocol. Create a new Unity project or open an existing open. Open Window -> Package Manager, click on the plus sign, Add Package by name... and insert . Wait for the package to be installed. In the Project window, under , create a new C Script called with this content: Edit the variable according to your needs. In the Hierarchy window, find or create a scene. Inside the scene, add a Canvas. Inside the Canvas, add a Raw Image and an Audio Source. Then add the script as component of the canvas, by dragging it inside the Inspector window. then Press the Play button at the top of the page. Web browsers Web browsers can read a stream from the server in several ways WebRTC or HLS. You can read a stream by using the WebRTC protocol by visiting the web page: This web page can be embedded into another web page by using an iframe: For more advanced setups, you can create and serve a custom web page by starting from the source code of the WebRTC read page. In particular, there's a ready-to-use, standalone JavaScript class for reading streams with WebRTC, available in reader.js. Web browsers can also read a stream with the HLS protocol. Latency is higher but there are less problems related to connectivity between server and clients, furthermore the server load can be balanced by using a common HTTP CDN like CloudFront or Cloudflare, and this allows to handle readers in the order of millions. Visit the web page: This web page can be embedded into another web page by using an iframe: For more advanced setups, you can create and serve a custom web page by starting from the source code of the HLS read page. By protocol SRT SRT is a protocol that allows to publish and read live data stream, providing encryption, integrity and a retransmission mechanism. It is usually used to transfer media streams encoded with MPEG-TS. In order to read a stream from the server with the SRT protocol, use this URL: Replace with the path name. If credentials are enabled, append username and password to : If you need to use the standard stream ID syntax instead of the custom one in use by this server, see Standard stream ID syntax. Known clients that can read with SRT are FFmpeg, GStreamer and VLC. WebRTC WebRTC is an API that makes use of a set of protocols and methods to connect two clients together and allow them to exchange real-time media or data streams. You can read a stream with WebRTC and a web browser by visiting: WHEP is a WebRTC extensions that allows to read streams by using a URL, without passing through a web page. This allows to use WebRTC as a general purpose streaming protocol. If you are using a software that supports WHEP, you can read a stream from the server by using this URL: Regarding authentication, read Authenticating with WHIP/WHEP. Depending on the network it may be difficult to establish a connection between server and clients, read Solving WebRTC connectivity issues. Known clients that can read with WebRTC and WHEP are FFmpeg, GStreamer, Unity and web browsers. RTSP RTSP is a protocol that allows to publish and read streams. It supports different underlying transport protocols and allows to encrypt streams in transit see RTSP-specific features. In order to read a stream with the RTSP protocol, use this URL: Known clients that can read with RTSP are FFmpeg, GStreamer and VLC. Latency The RTSP protocol doesn't introduce any latency by itself. Latency is usually introduced by clients, that put frames in a buffer to compensate network fluctuations. In order to decrease latency, the best way consists in tuning the client. For instance, in VLC, latency can be decreased by decreasing the Network caching parameter, that is available in the Open network stream dialog or alternatively can be set with the command line: RTMP RTMP is a protocol that allows to read and publish streams, but is less versatile and less efficient than RTSP and WebRTC doesn't support UDP, doesn't support most RTSP codecs, doesn't support feedback mechanism. Streams can be read from the server by using the URL: In case authentication is enabled, credentials can be passed to the server by using the and query parameters: Known clients that can read with RTMP are FFmpeg, GStreamer and VLC. HLS HLS is a protocol that works by splitting streams into segments, and by serving these segments and a playlist with the HTTP protocol. You can use MediaMTX to generate a HLS stream, that is accessible through a web page: and can also be accessed without using the browsers, by software that supports the HLS protocol for instance VLC or MediaMTX itself by using this URL: Known clients that can read with HLS are FFmpeg, GStreamer, VLC and web browsers. LL-HLS Low-Latency HLS is a recently standardized variant of the protocol that allows to greatly reduce playback latency. It works by splitting segments into parts, that are served before the segment is complete. LL-HLS is enabled by default. If the stream is not shown correctly, try tuning the hlsPartDuration parameter, for instance: Compatibility with Apple devices In order to correctly display Low-Latency HLS streams in Safari running on Apple devices iOS or macOS, a TLS certificate is needed and can be generated with OpenSSL: Set the , and parameters in the configuration file: Keep also in mind that not all H264 video streams can be played on Apple Devices due to some intrinsic properties distance between I-Frames, profile. If the video can't be played correctly, you can either: re-encode it by following instructions in this README disable the Low-latency variant of HLS and go back to the legacy variant: Latency in HLS, latency is introduced since a client must wait for the server to generate segments before downloading them. This latency amounts to 500ms-3s when the low-latency HLS variant is enabled and it is by default, otherwise amounts to 1-15secs. To decrease the latency, you can: try decreasing the hlsPartDuration parameter try decreasing the hlsSegmentDuration parameter The segment duration is influenced by the interval between the IDR frames of the video track. An IDR frame is a frame that can be decoded independently from the others. The server changes the segment duration in order to include at least one IDR frame into each segment. Therefore, you need to decrease the interval between the IDR frames. This can be done in two ways: if the stream is being hardware-generated i.e. by a camera, there's usually a setting called Key-Frame Interval in the camera configuration page otherwise, the stream must be re-encoded. It's possible to tune the IDR frame interval by using ffmpeg's -g option: Other features Configuration All the configuration parameters are listed and commented in the configuration file. There are 3 ways to change the configuration: 1. By editing the file, that is included into the release bundle available in the root folder of the Docker image ; it can be overridden in this way: The configuration can be changed dynamically when the server is running hot reloading by writing to the configuration file. Changes are detected and applied without disconnecting existing clients, whenever it's possible. 2. By overriding configuration parameters with environment variables, in the format , where is the uppercase name of a parameter. For instance, the parameter can be overridden in the following way: Parameters that have array as value can be overridden by setting a comma-separated list. For example: Parameters in maps can be overridden by using underscores, in the following way: This method is particularly useful when using Docker; any configuration parameter can be changed by passing environment variables with the flag: 3. By using the Control API. Authentication Internal The server provides three methods to authenticate users: Internal: users are stored in the configuration file HTTP-based: an external HTTP URL is contacted to perform authentication JWT: an external identity server provides authentication through JWTs The internal authentication method is the default one. Users are stored inside the configuration file, in this format: Only clients that provide username and passwords will be able to perform a certain action: If storing plain credentials in the configuration file is a security problem, username and passwords can be stored as hashed strings. The Argon2 and SHA256 hashing algorithms are supported. To use Argon2, the string must be hashed using Argon2id recommended or Argon2i: Then stored with the prefix: To use SHA256, the string must be hashed with SHA256 and encoded with base64: Then stored with the prefix: WARNING: enable encryption or use a VPN to ensure that no one is intercepting the credentials in transit. HTTP-based Authentication can be delegated to an external HTTP server: Each time a user needs to be authenticated, the specified URL will be requested with the POST method and this payload: If the URL returns a status code that begins with i.e. , authentication is successful, otherwise it fails. Be aware that it's perfectly normal for the authentication server to receive requests with empty users and passwords, i.e.: This happens because RTSP clients don't provide credentials until they are asked to. In order to receive the credentials, the authentication server must reply with status code , then the client will send credentials. Some actions can be excluded from the process: JWT-based Authentication can be delegated to an external identity server, that is capable of generating JWTs and provides a JWKS endpoint. With respect to the HTTP-based method, this has the advantage that the external server is contacted once, and not for every request, greatly improving performance. In order to use the JWT-based authentication method, set and : The JWT is expected to contain a claim, with a list of permissions in the same format as the one of user permissions: Clients are expected to pass the JWT in one of the following ways from best to worst: 1. Through the HTTP header. This is possible if the protocol or feature is based on HTTP, like HLS, WebRTC, API, Metrics, pprof. 2. As password. Username is arbitrary. 3. As query parameter in the URL, with the key. This method is discouraged since the JWT is publicly shared when the URL is shared, causing a security issue. These are the recommended methods for each client: |client|protocol|method|notes| |------|--------|------|-----| |Web browsers|HLS|Authorization: Bearer|| |Web browsers|WebRTC|Authorization: Bearer|| |OBS Studio|WebRTC|Authorization: Bearer|| |OBS Studio|RTMP|Query parameter|| |FFmpeg|RTSP|Query parameter|password is truncated and cannot be used| |FFmpeg|RTMP|unsupported|Passwords and query parameters are currently truncated to 1024 characters by FFmpeg, so it's impossible to use FFMPEG+RTMP+JWT| |GStreamer|RTSP|Password|| |GStreamer|RTMP|Query parameter|| |any|SRT|unsupported|SRT truncates passwords and query parameters to 512 characters, so it's impossible to use SRT+JWT. See 3430| Here's a tutorial on how to setup the Keycloak identity server in order to provide JWTs: 1. Start Keycloak: 2. Open the Keycloak administration console on http://localhost:8080, click on master in the top left corner, create realm, set realm name to , Save 3. Open page Client scopes, create client scope, set name to , Save 4. Open tab Mappers, Configure a new Mapper, User Attribute Name: User Attribute: Token Claim Name: Claim JSON Type: Multivalued: Save 5. Open page Clients, Create client, set Client ID to , Next, Client authentication , Next, Save 6. Open tab Credentials, copy client secret somewhere 7. Open tab Client scopes, Add client scope, Select , Add, Default 8. Open page Users, Add user, Username , Tab credentials, Set password, pick a password, Save 9. Open tab Attributes, Add an attribute Key: Value: You can add as many attributes with key as you want, each with a single permission in it 10. In MediaMTX, use the following URL: 11. Perform authentication on Keycloak: The JWT is inside the key of the response: Encrypt the configuration The configuration file can be entirely encrypted for security purposes by using the function of the NaCL function. An online tool for performing this operation is available here. After performing the encryption, put the base64-encoded result into the configuration file, and launch the server with the variable: Remuxing, re-encoding, compression To change the format, codec or compression of a stream, use FFmpeg or GStreamer together with MediaMTX. For instance, to re-encode an existing stream, that is available in the path, and publish the resulting stream in the path, edit and replace everything inside section with the following content: Record streams to disk To save available streams to disk, set the and the parameter in the configuration file: All available recording parameters are listed in the sample configuration file. Be aware that not all codecs can be saved with all formats, as described in the compatibility matrix at the beginning of the README. To upload recordings to a remote location, you can use MediaMTX together with rclone, a command line tool that provides file synchronization capabilities with a huge variety of services including S3, FTP, SMB, Google Drive: 1. Download and install rclone. 2. Configure rclone: 3. Place into the and hooks: If you want to delete local segments after they are uploaded, replace with . Playback recorded streams Existing recordings can be served to users through a dedicated HTTP server, that can be enabled inside the configuration: The server provides an endpoint to list recorded timespans: Where: mypath is the name of a path start optional is the start date in RFC3339 format end optional is the end date in RFC3339 format The server will return a list of timespans in JSON format: The server provides an endpoint to download recordings: Where: mypath is the path name start is the start date in RFC3339 format duration is the maximum duration of the recording in seconds format optional is the output format of the stream. Available values are "fmp4" default and "mp4" All parameters must be url-encoded. For instance: The resulting stream uses the fMP4 format, that is natively compatible with any browser, therefore its URL can be directly inserted into a \<video> tag: The fMP4 format may offer limited compatibility with some players. To fix the issue, it's possible to use the standard MP4 format, by adding to a request: Forward streams to other servers To forward incoming streams to another server, use FFmpeg inside the parameter: Proxy requests to other servers The server allows to proxy incoming requests to other servers or cameras. This is useful to expose servers or cameras behind a NAT. Edit and replace everything inside section with the following content: All requests addressed to will be forwarded to and so on. On-demand publishing Edit and replace everything inside section with the following content: The command inserted into will start only when a client requests the path , therefore the file will start streaming only when requested. Route absolute timestamps Some streaming protocols allow to route absolute timestamps, associated with each frame, that are useful for synchronizing several video or data streams together. In particular, MediaMTX supports receiving absolute timestamps with the following protocols and devices: HLS through the tag in playlists RTSP through RTCP reports, when is in settings WebRTC through RTCP reports, when is in settings Raspberry Pi Camera and supports sending absolute timestamps with the following protocols: HLS through the tag in playlists RTSP through RTCP reports WebRTC through RTCP reports A library that can read absolute timestamps with HLS is gohlslib. A library that can read absolute timestamps with RTSP is gortsplib. A browser can read read absolute timestamps with WebRTC if it exposes the estimatedPlayoutTimestamp statistic. Expose the server in a subfolder HTTP-based services WebRTC, HLS, Control API, Playback Server, Metrics, pprof can be exposed in a subfolder of an existing HTTP server or reverse proxy. The reverse proxy must be able to intercept HTTP requests addressed to MediaMTX and corresponding responses, and perform the following changes: The subfolder path must be stripped from request paths. For instance, if the server is exposed behind and the reverse proxy receives a request with path , this has to be changed into . Any header in responses must be prefixed with the subfolder path. For instance, if the server is exposed behind and the server sends a response with , this has to be changed into . If nginx is the reverse proxy, this can be achieved with the following configuration: If Apache HTTP Server is the reverse proxy, this can be achieved with the following configuration: If Caddy is the reverse proxy, this can be achieved with the following configuration: Start on boot Linux On most Linux distributions including Ubuntu and Debian, but not OpenWrt, systemd is in charge of managing services and starting them on boot. Move the server executable and configuration in global folders: Create a systemd service: If SELinux is enabled for instance in case of RedHat, Rocky, CentOS++, add correct security context: Enable and start the service: OpenWrt Move the server executable and configuration in global folders: Create a procd service: Enable and start the service: Read the server logs: Windows Download the WinSW v2 executable and place it into the same folder of . In the same folder, create a file named with this content: Open a terminal, navigate to the folder and run: The server is now installed as a system service and will start at boot time. Hooks The server allows to specify commands that are executed when a certain event happens, allowing the propagation of events to external software. allows to run a command when a client connects to the server: allows to run a command when a client disconnects from the server: allows to run a command when a path is initialized. This can be used to publish a stream when the server is launched: allows to run a command when a path is requested by a reader. This can be used to publish a stream on demand: allows to run a command when there are no readers anymore: allows to run a command when a stream is ready to be read: allows to run a command when a stream is not available anymore: allows to run a command when a client starts reading: allows to run a command when a client stops reading: allows to run a command when a recording segment is created: allows to run a command when a recording segment is complete: Control API The server can be queried and controlled with an API, that can be enabled by setting the parameter in the configuration: To obtain a list of of active paths, run: The control API is documented in the OpenAPI / Swagger file and in a dedicated site. Be aware that by default the Control API is accessible by localhost only; to increase visibility or add authentication, check Authentication. Metrics A metrics exporter, compatible with Prometheus, can be enabled with the parameter ; then the server can be queried for metrics with Prometheus or with a simple HTTP request: Obtaining: Metrics can be tuned and filtered by using query parameters: : show metrics of a certain type only where TYPE can be , , , , , , , , , : show metrics belonging to a specific path only : show metrics belonging to a specific HLS muxer only show metrics belonging to a specific RTSP connection only : show metrics belonging to a specific RTSP session only show metrics belonging to a specific RTSPS connection only : show metrics belonging to a specific RTSPS session only show metrics belonging to a specific RTMP connection only show metrics belonging to a specific RTMPS connection only show metrics belonging to a specific SRT connection only show metrics belonging to a specific WebRTC session only pprof A performance monitor, compatible with pprof, can be enabled with the parameter ; then the server can be queried for metrics with pprof-compatible tools, like: SRT-specific features Standard stream ID syntax In SRT, the stream ID is a string that is sent to the remote part in order to advertise what action the caller is gonna do publish or read, the path and the credentials. All these informations have to be encoded into a single string. This server supports two stream ID syntaxes, a custom one that is the one reported in rest of the README and also a standard one proposed by the authors of the protocol and enforced by some hardware. The standard syntax can be used in this way: Where: key contains the action or key contains the path key contains the username key contains the password WebRTC-specific features Authenticating with WHIP/WHEP When using WHIP or WHEP to establish a WebRTC connection, there are several ways to provide credentials. If internal authentication or HTTP-based authentication is in use, username and password can be passed through the HTTP header: Where is the base64 encoding of "user:pass". When the header cannot be used for instance, in software like OBS Studio, credentials can be passed through the header, where value is the concatenation of username and password, separated by a colon: If JWT-based authentication is in use, the JWT can be passed through the header: Solving WebRTC connectivity issues If the server is hosted inside a container or is behind a NAT, additional configuration is required in order to allow the two WebRTC parts server and client to establish a connection. Make sure that includes your public IPs, that are IPs that can be used by clients to reach the server. If clients are on the same LAN as the server, add the LAN address of the server. If clients are coming from the internet, add the public IP address of the server, or alternatively a DNS name, if you have one. You can add several values to support all scenarios: If there's a NAT / container between server and clients, it must be configured to route all incoming UDP packets on port 8189 to the server. If you're using Docker, this can be achieved with the flag: If you still have problems, the UDP protocol might be blocked by a firewall. Enable the TCP protocol by enabling the local TCP listener: If there's a NAT / container between server and clients, it must be configured to route all incoming TCP packets on port 8189 to the server. If you still have problems, add a STUN server. When a STUN server is in use, server IP is obtained automatically and connections are established with the "UDP hole punching" technique, that uses a random UDP port that does not need to be open. For instance: If you really still have problems, you can force all WebRTC/ICE connections to pass through a TURN server, like coturn, that must be configured externally. The server address and credentials must be set in the configuration file: Where user and pass are the username and password of the server. Note that port is not optional. If the server uses a secret-based authentication for instance, coturn with the use-auth-secret option, it must be configured by using AUTHSECRET as username, and the secret as password: where secret is the secret of the TURN server. MediaMTX will generate a set of credentials by using the secret, and credentials will be sent to clients before the WebRTC/ICE connection is established. In some cases you may want the browser to connect using TURN servers but have mediamtx not using TURN for example if the TURN server is on the same network as mediamtx. To allow this you can configure the TURN server to be client only: Supported browsers The server can ingest and broadcast with WebRTC a wide variety of video and audio codecs that are listed at the beginning of the README, but not all browsers can publish and read all codecs due to internal limitations that cannot be overcome by this or any other server. In particular, reading and publishing H265 tracks with WebRTC was not possible until some time ago due to lack of browser support. The situation improved recently and can be described as following: Safari on iOS and macOS fully supports publishing and reading H265 tracks Chrome on Windows supports publishing and reading H265 tracks when a GPU is present and when the browser is launched with the following flags: We are expecting these flags to become redundant in the future and the feature to be turned on by default. You can check what codecs your browser can publish or read with WebRTC by using this tool. If you want to support most browsers, you can to re-encode the stream by using H264 and Opus codecs, for instance by using FFmpeg: HLS-specific features Supported browsers The server can produce HLS streams with a variety of video and audio codecs that are listed at the beginning of the README, but not all browsers can read all codecs due to internal limitations that cannot be overcome by this or any other server. You can check what codecs your browser can read with HLS by using this tool. If you want to support most browsers, you can to re-encode the stream by using H264 and AAC codecs, for instance by using FFmpeg: RTSP-specific features Transport protocols The RTSP protocol supports different underlying transport protocols, that are chosen by clients during the handshake with the server: UDP: the most performant, but doesn't work when there's a NAT/firewall between server and clients. UDP-multicast: allows to save bandwidth when clients are all in the same LAN, by sending packets once to a fixed multicast IP. TCP: the most versatile. The default transport protocol is UDP. To change the transport protocol, you have to tune the configuration of your client of choice. Encryption Incoming and outgoing RTSP streams can be encrypted with TLS, obtaining the RTSPS protocol. A TLS certificate is needed and can be generated with OpenSSL: Edit and set the , and serverCert parameters: Streams can be published and read with the scheme and the port: Corrupted frames In some scenarios, when publishing or reading from the server with RTSP, frames can get corrupted. This can be caused by several reasons: When the transport protocol is UDP which is default one, packets sent to the server might get discarded because the UDP read buffer size is too small. This can be noticed in logs through the "RTP packets lost" message. Try increasing the UDP read buffer size: If the source of the stream is a camera: Both these options require the system parameter to be equal or greater than : When the transport protocol is UDP which is the default one, packets sent from the server to readers might get discarded because the write queue is too small. This can be noticed in logs through the "reader is too slow" message. Try increasing the write queue: The stream is too big and it can't be transmitted correctly with the UDP transport protocol. UDP is more performant, faster and more efficient than TCP, but doesn't have a retransmission mechanism, that is needed in case of streams that need a large bandwidth. A solution consists in switching to TCP: In case the source is a camera: The stream throughput is too big to be handled by the network between server and readers. Upgrade the network or decrease the stream bitrate by re-encoding it. RTMP-specific features Encryption RTMP connections can be encrypted with TLS, obtaining the RTMPS protocol. A TLS certificate is needed and can be generated with OpenSSL: Edit mediamtx.yml and set the , and parameters: Streams can be published and read with the rtmps scheme and the 1937 port: Be aware that RTMPS is currently unsupported by all major players. However, you can use a proxy like stunnel or nginx or a dedicated MediaMTX instance to decrypt streams before reading them. Compile from source Standard Install git and Go &ge; 1.24. Clone the repository, enter into the folder and start the building process: The command will produce the binary. OpenWrt The compilation procedure is the same as the standard one. On the OpenWrt device, install git and Go: Clone the repository, enter into the folder and start the building process: The command will produce the binary. If the OpenWrt device doesn't have enough resources to compile, you can cross compile from another machine. Custom libcamera If you need to use a custom or external libcamera when interacting with the Raspberry Pi Camera, you have to compile mediamtx-rpicamera before compiling the server. Instructions are present in the repository. Cross compile Cross compilation allows to build an executable for a target machine from another machine with different operating system or architecture. This is useful in case the target machine doesn't have enough resources for compilation or if you don't want to install the compilation dependencies on it. On the machine you want to use to compile, install git and Go &ge; 1.24. Clone the repository, enter into the folder and start the building process: Replace and with the operating system and architecture of your target machine. A list of all supported combinations can be obtained with: For instance: In case of the architecture, there's an additional flag available, , that allows to set the ARM version: In case of the architecture, there's an additional flag available, , that allows to set additional parameters: The command will produce the binary. Compile for all supported platforms Install Docker and launch: The command will produce tarballs in folder . Docker image The official Docker image can be recompiled by following these steps: 1. Build binaries for all supported platforms: 2. Build the image by using one of the Dockerfiles inside the folder: A Dockerfile is available for each image variant , , , . License All the code in this repository is released under the MIT License. Compiled binaries include some third-party dependencies: all the Golang-based dependencies listed into the go.mod file, which are all released under either the MIT license, BSD 3-Clause license or Apache License 2.0. hls.js, released under the Apache License 2.0. mediamtx-rpicamera, which is released under the same license of MediaMTX but includes some third-party dependencies. Specifications |name|area| |----|----| |RTSP / RTP / RTCP specifications|RTSP| |HLS specifications|HLS| |Action Message Format - AMF 0|RTMP| |FLV|RTMP| |RTMP|RTMP| |Enhanced RTMP v2|RTMP| |WebRTC: Real-Time Communication in Browsers|WebRTC| |RFC8835, Transports for WebRTC|WebRTC| |RFC7742, WebRTC Video Processing and Codec Requirements|WebRTC| |RFC7847, WebRTC Audio Codec and Processing Requirements|WebRTC| |RFC7875, Additional WebRTC Audio Codecs for Interoperability|WebRTC| |H.265 Profile for WebRTC|WebRTC| |WebRTC HTTP Ingestion Protocol WHIP|WebRTC| |WebRTC HTTP Egress Protocol WHEP|WebRTC| |The SRT Protocol|SRT| |Codec specifications|codecs| |Golang project layout|project layout| Related projects gortsplib RTSP library used internally gohlslib HLS library used internally mediacommon codecs and formats library used internally mediamtx-rpicamera Raspberry Pi Camera component datarhei/gosrt SRT library used internally pion/webrtc WebRTC library used internally pion/sdp SDP library used internally pion/rtp RTP library used internally pion/rtcp RTCP library used internally go-astits MPEG-TS library used internally go-mp4 MP4 library used internally hls.js browser-side HLS library used internally