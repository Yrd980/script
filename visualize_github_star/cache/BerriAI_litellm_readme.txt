<h1 align="center"> 🚅 LiteLLM </h1> <p align="center"> <p align="center"> <a href="https://render.com/deploy?repo=https://github.com/BerriAI/litellm" target="blank" rel="nofollow"><img src="https://render.com/images/deploy-to-render-button.svg" alt="Deploy to Render"></a> <a href="https://railway.app/template/HLP0Ub?referralCode=jch2ME"> <img src="https://railway.app/button.svg" alt="Deploy on Railway"> </a> </p> <p align="center">Call all LLM APIs using the OpenAI format Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc. <br> </p> <h4 align="center"><a href="https://docs.litellm.ai/docs/simpleproxy" target="blank">LiteLLM Proxy Server LLM Gateway</a> | <a href="https://docs.litellm.ai/docs/hosted" target="blank"> Hosted Proxy Preview</a> | <a href="https://docs.litellm.ai/docs/enterprise"target="blank">Enterprise Tier</a></h4> <h4 align="center"> <a href="https://pypi.org/project/litellm/" target="blank"> <img src="https://img.shields.io/pypi/v/litellm.svg" alt="PyPI Version"> </a> <a href="https://www.ycombinator.com/companies/berriai"> <img src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square" alt="Y Combinator W23"> </a> <a href="https://wa.link/huol9n"> <img src="https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square" alt="Whatsapp"> </a> <a href="https://discord.gg/wuPM9dRgDw"> <img src="https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square" alt="Discord"> </a> <a href="https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3"> <img src="https://img.shields.io/static/v1?label=Chat%20on&message=Slack&color=black&logo=Slack&style=flat-square" alt="Slack"> </a> </h4> LiteLLM manages: - Translate inputs to provider's , , and endpoints - Consistent output, text responses will always be available at - Retry/fallback logic across multiple deployments e.g. Azure/OpenAI - Router - Set Budgets & Rate limits per project, api key, model LiteLLM Proxy Server LLM Gateway Jump to LiteLLM Proxy LLM Gateway Docs <br> Jump to Supported LLM Providers 🚨 Stable Release: Use docker images with the tag. These have undergone 12 hour load tests, before being published. More information about the release cycle here Support for more providers. Missing a provider or LLM Platform, raise a feature request. Usage Docs > !IMPORTANT > LiteLLM v1.0.0 now requires . Migration guide here > LiteLLM v1.40.14+ now requires . No changes required. <a target="blank" href="https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLMGettingStarted.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> Response OpenAI Format Call any model supported by a provider, with . There might be provider-specific details here, so refer to provider docs for more information Async Docs Streaming Docs liteLLM supports streaming the model response back, pass to get a streaming iterator in response. Streaming is supported for all models Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc. Response chunk OpenAI Format Logging Observability Docs LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack LiteLLM Proxy Server LLM Gateway - Docs Track spend + Load Balance across multiple projects Hosted Proxy Preview The proxy provides: 1. Hooks for auth 2. Hooks for logging 3. Cost tracking 4. Rate Limiting 📖 Proxy Endpoints - Swagger Docs Quick Start Proxy - CLI Step 1: Start litellm proxy Step 2: Make ChatCompletions Request to Proxy > !IMPORTANT > 💡 Use LiteLLM Proxy with Langchain Python, JS, OpenAI SDK Python, JS Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl Proxy Key Management Docs Connect the proxy with a Postgres DB to create proxy keys UI on on your proxy server !ui3 Set budgets and rate limits across multiple projects Request Expected Response Supported Providers Docs | Provider | Completion | Streaming | Async Completion | Async Streaming | Async Embedding | Async Image Generation | |-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------| | openai | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | | Meta - Llama API | ✅ | ✅ | ✅ | ✅ | | | | azure | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | | AI/ML API | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | | aws - sagemaker | ✅ | ✅ | ✅ | ✅ | ✅ | | | aws - bedrock | ✅ | ✅ | ✅ | ✅ | ✅ | | | google - vertexai | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | | google - palm | ✅ | ✅ | ✅ | ✅ | | | | google AI Studio - gemini | ✅ | ✅ | ✅ | ✅ | | | | mistral ai api | ✅ | ✅ | ✅ | ✅ | ✅ | | | cloudflare AI Workers | ✅ | ✅ | ✅ | ✅ | | | | cohere | ✅ | ✅ | ✅ | ✅ | ✅ | | | anthropic | ✅ | ✅ | ✅ | ✅ | | | | empower | ✅ | ✅ | ✅ | ✅ | | huggingface | ✅ | ✅ | ✅ | ✅ | ✅ | | | replicate | ✅ | ✅ | ✅ | ✅ | | | | togetherai | ✅ | ✅ | ✅ | ✅ | | | | openrouter | ✅ | ✅ | ✅ | ✅ | | | | ai21 | ✅ | ✅ | ✅ | ✅ | | | | baseten | ✅ | ✅ | ✅ | ✅ | | | | vllm | ✅ | ✅ | ✅ | ✅ | | | | nlpcloud | ✅ | ✅ | ✅ | ✅ | | | | aleph alpha | ✅ | ✅ | ✅ | ✅ | | | | petals | ✅ | ✅ | ✅ | ✅ | | | | ollama | ✅ | ✅ | ✅ | ✅ | ✅ | | | deepinfra | ✅ | ✅ | ✅ | ✅ | | | | perplexity-ai | ✅ | ✅ | ✅ | ✅ | | | | Groq AI | ✅ | ✅ | ✅ | ✅ | | | | Deepseek | ✅ | ✅ | ✅ | ✅ | | | | anyscale | ✅ | ✅ | ✅ | ✅ | | | | IBM - watsonx.ai | ✅ | ✅ | ✅ | ✅ | ✅ | | | voyage ai | | | | | ✅ | | | xinference Xorbits Inferencehttps://docs.litellm.ai/docs/providers/xinference | | | | | ✅ | | | FriendliAI | ✅ | ✅ | ✅ | ✅ | | | | Galadriel | ✅ | ✅ | ✅ | ✅ | | | | GradientAI | ✅ | ✅ | | | | | | Novita AI | ✅ | ✅ | ✅ | ✅ | | | | Featherless AI | ✅ | ✅ | ✅ | ✅ | | | | Nebius AI Studio | ✅ | ✅ | ✅ | ✅ | ✅ | | Read the Docs Contributing Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and LLM integrations are both accepted and highly encouraged! Quick start: → → → → See our comprehensive Contributing Guide CONTRIBUTING.md for detailed instructions. Enterprise For companies that need better security, user management and professional support Talk to founders This covers: - ✅ Features under the LiteLLM Commercial License: - ✅ Feature Prioritization - ✅ Custom Integrations - ✅ Professional Support - Dedicated discord + slack - ✅ Custom SLAs - ✅ Secure access with Single Sign-On Contributing We welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help. Quick Start for Contributors For detailed contributing guidelines, see CONTRIBUTING.md. Code Quality / Linting LiteLLM follows the Google Python Style Guide. Our automated checks include: - Black for code formatting - Ruff for linting and code quality - MyPy for type checking - Circular import detection - Import safety checks Run all checks locally: All these checks must pass before your PR can be merged. Support / talk with founders - Schedule Demo 👋 - Community Discord 💭 - Community Slack 💭 - Our numbers 📞 +1 770 8783-106 / ‭+1 412 618-6238‬ - Our emails ✉️ ishaan@berri.ai / krrish@berri.ai Why did we build this - Need for simplicity: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere. Contributors <!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <!-- markdownlint-restore --> <!-- prettier-ignore-end --> <!-- ALL-CONTRIBUTORS-LIST:END --> <a href="https://github.com/BerriAI/litellm/graphs/contributors"> <img src="https://contrib.rocks/image?repo=BerriAI/litellm" /> </a> Run in Developer mode Services 1. Setup .env file in root 2. Run dependant services Backend 1. In root create virtual environment 2. Activate virtual environment 3. Install dependencies 4. Start proxy backend Frontend 1. Navigate to 2. Install dependencies 3. Run to start the dashboard