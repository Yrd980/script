<details> <summary>Table of Contents</summary> <!--toc:start--> - What is Kwaak? - High level features - How is Kwaak different from other tools? - Latest updates on our blog :fire: - Getting started - Requirements - Docker - Api keys - Installation and setup - Homebrew - Linux and MacOS using curl - Cargo - Setup - Running Kwaak - How does it work? - Configuration - General Configuration - Command Configuration - API Key Management - Docker and GitHub Configuration - LLM Configuration - Backoff Configuration - Other configuration - Upcoming - Troubleshooting & FAQ - Community - Contributing - License <!--toc:end--> </details> <a name="readme-top"></a> <!-- PROJECT SHIELDS --> <!-- I'm using markdown "reference style" links for readability. Reference links are enclosed in brackets instead of parentheses . See the bottom of this document for the declaration of the reference variables for contributors-url, forks-url, etc. This is an optional, concise syntax you may use. https://www.markdownguide.org/basic-syntax/reference-style-links --> !CI !Coverage Status !Crate BadgeCrate !Contributorscontributors-shieldcontributors-url !Stargazersstars-shieldstars-url !MIT Licenselicense-shieldlicense-url !LinkedInlinkedin-shieldlinkedin-url !Discord <!-- PROJECT LOGO --> <br /> <div align="center"> <a href="https://github.com/bosun-ai/kwaak"> <img src="https://github.com/bosun-ai/kwaak/blob/master/images/logo.webp" alt="Logo" width="250" height="250"> </a> <h3 align="center">Kwaak</h3> <p align="center"> Burn through tech debt with AI agents!<br /> <br /> <br /> <!-- <a href="https://github.com/bosun-ai/swiftide">View Demo</a> --> <a href="https://github.com/bosun-ai/kwaak/issues/new?labels=bug&template=bugreport.md">Report Bug</a> · <a href="https://github.com/bosun-ai/kwaak/issues/new?labels=enhancement&template=featurerequest.md">Request Feature</a> · <a href="https://discord.gg/3jjXYen9UY">Discord</a> </p> </div> <!-- ABOUT THE PROJECT --> What is Kwaak? <!-- !Product Name Screen Shotproduct-screenshothttps://example.com --> Always wanted to run a team of AI agents locally from your own machine? Write code, improve test coverage, update documentation, or improve code quality, while you focus on building the cool stuff? Kwaak enables you to run a team of autonomous AI agents right from your terminal, in parallel. You interact with Kwaak in a chat-like terminal interface. Kwaak is free and open-source. You can bring your own API keys, or your own models via Ollama. <p align="center"> !demo </p> Kwaak is aware of your codebase and can answer questions about your code, find examples, write and execute code, create pull requests, and more. Unlike other tools, Kwaak is focussed on autonomous agents, and can run multiple agents at the same time. > !CAUTION > Kwaak can be considered alpha software. The project is under active development; expect breaking changes. Contributions, feedback, and bug reports are very welcome. Kwaak is part of the bosun.ai project. An upcoming platform for autonomous code improvement. Powered by Swiftide <p align="right"><a href="readme-top">back to top</a></p> High level features - Run multiple agents in parallel - Quacking terminal interface - As fast as it gets; written in Rust, powered by Swiftide - Agents operate on code, use tools, and can be interacted with - View and pull code changes from an agent; or have it create a pull request - Sandboxed execution in docker - OpenAI, Ollama, Anthropic, Azure, and many other models via OpenRouter - Python, TypeScript/Javascript, Go, Java, Ruby, Solidity, C, C++, and Rust <p align="right"><a href="readme-top">back to top</a></p> How is Kwaak different from other tools? Kwaak focuses on out-of-your-way autonomous agents. There are great tools available to utilize AI in your own coding workflow, Kwaak does the opposite. Throw your backlog at Kwaak, so you can work on the cool stuff. Latest updates on our blog :fire: - Releasing kwaak with kwaak Getting started Requirements Before you can run Kwaak, make sure you have Docker installed on your machine. Docker Kwaak expects a Dockerfile in the root of your project. If you already have a Dockerfile, you can just name it differently and configure it in the configuration file. This Dockerfile should contain all the dependencies required to test and run your code. > !NOTE > Docker is used to provide a safe execution environment for the agents. It does not affect the performance of the LLMs. The LLMs are running either locally or in the cloud, and the docker container is only used to run the code. This is done to ensure that the agents cannot access your local system. Kwaak itself runs locally. Additionally, the Dockerfile expects and should be based. A simple example for Rust: If you already have a Dockerfile for other purposes, you can either extend it or provide a new one and override the dockerfile path in the configuration. For an example Dockerfile in Rust, see this project's Dockerfile Api keys Additionally, you will need an API key for your LLM of choice. If you'd like kwaak to be able to make pull requests, search github code, and automatically push to a remote, a github token. <p align="right"><a href="readme-top">back to top</a></p> Installation and setup Pre-built binaries are available from the releases page. Homebrew Linux and MacOS using curl Cargo Install the binary directly with binstall Or compile from source with Cargo: Arch Linux The package is available in the extra repositories and can be installed with pacman: Setup Once installed, you can run in the project you want to use Kwaak in. This will guide you through the setup process and it will help you create a configuration file. See Configuration for more customization options. Api keys can be prefixed by , and to read secrets from the environment, a text string, or a file respectively. We highly recommend taking a look at the configuration file and adjusting it to your needs. There are various options that can improve the performance and accuracy of the agents. Running Kwaak You can then run in the root of your project. On initial bootup, Kwaak will index your codebase. This can take a while, depending on the size. Once indexing has been completed, subsequent startups will be faster. Keybindings: - ctrl-s: Send the current message to the agent - ctrl-x: Exit the agent - ctrl-q: Exit kwaak - ctrl-n: Create a new agent - Page Up: Scroll chat up - Page Down: Scroll chat down - tab: Switch between agents Additionally, kwaak provides a number of slash commands, will show all available commands. <p align="right"><a href="readme-top">back to top</a></p> How does it work? On initial boot up, Kwaak will index your codebase. This can take a while, depending on the size. Once indexing has been completed once, subsequent startups will be faster. Indexes are stored with duckdb. Kwaak uses the index to provide context to the agents. Kwaak provides a chat interface similar to other LLM chat applications. You can type messages to the agent, and the agent will try to accomplish the task and respond. When starting a chat, the code of the current branch is copied into an on-the-fly created docker container. This container is then used to run the code and execute the commands. After each chat completion, kwaak can lint, commit, and push the code to the remote repository if any code changes have been made. Kwaak can also create a pull request. Pull requests include an issue link to 48. This helps us identify the success rate of the agents, and also enforces transparency for code reviewers. This behaviour is fully configurable. Kwaak uses patch based editing by default. This means that only the changed lines are sent to the agent. This is more efficient. If you experience issues, try changing the edit mode to or . <p align="right"><a href="readme-top">back to top</a></p> Configuration Kwaak supports configuring different Large Language Models LLMs for distinct tasks like indexing, querying, and embedding to optimize performance and accuracy. Be sure to tailor the configurations to fit the scope and blend of the tasks you're tackling. Configuration is done in a file in the root of the project, and can be overridden by a file and/or via ENV variables. General Configuration All of these are inferred from the project directory and can be overridden in the configuration file. - : Defaults to the current directory name. Represents the name of your project. - : The programming language of the project, for instance, Rust, Go, Python, JavaScript, etc. Command Configuration Kwaak uses tests, coverages, and lints as an additional opportunity to steer the agent. Configuring these will significantly improve the agents' performance. - : Command to run tests, e.g., . - : Command for running coverage checks, e.g., . Expects coverage results as output. Currently handled unparsed via an LLM call. A friendly output is preferred - : Optional command to lint and fix project issues, e.g., in Rust. Custom tools via MCP The community has made many amazing tools available via the MCP protocol. Kwaak can be extended with these. Example: You can verify the mcp tools work by listing them with . API Key Management - API keys and tokens can be configured through environment variables , directly in the configuration , or through files . Docker and GitHub Configuration - , : Paths to Dockerfile and context, default to project root and . - , , , : GitHub repository details and token configuration. LLM Configuration Supported providers: - OpenAI - Ollama - Anthropic - Azure - OpenRouter no embeddings OpenAI Configuration: Ollama Configuration: WARN: We do not recommend using the smaller models with kwaak, apart from indexing. The model should be able to make tool calls fairly reliable. Azure Configuration: For both you can provide a to use a custom API endpoint. The can be set per provider, or globally. You can mix and match models from different providers for different tasks. Backoff Configuration Kwaak uses the exponential backoff strategy to handle retries. Currently, only OpenAI, OpenRouter, and Anthropic calls will make use of the backoff parameters. You can configure the backoff settings in the file under a section. These settings are optional, and default to the following values: - : Defaults to 15 seconds. This sets the initial waiting time between retries. - : Defaults to 2.0. This factor multiplies the interval on each retry attempt. - : Defaults to 0.05. Introduces randomness to avoid retry storms. - : Defaults to 120 seconds. This total time all attempts are allowed. Example Configuration: Other configuration - : Additional constraints / instructions for the agent. These are passes to the agent in the system prompt. If you intend to use more complicated instructions, consider adding a file to read in the repository instead. - , : Directories for cache and logs. Defaults are within your system's cache directory. - : Adjust concurrency for indexing, defaults based on CPU count. - : Batch size setting for indexing. Defaults to a higher value for Ollama and a lower value for OpenAI. - : DANGER If enabled, agents run continuously until manually stopped or completion is reached. This is meant for debugging and evaluation purposes. - : Enables OpenTelemetry tracing if set and respects all the standard OpenTelemetry environment variables. - : Defaults to . Can also be . We HIGHLY recommend using for security reasons unless you are running in a secure environment. - : Enables the agent to use tavily for web search. Their entry-level plan is free. we are not affiliated - : Defaults to . Other options are and . If you experience issues, try changing the edit mode. will always write the full file. This consumes more tokens and can have side effects. - : Enabled by default if a github key is present. Automatically pushes to the remote repository after each chat completion. You can disable this by setting it to . - : Opt-out of automatic commits after each chat completion. - : A list of tool names to enable or disable. Example: Possible values: , , , , , , , , , , , , , , , , - : Optionally hide the top header in the UI. Defaults to . - : Number of completions before the agent summarizes the conversation. Defaults to 10; - : Name which the kwaak agent will make commands with. Defaults to "kwaak"git.agentuseremailagenteditmodekwaak clear-indexerror from Bollard: Socket not found /var/run/docker.sockdata did not match any variant of untagged enum LLMConfigurationskwaak.local.toml.gitignoreKWAAKKWAAKCOMMANDTEST=cargo nextest runllmkwaakARCHITECTURE.mdgit checkout -b feature/AmazingFeaturegit commit -m 'feat: Add some AmazingFeature'git push origin feature/AmazingFeatureLICENSE for more information. <p align="right"><a href="readme-top">back to top</a></p> <!-- MARKDOWN LINKS & IMAGES --> <!-- https://www.markdownguide.org/basic-syntax/reference-style-links --> contributors-shield: https://img.shields.io/github/contributors/bosun-ai/kwaak.svg?style=flat-square contributors-url: https://github.com/bosun-ai/kwaak/graphs/contributors stars-shield: https://img.shields.io/github/stars/bosun-ai/kwaak.svg?style=flat-square stars-url: https://github.com/bosun-ai/kwaak/stargazers license-shield: https://img.shields.io/github/license/bosun-ai/kwaak.svg?style=flat-square license-url: https://github.com/bosun-ai/kwaak/blob/master/LICENSE.txt linkedin-shield: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555 linkedin-url: https://www.linkedin.com/company/bosun-ai Crate Badge: https://img.shields.io/crates/v/kwaak?logo=rust&style=flat-square&logoColor=E05D44&color=E05D44 Crate: https://crates.io/crates/kwaak