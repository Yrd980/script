OpenUI <p align="center"> <img src="./assets/openui.png" width="150" alt="OpenUI" /> </p> Building UI components can be a slog. OpenUI aims to make the process fun, fast, and flexible. It's also a tool we're using at W&B to test and prototype our next generation tooling for building powerful applications on top of LLM's. Overview !Demo OpenUI let's you describe UI using your imagination, then see it rendered live. You can ask for changes and convert HTML to React, Svelte, Web Components, etc. It's like v0 but open source and not as polished :stuckouttongueclosedeyes:. Live Demo Try the demo Running Locally OpenUI supports OpenAI, Groq, and any model LiteLLM supports such as Gemini or Anthropic Claude. The following environment variables are optional, but need to be set in your environment for alternative models to work: - OpenAI - Groq - Gemini - Anthropic - Cohere - Mistral - OpenAI Compatible and For example, if you're running a tool like localai you can set and optionally to have the models available listed in the UI's model selector under LiteLLM. Ollama You can also use models available to Ollama. Install Ollama and pull a model like Llava. If Ollama is not running on http://127.0.0.1:11434, you can set the environment variable to the host and port of your Ollama instance. For example when running in docker you'll need to point to http://host.docker.internal:11434 as shown below. Docker preferred The following command would forward the specified API keys from your shell environment and tell Docker to use the Ollama instance running on your machine. Now you can goto http://localhost:7878 and generate new UI's! From Source / Python Assuming you have git and uv installed: LiteLLM LiteLLM can be used to connect to basically any LLM service available. We generate a config automatically based on your environment variables. You can create your own proxy config to override this behavior. We look for a custom config in the following locations: 1. in the current directory 2. when running in a docker container 3. An arbitrary path specified by the environment variable For example to use a custom config in docker you can run: To use litellm from source you can run: Groq To use the super fast Groq models, set to your Groq api key which you can find here. To use one of the Groq models, click the settings icon in the nav bar. Docker Compose > DISCLAIMER: This is likely going to be very slow. If you have a GPU you may need to change the tag of the container to one that supports it. If you're running on a Mac, follow the instructions above and run Ollama natively to take advantage of the M1/M2. From the root directory you can run: If you have your OPENAIAPIKEY set in the environment already, just remove from the line. You can also replace in the command above with your open source model of choice llava is one of the only Ollama models that support images currently. You should now be able to access OpenUI at http://localhost:7878. If you make changes to the frontend or backend, you'll need to run to have them reflected in the service. Development A dev container is configured in this repository which is the quickest way to get started. Codespace <img src="./assets/codespace.png" alt="New with options..." width="500" /> Choose more options when creating a Codespace, then select New with options.... Select the US West region if you want a really fast boot time. You'll also want to configure your OPENAIAPIKEY secret or just set it to if you want to try Ollama you'll want at least 16GB of Ram. Once inside the code space you can run the server in one terminal: . Then in a new terminal: This should open another service on port 5173, that's the service you'll want to visit. All changes to both the frontend and backend will automatically be reloaded and reflected in your browser. Ollama The codespace installs ollama automaticaly and downloads the model. You can verify Ollama is running with if that fails, open a new terminal and run . In Codespaces we pull llava on boot so you should see it in the list. You can select Ollama models from the settings gear icon in the upper left corner of the application. Any models you pull i.e. will show up in the settings modal. <img src="./assets/ollama.png" width="500" alt="Select Ollama models" /> Gitpod You can easily use Open UI via Gitpod, preconfigured with Open AI. !Open in Gitpodhttps://gitpod.io/https://github.com/wandb/openui On launch Open UI is automatically installed and launched. Before you can use Gitpod: Make sure you have a Gitpod account. To use Open AI models set up the environment variable in your Gitpod User Account. Set the scope to or your repo if you forked it. > NOTE: Other local models might also be used with a bigger Gitpod instance type. Required models are not preconfigured in Gitpod but can easily be added as documented above. Resources See the readmes in the frontend and backend directories.