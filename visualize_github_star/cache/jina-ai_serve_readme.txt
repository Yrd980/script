Jina-Serve <a href="https://pypi.org/project/jina/"><img alt="PyPI" src="https://img.shields.io/pypi/v/jina?label=Release&style=flat-square"></a> <a href="https://discord.jina.ai"><img src="https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square"></a> <a href="https://pypistats.org/packages/jina"><img alt="PyPI - Downloads from official pypistats" src="https://img.shields.io/pypi/dm/jina?style=flat-square"></a> <a href="https://github.com/jina-ai/jina/actions/workflows/cd.yml"><img alt="Github CD status" src="https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg"></a> Jina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets. Scale your services from local development to production while focusing on your core logic. Key Features - Native support for all major ML frameworks and data types - High-performance service design with scaling, streaming, and dynamic batching - LLM serving with streaming output - Built-in Docker integration and Executor Hub - One-click deployment to Jina AI Cloud - Enterprise-ready with Kubernetes and Docker Compose support <details> <summary><strong>Comparison with FastAPI</strong></summary> Key advantages over FastAPI: - DocArray-based data handling with native gRPC support - Built-in containerization and service orchestration - Seamless scaling of microservices - One-command cloud deployment </details> Install See guides for Apple Silicon and Windows. Core Concepts Three main layers: - Data: BaseDoc and DocList for input/output - Serving: Executors process Documents, Gateway connects services - Orchestration: Deployments serve Executors, Flows create pipelines Build AI Services Let's create a gRPC-based AI service using StableLM: Deploy with Python or YAML: Use the client: Build Pipelines Chain services into a Flow: Scaling and Deployment Local Scaling Boost throughput with built-in features: - Replicas for parallel processing - Shards for data partitioning - Dynamic batching for efficient model inference Example scaling a Stable Diffusion deployment: Cloud Deployment Containerize Services 1. Structure your Executor: 2. Configure: 3. Push to Hub: Deploy to Kubernetes Use Docker Compose JCloud Deployment Deploy with a single command: LLM Streaming Enable token-by-token streaming for responsive LLM applications: 1. Define schemas: 2. Initialize service: 3. Implement streaming: 4. Serve and use: Support Jina-serve is backed by Jina AI and licensed under Apache-2.0.