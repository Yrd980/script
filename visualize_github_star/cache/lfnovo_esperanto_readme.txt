Esperanto 🌐 !PyPI versionhttps://badge.fury.io/py/esperanto !PyPI Downloadshttps://pypi.org/project/esperanto/ !Coveragehttps://github.com/lfnovo/esperanto !Python Versionshttps://pypi.org/project/esperanto/ !License: MIThttps://opensource.org/licenses/MIT Esperanto is a powerful Python library that provides a unified interface for interacting with various Large Language Model LLM providers. It simplifies the process of working with different AI models LLMs, Embedders, Transcribers, and TTS APIs by offering a consistent interface while maintaining provider-specific optimizations. Why Esperanto? 🚀 🪶 Ultra-Lightweight Architecture - Direct HTTP Communication: All providers communicate directly via HTTP APIs using - no bulky vendor SDKs required - Minimal Dependencies: Unlike LangChain and similar frameworks, Esperanto has a tiny footprint with zero overhead layers - Production-Ready Performance: Direct API calls mean faster response times and lower memory usage 🔄 True Provider Flexibility - Standardized Responses: Switch between any provider OpenAI ↔ Anthropic ↔ Google ↔ etc. without changing a single line of code - Consistent Interface: Same methods, same response objects, same patterns across all 15+ providers - Future-Proof: Add new providers or change existing ones without refactoring your application ⚡ Perfect for Production - Prototyping to Production: Start experimenting and deploy the same code to production - No Vendor Lock-in: Test different providers, optimize costs, and maintain flexibility - Enterprise-Ready: Direct HTTP calls, standardized error handling, and comprehensive async support Whether you're building a quick prototype or a production application serving millions of requests, Esperanto gives you the performance of direct API calls with the convenience of a unified interface. Features ✨ - Unified Interface: Work with multiple LLM providers using a consistent API - Provider Support: - OpenAI GPT-4o, o1, o3, o4, Whisper, TTS - OpenAI-Compatible LM Studio, Ollama, vLLM, custom endpoints - Anthropic Claude models - OpenRouter Access to multiple models - xAI Grok - Perplexity Sonar models - Groq Mixtral, Llama, Whisper - Google GenAI Gemini LLM, Text To Speech, Embedding with native task optimization - Vertex AI Google Cloud, LLM, Embedding, TTS - Ollama Local deployment multiple models - Transformers Universal local models - Qwen, CrossEncoder, BAAI, Jina, Mixedbread - ElevenLabs Text-to-Speech, Speech-to-Text - Azure OpenAI Chat, Embedding - Mistral Mistral Large, Small, Embedding, etc. - DeepSeek deepseek-chat - Voyage Embeddings, Reranking - Jina Advanced embedding models with task optimization, Reranking - Embedding Support: Multiple embedding providers for vector representations - Reranking Support: Universal reranking interface for improving search relevance - Speech-to-Text Support: Transcribe audio using multiple providers - Text-to-Speech Support: Generate speech using multiple providers - Async Support: Both synchronous and asynchronous API calls - Streaming: Support for streaming responses - Structured Output: JSON output formatting where supported - LangChain Integration: Easy conversion to LangChain chat models For detailed information about our providers, check out: - LLM Providers Documentation - Embedding Providers Documentation - Reranking Providers Documentation - Speech-to-Text Providers Documentation - Text-to-Speech Providers Documentation Installation 🚀 Install Esperanto using pip: Optional Dependencies Transformers Provider If you plan to use the transformers provider, install with the transformers extra: This installs: - - Core Hugging Face library - - PyTorch framework - - Fast tokenization - - CrossEncoder support - - Advanced embedding features - - Numerical computations LangChain Integration If you plan to use any of the methods, you need to install the correct LangChain SDKs manually: Provider Support Matrix | Provider | LLM Support | Embedding Support | Reranking Support | Speech-to-Text | Text-to-Speech | JSON Mode | |--------------|-------------|------------------|-------------------|----------------|----------------|-----------| | OpenAI | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | | OpenAI-Compatible | ✅ | ❌ | ❌ | ❌ | ❌ | ⚠️ | | Anthropic | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | | Groq | ✅ | ❌ | ❌ | ✅ | ❌ | ✅ | | Google GenAI | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | | Vertex AI | ✅ | ✅ | ❌ | ❌ | ✅ | ❌ | | Ollama | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | | Perplexity | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | | Transformers | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | | ElevenLabs | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | | Azure OpenAI | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | | Mistral | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | | DeepSeek | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | | Voyage | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | | Jina | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | | xAI | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | | OpenRouter | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ⚠️ OpenAI-Compatible: JSON mode support depends on the specific endpoint implementation Quick Start 🏃‍♂️ You can use Esperanto in two ways: directly with provider-specific classes or through the AI Factory. Using AI Factory The AI Factory provides a convenient way to create model instances and discover available providers: Using Provider-Specific Classes Here's a simple example to get you started: Standardized Responses All providers in Esperanto return standardized response objects, making it easy to work with different models without changing your code. LLM Responses Embedding Responses Reranking Responses Task-Aware Embeddings 🎯 Esperanto supports advanced task-aware embeddings that optimize vector representations for specific use cases. This works across all embedding providers through a universal interface: Universal Task Types: - - Optimize for search queries - - Optimize for document storage - - General text similarity - - Text classification tasks - - Document clustering - - Code search optimization - - Optimize for Q&A tasks - - Optimize for fact checking Provider Support: - Jina: Native API support for all features - Google: Native task type translation to Gemini API - OpenAI: Task optimization via intelligent text prefixes - Transformers: Local emulation with task-specific processing - Others: Graceful degradation with consistent interface The standardized response objects ensure consistency across different providers, making it easy to: - Switch between providers without changing your application code - Handle responses in a uniform way - Access common attributes like token usage and model information Provider Configuration 🔧 OpenAI OpenAI-Compatible Endpoints Use any OpenAI-compatible endpoint LM Studio, Ollama, vLLM, custom deployments with the same interface: Common Use Cases: - LM Studio: Local model serving with GUI - Ollama: with OpenAI compatibility - vLLM: High-performance inference server - Custom Deployments: Any server implementing OpenAI chat completions API Features: - ✅ Streaming: Real-time response streaming - ✅ Pass-through Model Names: Use any model name your endpoint supports - ✅ Graceful Degradation: Automatically handles varying feature support - ✅ Error Handling: Clear error messages for troubleshooting - ⚠️ JSON Mode: Depends on endpoint implementation Perplexity Perplexity uses an OpenAI-compatible API but includes additional parameters for controlling search behavior. Streaming Responses 🌊 Enable streaming to receive responses token by token: Structured Output 📊 Request JSON-formatted responses supported by OpenAI and some OpenRouter models: LangChain Integration 🔗 Convert any provider to a LangChain chat model: Documentation 📚 You can find the documentation for Esperanto in the docs directory. There is also a cool beginner's tutorial in the tutorial directory. Contributing 🤝 We welcome contributions! Please see our Contributing Guidelines for details on how to get started. License 📄 This project is licensed under the MIT License - see the LICENSE file for details. Development 🛠️ 1. Clone the repository: 2. Install dependencies: 3. Run tests: bash pytest