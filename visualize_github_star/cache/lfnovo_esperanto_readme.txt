Esperanto ğŸŒ !PyPI versionhttps://badge.fury.io/py/esperanto !PyPI Downloadshttps://pypi.org/project/esperanto/ !Coveragehttps://github.com/lfnovo/esperanto !Python Versionshttps://pypi.org/project/esperanto/ !License: MIThttps://opensource.org/licenses/MIT Esperanto is a powerful Python library that provides a unified interface for interacting with various Large Language Model LLM providers. It simplifies the process of working with different AI models LLMs, Embedders, Transcribers, and TTS APIs by offering a consistent interface while maintaining provider-specific optimizations. Why Esperanto? ğŸš€ ğŸª¶ Ultra-Lightweight Architecture - Direct HTTP Communication: All providers communicate directly via HTTP APIs using - no bulky vendor SDKs required - Minimal Dependencies: Unlike LangChain and similar frameworks, Esperanto has a tiny footprint with zero overhead layers - Production-Ready Performance: Direct API calls mean faster response times and lower memory usage ğŸ”„ True Provider Flexibility - Standardized Responses: Switch between any provider OpenAI â†” Anthropic â†” Google â†” etc. without changing a single line of code - Consistent Interface: Same methods, same response objects, same patterns across all 15+ providers - Future-Proof: Add new providers or change existing ones without refactoring your application âš¡ Perfect for Production - Prototyping to Production: Start experimenting and deploy the same code to production - No Vendor Lock-in: Test different providers, optimize costs, and maintain flexibility - Enterprise-Ready: Direct HTTP calls, standardized error handling, and comprehensive async support Whether you're building a quick prototype or a production application serving millions of requests, Esperanto gives you the performance of direct API calls with the convenience of a unified interface. Features âœ¨ - Unified Interface: Work with multiple LLM providers using a consistent API - Provider Support: - OpenAI GPT-4o, o1, o3, o4, Whisper, TTS - OpenAI-Compatible LM Studio, Ollama, vLLM, custom endpoints - Anthropic Claude models - OpenRouter Access to multiple models - xAI Grok - Perplexity Sonar models - Groq Mixtral, Llama, Whisper - Google GenAI Gemini LLM, Text To Speech, Embedding with native task optimization - Vertex AI Google Cloud, LLM, Embedding, TTS - Ollama Local deployment multiple models - Transformers Universal local models - Qwen, CrossEncoder, BAAI, Jina, Mixedbread - ElevenLabs Text-to-Speech, Speech-to-Text - Azure OpenAI Chat, Embedding - Mistral Mistral Large, Small, Embedding, etc. - DeepSeek deepseek-chat - Voyage Embeddings, Reranking - Jina Advanced embedding models with task optimization, Reranking - Embedding Support: Multiple embedding providers for vector representations - Reranking Support: Universal reranking interface for improving search relevance - Speech-to-Text Support: Transcribe audio using multiple providers - Text-to-Speech Support: Generate speech using multiple providers - Async Support: Both synchronous and asynchronous API calls - Streaming: Support for streaming responses - Structured Output: JSON output formatting where supported - LangChain Integration: Easy conversion to LangChain chat models For detailed information about our providers, check out: - LLM Providers Documentation - Embedding Providers Documentation - Reranking Providers Documentation - Speech-to-Text Providers Documentation - Text-to-Speech Providers Documentation Installation ğŸš€ Install Esperanto using pip: Optional Dependencies Transformers Provider If you plan to use the transformers provider, install with the transformers extra: This installs: - - Core Hugging Face library - - PyTorch framework - - Fast tokenization - - CrossEncoder support - - Advanced embedding features - - Numerical computations LangChain Integration If you plan to use any of the methods, you need to install the correct LangChain SDKs manually: Provider Support Matrix | Provider | LLM Support | Embedding Support | Reranking Support | Speech-to-Text | Text-to-Speech | JSON Mode | |--------------|-------------|------------------|-------------------|----------------|----------------|-----------| | OpenAI | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | | OpenAI-Compatible | âœ… | âŒ | âŒ | âŒ | âŒ | âš ï¸ | | Anthropic | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | | Groq | âœ… | âŒ | âŒ | âœ… | âŒ | âœ… | | Google GenAI | âœ… | âœ… | âŒ | âŒ | âœ… | âœ… | | Vertex AI | âœ… | âœ… | âŒ | âŒ | âœ… | âŒ | | Ollama | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | | Perplexity | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | | Transformers | âŒ | âœ… | âœ… | âŒ | âŒ | âŒ | | ElevenLabs | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ | | Azure OpenAI | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… | | Mistral | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… | | DeepSeek | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | | Voyage | âŒ | âœ… | âœ… | âŒ | âŒ | âŒ | | Jina | âŒ | âœ… | âœ… | âŒ | âŒ | âŒ | | xAI | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | | OpenRouter | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | âš ï¸ OpenAI-Compatible: JSON mode support depends on the specific endpoint implementation Quick Start ğŸƒâ€â™‚ï¸ You can use Esperanto in two ways: directly with provider-specific classes or through the AI Factory. Using AI Factory The AI Factory provides a convenient way to create model instances and discover available providers: Using Provider-Specific Classes Here's a simple example to get you started: Standardized Responses All providers in Esperanto return standardized response objects, making it easy to work with different models without changing your code. LLM Responses Embedding Responses Reranking Responses Task-Aware Embeddings ğŸ¯ Esperanto supports advanced task-aware embeddings that optimize vector representations for specific use cases. This works across all embedding providers through a universal interface: Universal Task Types: - - Optimize for search queries - - Optimize for document storage - - General text similarity - - Text classification tasks - - Document clustering - - Code search optimization - - Optimize for Q&A tasks - - Optimize for fact checking Provider Support: - Jina: Native API support for all features - Google: Native task type translation to Gemini API - OpenAI: Task optimization via intelligent text prefixes - Transformers: Local emulation with task-specific processing - Others: Graceful degradation with consistent interface The standardized response objects ensure consistency across different providers, making it easy to: - Switch between providers without changing your application code - Handle responses in a uniform way - Access common attributes like token usage and model information Provider Configuration ğŸ”§ OpenAI OpenAI-Compatible Endpoints Use any OpenAI-compatible endpoint LM Studio, Ollama, vLLM, custom deployments with the same interface: Common Use Cases: - LM Studio: Local model serving with GUI - Ollama: with OpenAI compatibility - vLLM: High-performance inference server - Custom Deployments: Any server implementing OpenAI chat completions API Features: - âœ… Streaming: Real-time response streaming - âœ… Pass-through Model Names: Use any model name your endpoint supports - âœ… Graceful Degradation: Automatically handles varying feature support - âœ… Error Handling: Clear error messages for troubleshooting - âš ï¸ JSON Mode: Depends on endpoint implementation Perplexity Perplexity uses an OpenAI-compatible API but includes additional parameters for controlling search behavior. Streaming Responses ğŸŒŠ Enable streaming to receive responses token by token: Structured Output ğŸ“Š Request JSON-formatted responses supported by OpenAI and some OpenRouter models: LangChain Integration ğŸ”— Convert any provider to a LangChain chat model: Documentation ğŸ“š You can find the documentation for Esperanto in the docs directory. There is also a cool beginner's tutorial in the tutorial directory. Contributing ğŸ¤ We welcome contributions! Please see our Contributing Guidelines for details on how to get started. License ğŸ“„ This project is licensed under the MIT License - see the LICENSE file for details. Development ğŸ› ï¸ 1. Clone the repository: 2. Install dependencies: 3. Run tests: bash pytest