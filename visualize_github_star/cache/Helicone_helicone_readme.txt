<p align="center" style="margin: 0; padding: 0;"> <img alt="helicone logo" src="https://github.com/user-attachments/assets/71c5896d-85e1-44fd-a966-0ac27170e343" width="400" style="display: block; margin: 0; padding: 0;"> </p> <div align="center"> | ğŸ” Observability | ğŸ•¸ï¸ Agent Tracing | ğŸ’¬ Prompt Management | | :--------------: | :--------------: | :------------------: | | ğŸ“Š Evaluations | ğŸ“š Datasets | ğŸ›ï¸ Fine-tuning | </div> <p align="center"> <img src="https://github.com/user-attachments/assets/e16332e9-d642-427e-b3ce-1a74a17f7b2c" alt="Open Source" width="600"> </a> <p align="center"> <a href="https://docs.helicone.ai/">Docs</a> â€¢ <a href="https://discord.gg/zsSTcH2qhG">Discord</a> â€¢ <a href="https://us.helicone.ai/roadmap">Roadmap</a> â€¢ <a href="https://www.helicone.ai/changelog">Changelog</a> â€¢ <a href="https://github.com/helicone/helicone/issues">Bug reports</a> </p> <p align="center"> <em><a href="https://helicone.ai/demo">See Helicone in Action! Free</a></em> </p> <p align="center"> <a href='https://github.com/helicone/helicone/graphs/contributors'><img src='https://img.shields.io/github/contributors/helicone/helicone?style=flat-square' alt='Contributors' /></a> <a href='https://github.com/helicone/helicone/stargazers'><img alt="GitHub stars" src="https://img.shields.io/github/stars/helicone/helicone?style=flat-square"/></a> <a href='https://github.com/helicone/helicone/pulse'><img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/helicone/helicone?style=flat-square"/></a> <a href='https://github.com/helicone/helicone/issues?q=is%3Aissue+is%3Aclosed'><img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/helicone/helicone?style=flat-square"/></a> <a href='https://www.ycombinator.com/companies/helicone'><img alt="Y Combinator" src="https://img.shields.io/badge/Y%20Combinator-Helicone-orange?style=flat-square"/></a> </p> Helicone is the all-in-one, open-source LLM developer platform - ğŸ”Œ Integrate: One-line of code to log all your requests to OpenAI, Anthropic, LangChain, Gemini, TogetherAI, LlamaIndex, LiteLLM, OpenRouter, and more - ğŸ“Š Observe: Inspect and debug traces & sessions for agents, chatbots, document processing pipelines, and more - ğŸ“ˆ Analyze: Track metrics like cost, latency, quality, and more. Export to PostHog in one-line for custom dashboards - ğŸ® Playground: Rapidly test and iterate on prompts, sessions and traces in our UI - ğŸ§  Prompt Management: Version and experiment with prompts using production data. Your prompts remain under your control, always accessible. - ğŸ” Evaluate: Automatically run evals on traces or sessions using the latest platforms: LastMile or Ragas more coming soon - ğŸ›ï¸ Fine-tune: Fine-tune with one of our fine-tuning partners: OpenPipe or Autonomi more coming soon - ğŸ›œ Gateway: Caching, custom rate limits, LLM security, and more with our gateway - ğŸ›¡ï¸ Enterprise Ready: SOC 2 and GDPR compliant > ğŸ Generous monthly free tier 10k requests/month - No credit card required! Quick Start âš¡ï¸ One line of code 1. Get your API key by signing up here. 2. Update only the in your code: or - use headers for more secure environments 3. ğŸ‰ You're all set! View your logs at Helicone. > This quick start uses Helicone Cloud with OpenAI. For other providers or self-hosted options, see below. Get Started For Free Helicone Cloud Recommended The fastest and most reliable way to get started with Helicone. Get started for free at Helicone US or Helicone EU. Your first 100k requests are free every month, after which you'll pay based on usage. Try our demo to see Helicone in action! Integrations: View our supported integrations. Latency Concerns: Helicone's Cloud offering is deployed on Cloudflare workers and ensures the lowest latency ~10ms add-on to your API requests. View our latency benchmarks. Self-Hosting Open Source LLM Observability with Helicone Docker Helicone is simple to self-host and update. To get started locally, just use our docker-compose file. Pre-Request: - Copy the shared directory to the valhalla directory - Create a valhalla folder in the valhalla directory and put /valhalla/jawn in it Helm For Enterprise workloads, we also have a production-ready Helm chart available. To access, contact us at enterprise@helicone.ai. Manual Not Recommended Manual deployment is not recommended. Please use Docker or Helm. If you must, follow the instructions here. Architecture Helicone is comprised of five services: - Web: Frontend Platform NextJS - Worker: Proxy Logging Cloudflare Workers - Jawn: Dedicated Server for serving collecting logs Express + Tsoa - Supabase: Application Database and Auth - ClickHouse: Analytics Database - Minio: Object Storage for logs. LLM Observability Integrations Main Integrations | Integration | Supports | Description | | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- | | Generic Gateway | Python, Node.js, Python w/package, LangChain JS, LangChain, cURL | Flexible integration method for various LLM providers | | Async Logging OpenLLMetry | JS/TS, Python | Asynchronous logging for multiple LLM platforms | | OpenAI | JS/TS, Python | - | | Azure OpenAI | JS/TS, Python | - | | Anthropic | JS/TS, Python | - | | Ollama | JS/TS | Run and use large language models locally | | AWS Bedrock | JS/TS | - | | Gemini API | JS/TS | - | | Gemini Vertex AI | JS/TS | Gemini models on Google Cloud's Vertex AI | | Vercel AI | JS/TS | AI SDK for building AI-powered applications | | Anyscale | JS/TS, Python | - | | TogetherAI | JS/TS, Python | - | | Hyperbolic | JS/TS, Python | High-performance AI inference platform | | Groq | JS/TS, Python | High-performance models | | DeepInfra | JS/TS, Python | Serverless AI inference for various models | | OpenRouter | JS/TS, Python | Unified API for multiple AI models | | LiteLLM | JS/TS, Python | Proxy server supporting multiple LLM providers | | Fireworks AI | JS/TS, Python | Fast inference API for open-source LLMs | Supported Frameworks | Framework | Supports | Description | | --------------------------------------------------------------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | | LangChain | JS/TS, Python | - | | LlamaIndex | Python | Framework for building LLM-powered data applications | | CrewAI | - | Framework for orchestrating role-playing AI agents | | Big-AGI | JS/TS | Generative AI suite | | ModelFusion | JS/TS | Abstraction layer for integrating AI models into JavaScript and TypeScript applications | Other Integrations | Integration | Description | | ------------------------------------------------------------------------------ | ------------------------------------------------------- | | PostHog | Product analytics platform. Build custom dashboards. | | RAGAS | Evaluation framework for retrieval-augmented generation | | Open WebUI | Web interface for interacting with local LLMs | | MetaGPT | Multi-agent framework | | Open Devin | AI software engineer | | Mem0 EmbedChain | Framework for building RAG applications | | Dify | LLMOps platform for AI-native application development | > This list may be out of date. Don't see your provider or framework? Check out the latest integrations in our docs. If not found there, request a new integration by contacting help@helicone.ai. Community ğŸŒ Learn this repo with Greptile learnthisrepo.com/helicone | Contributing We â¤ï¸ our contributors! We warmly welcome contributions for documentation, integrations, costs, and feature requests. - If you have an idea for how Helicone can be better, create a GitHub issue or vote on the roadmap - Update costs instructions in costs/README.md - Join discord to ask questions License Helicone is licensed under the Apache v2.0 License. Additional Resources - Data Management: Manage and export your Helicone data with our API. - Guides: ETL, Request Exporting - Data Ownership: Learn about Data Ownership and Autonomy For more information, visit our documentation. Contributors <a href="https://github.com/Helicone/helicone/graphs/contributors"> <img src="https://contrib.rocks/image?repo=Helicone/helicone" /> </a>